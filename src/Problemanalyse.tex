\pagebreak
\section{Problemanalyse}\label{sec:problemanalyse}

\subsection{Ist-Zustand}\label{subsec:ist-zustand-und-problemstellung}

In modernen Organisationen wächst die Menge an digital verfügbaren Informationen rasant.
Wissen liegt in Form unzähliger Dokumente, E-Mails, Tickets, Wikiseiten und Datenbankeinträge vor, verteilt über unterschiedliche Systeme wie etwa SharePoint-Sites, OneDrive-Verzeichnisse, File-Server oder Fachanwendungen.
Empirische Studien zeigen, dass Wissensarbeiter einen erheblichen Anteil ihrer Arbeitszeit damit verbringen, Informationen zu suchen oder doppelt zu erzeugen, weil sie bestehende Inhalte nicht (wieder)finden.\footnote{Vgl. etwa IDC-Whitepaper \glqq The High Cost of Not Finding Information\grqq{}, 2004, das von 15--35\,\% der Arbeitszeit für Informationssuche ausgeht.}
Diese Situation wird häufig als \textit{Informationsüberlastung} beschrieben: Es existiert zwar viel Wissen, es ist aber nur begrenzt zugänglich.

Im hier betrachteten Unternehmenskontext werden zentrale Informationen vor allem in Microsoft-365-Diensten wie SharePoint und OneDrive abgelegt.
Dort befinden sich unter anderem Produktdokumentationen, Prozessbeschreibungen, Projektunterlagen sowie Schulungs- und Präsentationsmaterialien.
Mitarbeitende, die Antworten auf spezifische Fragen suchen (etwa zu Richtlinien, Projektberichten oder technischen Details), greifen derzeit primär auf folgende Strategien zurück:

\begin{itemize}
    \item manuelle Volltextsuche in SharePoint oder OneDrive,
    \item das gezielte Durcharbeiten längerer Dokumente,
    \item das Nachfragen bei Kolleginnen und Kollegen (\glqq informelles Expertennetz\grqq{}).
\end{itemize}

Diese Vorgehensweisen sind erfahrungsgemäß zeitaufwendig, fehleranfällig und schlecht skalierbar:
Trefferlisten klassischer Suchfunktionen liefern häufig eine große Menge an potentiell relevanten Dokumenten, ohne jedoch konkrete Antworten auf die eigentliche Frage zu geben.
Insbesondere neue Mitarbeitende oder Kolleginnen und Kollegen ohne tiefes Domänenwissen haben Schwierigkeiten, die Relevanz und Aktualität einzelner Dokumente zu beurteilen.

Hinzu kommt, dass moderne generative KI-Systeme wie GPT-4 zwar beeindruckende sprachliche Fähigkeiten besitzen, jedoch im Standardbetrieb keinen Zugriff auf interne Unternehmensdokumente haben.
Ohne zusätzliche Architekturkomponenten verfügen sie nur über allgemeines Weltwissen bis zu ihrem Trainingsstand, aber weder über aktuelle noch über firmeninterne Informationen.
Daraus resultiert die Gefahr, dass ein LLM auf spezifische Fragen entweder gar keine oder fehlerhafte Antworten (sog.\ \textit{Halluzinationen}) gibt.\href{https://en.wikipedia.org/wiki/Retrieval-augmented_generation#:~:text=Retrieval,responses%20based%20on%20authoritative%20sources}{en.wikipedia.org}
Beispielsweise kann ein Standard-LLM nicht wissen, welche internen Richtlinien ein bestimmtes Unternehmen hat, wenn diese Informationen nicht öffentlich sind oder im Training nicht berücksichtigt wurden.

Zudem wäre ein Einsatz von Cloud-basierten LLM-Diensten in vielen Fällen aus Datenschutz- und Compliance-Sicht problematisch.
Vertrauliche Unternehmensdaten dürfen nicht unkontrolliert an externe Dienste gesendet werden.\href{https://punctuations.ai/ai-agents-workflows/your-private-gpt-the-case-for-secure-on-premise-llms/#:~:text=pharmaceutical%20companies%2C%20manufacturers%2C%20government%20contractors,keeping%20AI%20close%20to%20home}{punctuations.ai}
In streng regulierten Branchen oder bei sensiblen Informationen (etwa personenbezogene Daten, Geschäftsgeheimnisse) überwiegen die Risiken gegenüber dem Nutzen, wenn öffentliche KI-APIs ohne zusätzliche Schutzmaßnahmen verwendet werden.
Zusätzlich fehlt es generischen LLMs an Aktualität und Domänenwissen: Sie könnten halluzinierende Antworten geben, also sachlich falsche Auskünfte erteilen, wenn die Anfrage sich auf interne Details bezieht.\href{https://en.wikipedia.org/wiki/Retrieval-augmented_generation#:~:text=RAG%20improves%20large%20language%20models,4}{en.wikipedia.org}
Dieses Risiko ist in professionellen Anwendungsfällen nicht akzeptabel.

Ein typischer Nutzer des geplanten Chatbots ist ein Mitarbeiter im Unternehmen, der regelmäßig auf interne Dokumente zugreifen muss, um seine Aufgaben zu erfüllen.
Beispielsweise könnte dies ein Projektmanager sein, der Informationen zu aktuellen Projektrichtlinien, technischen Spezifikationen oder Berichten benötigt.
Noch häufiger ist dies ein Anwendungsberater, der Kundenfragen zu Produkten oder Dienstleistungen beantworten muss.
In einer zweiten Prioritätsstufe sind es die technischen Berater, die intern auf technische Dokumentationen und Anleitungen zugreifen müssen.
Diese Nutzergruppe profitiert besonders von einem Chatbot, der ihnen schnell präzise Antworten liefert, ohne dass sie lange technische Dokumente für Infor LN oder Infor OS durchsuchen müssen.
Diesen Dokumenten fehlt es heutzutage oftmals an Qualität in Bezug auf Struktur, Formatierung und Verständlichkeit, da sie oft aus anderen Sprachen übersetzt wurden oder von verschiedenen Autoren mit unterschiedlichen Stilen verfasst sind.
Die Nutzer erwarten hingegen eine einfache, intuitive Bedienung, ähnlich wie bei modernen Chat-Anwendungen, und konsistente Antworten, die sich im Zweifel anhand der zugrunde liegenden Dokumente nachvollziehen lassen.

\subsubsection{Stakeholder und Rollen}\label{subsubsec:stakeholder}

Aus Sicht des Requirements Engineering lassen sich mehrere primäre Stakeholder identifizieren:

\begin{itemize}
    \item \textbf{Projektmanagerinnen und Projektmanager:}
    Planen und steuern Projekte und benötigen schnellen Zugriff auf Richtlinien, Vorlagen, Prozessbeschreibungen und Statusberichte.
    Sie legen Wert auf verlässliche, aktuelle Informationen und wollen nicht jede Detailfrage an Fachabteilungen eskalieren.

    \item \textbf{Anwendungsberaterinnen und Anwendungsberater:}
    Agieren an der Schnittstelle zum Kunden und beantworten Fragen zu Produkten, Releases, Konfigurationsmöglichkeiten und Best Practices.
    Sie stehen oft unter Zeitdruck und müssen sich darauf verlassen können, dass der Chatbot konsistente, produktspezifische Aussagen liefert, die mit der offiziellen Dokumentation übereinstimmen.

    \item \textbf{Technische Berater (z.\,B.\ System Engineers):}
    Bearbeiten Supportfälle, führen Installationen, Upgrades oder Fehleranalysen durch und benötigen Zugriff auf technische Handbücher, Troubleshooting-Guides und interne Wissensartikel.
    Für sie ist es wichtig, schnell konkrete Schritte und Konfigurationsbeispiele zu erhalten.

    \item \textbf{IT-Administration und Governance:}
    Verantwortlich für Betrieb, Sicherheit und Compliance des Systems.
    Diese Gruppe achtet insbesondere auf Datenschutz, Zugriffskontrolle, Logging und Wartbarkeit.

    \item \textbf{Management:}
    Interessiert an Produktivitätsgewinnen, Risikoreduktion und der strategischen Positionierung des Unternehmens im Bereich moderner KI-Technologien.
\end{itemize}

Diese Stakeholder haben teilweise unterschiedliche, teilweise überlappende Anforderungen.
Eine strukturierte Problemanalyse muss diese Vielfalt berücksichtigen, um nicht nur einen \enquote{technisch interessanten}, sondern auch tatsächlich nutzbaren Chatbot zu konzipieren.

\subsubsection{User Stories}\label{subsec:user-stories}

Um die Perspektive der Anwender explizit zu machen, werden im Folgenden User Stories formuliert, die typische Anwendungsfälle und Anforderungen der Nutzer beschreiben:

\begin{itemize}
    \item \textbf{Als Projektmanager} möchte ich schnell Antworten auf Fragen zu internen Richtlinien und Verfahren erhalten, damit ich meine Projekte effizient planen und durchführen kann, ohne lange in SharePoint nach Dokumenten suchen zu müssen.

    \item \textbf{Als Anwendungsberater} möchte ich in der Lage sein, Kundenanfragen zu Produkten und Dienstleistungen zügig zu beantworten, indem ich auf relevante interne Dokumente zugreife, damit ich dem Kunden eine konsistente, verlässliche und aktuelle Antwort geben kann.

    \item \textbf{Als technischer Berater} möchte ich technische Anleitungen und Dokumentationen schnell finden können, um Supportanfragen effektiv zu bearbeiten und Konfigurationsfehler oder Bugs zielgerichtet identifizieren zu können.
\end{itemize}

Im Sinne gängiger Requirements-Engineering-Praxis können diese User Stories durch Akzeptanzkriterien präzisiert werden.
Tabelle~\ref{tab:user-stories-akzeptanzkriterien} fasst die zentralen Stories und deren Erfolgskriterien zusammen.

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{p{2.5cm}p{3cm}p{7.5cm}}
        \toprule
        \textbf{Rolle} & \textbf{User Story-ID} & \textbf{Ausgewählte Akzeptanzkriterien} \\
        \midrule
        Projektmanager & US1 &
        \begin{itemize}[leftmargin=*]
            \item Mindestens 80\,\% der Standardanfragen (z.\,B.\ \enquote{Wie ist der Freigabeprozess für Dokumente?}) können innerhalb von wenigen Sekunden beantwortet werden.
            \item Die Antwort verweist auf die zugrunde liegenden Richtliniendokumente (z.\,B.\ per Link oder Zitation).
        \end{itemize} \\
        \addlinespace
        Anwendungsberater & US2 &
        \begin{itemize}[leftmargin=*]
            \item Der Chatbot kann auf Basis interner Produktdokumentation konkrete Feature-Beschreibungen oder Konfigurationsschritte liefern.
            \item Widersprüche zwischen Antwort und offizieller Dokumentation treten in Stichprobenprüfungen nur selten auf.
        \end{itemize} \\
        \addlinespace
        Technischer Berater & US3 &
        \begin{itemize}[leftmargin=*]
            \item Der Chatbot liefert bei Fehlerbeschreibungen geeignete Troubleshooting-Schritte oder verweist explizit auf relevante Abschnitte in Handbüchern.
            \item Antworten enthalten, wo sinnvoll, konkrete Konfigurationsbeispiele oder Code-Snippets.
        \end{itemize} \\
        \bottomrule
    \end{tabular}
    \caption{Zentrale User Stories und exemplarische Akzeptanzkriterien}
    \label{tab:user-stories-akzeptanzkriterien}
\end{table}

\subsubsection{Use Cases}\label{subsec:use-cases}

Auf Basis der Stakeholderanalyse und User Stories lassen sich konkrete fachliche Use Cases identifizieren.
Diese Use Cases strukturieren die Anforderungen und dienen als Grundlage für Architektur- und Designentscheidungen.
Tabelle~\ref{tab:use-cases} gibt einen Überblick über zentrale Anwendungsfälle.

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{p{1.2cm}p{3.4cm}p{3.2cm}p{5.8cm}p{1.6cm}}
        \toprule
        \textbf{ID} & \textbf{Name} & \textbf{Primärer Akteur} & \textbf{Kurzbeschreibung} & \textbf{Priorität} \\
        \midrule
        UC1 & Ad-hoc-Frage zu Richtlinien & Projektmanager &
        Nutzer stellt eine freie Frage zu internen Prozessen oder Richtlinien (z.\,B.\ Freigabeprozesse, Rollenmodelle) und erhält eine zusammenfassende Antwort mit Verweis auf die Originaldokumente. & hoch \\
        \addlinespace
        UC2 & Kundenfrage zu Produktfunktion & Anwendungsberater &
        Berater gibt eine Kundenfrage in natürlicher Sprache ein (z.\,B.\ zu einer Funktion in Infor LN) und erhält eine Antwort, die auf der internen Produktdokumentation basiert. & sehr hoch \\
        \addlinespace
        UC3 & Technische Fehlersuche & Technischer Berater &
        Nutzer beschreibt einen Fehler oder eine Fehlermeldung.
        Das System sucht in technischen Handbüchern und Wissensartikeln und schlägt mögliche Ursachen und Lösungswege vor. & hoch \\
        \addlinespace
        UC4 & Dokumentenrecherche nach Schlagworten & Alle Rollen &
        Nutzer sucht nach Dokumenten oder Abschnitten zu einem Thema (z.\,B.\ \enquote{Lizenzbedingungen}) und erhält eine Liste relevanter Ausschnitte mit Kurzbeschreibung. & mittel \\
        \addlinespace
        UC5 & Verwaltung der Wissensbasis & IT-Administration &
        Administrator fügt neue Dokumentquellen hinzu, stößt Reindizierung an oder entfernt veraltete Inhalte, ohne den laufenden Betrieb zu stören. & mittel \\
        \addlinespace
        UC6 & Einsicht in Nutzungs- und Fehlerprotokolle & IT-Administration / Management &
        Berechtigte Personen können Statistiken (z.\,B.\ Anzahl Anfragen, Antwortzeiten, Fehlerraten) und ausgewählte Chatverläufe einsehen, um Systemqualität und Nutzung zu evaluieren. & mittel \\
        \bottomrule
    \end{tabular}
    \caption{Überblick über zentrale Use Cases des RAG-Chatbots}
    \label{tab:use-cases}
\end{table}

Diese Use Cases machen deutlich, dass der geplante Chatbot nicht lediglich eine \enquote{komfortablere Suchmaske} sein soll, sondern eine integrierte Wissensschnittstelle, die Retrieval- und Generationsfähigkeiten kombiniert und zugleich Anforderungen an Sicherheit, Datenschutz und Wartbarkeit erfüllt.

\subsubsection{Zusammenfassung der Problemstellung}

Die zentrale Problemstellung liegt somit darin, einen Chatbot zu konzipieren, der auf firmeninternes Wissen zurückgreift, ohne dieses Wissen nach außen preiszugeben, und der gleichzeitig die Heterogenität der Datenquellen und der IT-Landschaft berücksichtigt.
Hierbei treten mehrere, eng miteinander verknüpfte Herausforderungen auf:

\begin{enumerate}
    \item \textbf{Integration heterogener Datenquellen:}
    SharePoint und OneDrive enthalten Dateien in unterschiedlichen Formaten (PDF, Word, E-Mails, etc.), die automatisiert ausgelesen werden sollen.
    Dies erfordert robuste Mechanismen zur Dateiverarbeitung und -indexierung.
    Außerdem sind diese Plattformen oft in komplexe Authentifizierungs- und Berechtigungssysteme eingebunden.
    So kann es beispielsweise passieren, dass \textit{automatisierte Zugriffe} von Sicherheitsmechanismen blockiert werden – ein Umstand, der in ersten Tests tatsächlich beobachtet wurde.\href{https://alain-airom.medium.com/populating-a-rag-with-data-from-enterprise-documents-repositories-for-generative-ai-4ded82952c67#:~:text=W%20riting%20the%20main%20application,activities%20on%20my%20account}{alain-airom.medium.com}

    \item \textbf{Datenschutz und Zugriffskontrolle:}
    Selbst wenn alle Daten intern verarbeitet werden, muss gewährleistet sein, dass nur berechtigte Informationen im jeweiligen Kontext ausgegeben werden und keine sensiblen Inhalte unkontrolliert verbreitet werden.
    Insbesondere müssen bestehende Berechtigungskonzepte aus Systemen wie SharePoint respektiert werden, sodass der Chatbot einem Nutzer nur solche Informationen liefert, die dieser auch regulär lesen darf.\href{https://www.realmlabs.ai/security/building-a-secure-rag-chatbot-on-microsoft-sharepoint#:~:text=Role}{realmlabs.ai}
    Darüber hinaus muss die Verarbeitung sensibler Inhalte zielgerichtet und minimalinvasiv erfolgen.

    \item \textbf{Technische Realisierbarkeit im vorhandenen IT-Umfeld:}
    Viele KI-Frameworks und -Tools sind primär auf Linux-basierte Umgebungen ausgerichtet, während im Unternehmen möglicherweise ein Windows Server als Plattform vorgesehen ist.
    Tatsächlich traten bei Tests auf einem Windows Server einige Einschränkungen auf – gewisse Software-Komponenten funktionierten nur eingeschränkt, und Performance-Probleme mussten adressiert werden (siehe Kapitel~\ref{sec:technische-umsetzung}).
    Hinzu kommt, dass initial nur eingeschränkte Hardware-Ressourcen (z.\,B.\ ein Linux-VPS ohne GPU) zur Verfügung standen.
\end{enumerate}

Die Problemanalyse zeigt damit, dass es nicht ausreicht, ein beliebiges LLM \enquote{vor} die existierenden Dokumente zu schalten.
Vielmehr ist eine durchdachte Gesamtarchitektur erforderlich, die Retrieval-Komponenten, Generierungsmodell, Datenquellen, Sicherheitsmechanismen und Benutzeroberfläche integriert.

\subsection{Soll-Zustand}

Der angestrebte Soll-Zustand ist ein datenschutzkonformer RAG-Chatbot, der einige Eigenschaften erfüllen muss.
Er muss auf kommerzieller Hardware innerhalb der Unternehmensinfrastruktur betrieben werden können (On-Premises).
Konnektoren zu SharePoint und OneDrive müssen im besten Fall automatisiert Dokumente einlesen können.
Der Chatbot soll in der Lage sein, natürlichsprachliche Fragen der Nutzer zu verstehen und präzise Antworten zu generieren, indem er relevante Informationen aus den internen Dokumenten extrahiert.
Dafür muss der Chatbot in der Lage sein, Deutsch und Englisch zu verstehen und auch in beiden Sprachen, unabhängig von der Sprache der Anfrage, zu antworten.
Die Antwortqualität soll so hoch sein, dass die Nutzer den Chatbot als verlässliche Informationsquelle wahrnehmen.
Das System muss sicherstellen, dass keine sensiblen Daten unkontrolliert nach außen gelangen.
Zudem soll der Chatbot in eine benutzerfreundliche Oberfläche integriert werden, die niederschwellig zugänglich ist (z.\,B.\ über den Browser).

Der Chatbot muss komplett on-premises betrieben werden können, um Datenschutzanforderungen zu erfüllen und regulatorische Vorgaben zur Datenresidenz einzuhalten.

\subsubsection{Zielbild und Qualitätsziele}\label{subsec:zielbild}

Aus der Gegenüberstellung von Ist- und Soll-Zustand lassen sich mehrere übergeordnete Qualitätsziele ableiten:

\begin{itemize}
    \item \textbf{Effizienzsteigerung in der Informationssuche:}
    Die Zeit, die Mitarbeitende für das Auffinden relevanter Informationen benötigen, soll signifikant reduziert werden, idealerweise von mehreren Minuten pro Anfrage auf wenige Sekunden.

    \item \textbf{Fachliche Korrektheit und Nachvollziehbarkeit:}
    Antworten des Chatbots sollen sich auf klar benennbare Quellen stützen.
    Nutzende sollen in der Lage sein, die Antwort anhand der zugrunde liegenden Dokumente zu überprüfen (z.\,B.\ durch Zitationen und Links).

    \item \textbf{Datenschutzkonformität und Sicherheit:}
    Sämtliche Verarbeitungsprozesse müssen die Prinzipien der DSGVO (Datenminimierung, Zweckbindung, Integrität und Vertraulichkeit) respektieren und bestehende Berechtigungskonzepte der Quellsysteme berücksichtigen.

    \item \textbf{Benutzerakzeptanz:}
    Die Interaktion mit dem Chatbot soll so gestaltet werden, dass sie der Nutzung moderner Chat-Anwendungen ähnelt.
    Eine niedrige Einstiegshürde ist entscheidend, damit der Chatbot im Alltag tatsächlich genutzt wird.

    \item \textbf{Wartbarkeit und Erweiterbarkeit:}
    Die Lösung soll so gestaltet sein, dass neue Dokumentquellen, Modelle oder Funktionalitäten (z.\,B.\ zusätzliche Agenten oder spezialisierte Wissensbereiche) ohne grundlegende Neuimplementierung integriert werden können.
\end{itemize}

Diese Qualitätsziele dienen im weiteren Verlauf als Referenzrahmen für Designentscheidungen und bilden die Grundlage für spätere Evaluationsschritte.

\subsubsection{Abgrenzung zu kommerziellen Cloud-Diensten}\label{subsec:abgrenzung-zu-kommerziellen-cloud-diensten}

Die Entscheidung, einen datenschutzkonformen RAG-Chatbot zu entwickeln, der ausschließlich on-premises betrieben wird, basiert auf mehreren Überlegungen.
Kommerzielle Cloud-Dienste für KI-Modelle bieten zwar eine einfache und skalierbare Lösung, bringen jedoch erhebliche Datenschutz- und Sicherheitsrisiken mit sich.
Insbesondere in regulierten Branchen oder bei sensiblen Unternehmensdaten ist es oft nicht zulässig, diese Daten an externe Cloud-Anbieter zu übermitteln.
Die Kontrolle über die Datenverarbeitung und -speicherung geht verloren, was zu Compliance-Verstößen führen kann.
Zudem besteht das Risiko, dass vertrauliche Informationen ungewollt offengelegt werden.

Man kann ohne Weiteres gegen ein Entgelt sofort auf vielerlei solcher Anbieter zugreifen, wie etwa OpenAI, Azure OpenAI, Google Cloud AI oder Amazon Bedrock.
Diese Dienste bieten leistungsstarke LLMs und einfache APIs, die eine schnelle Integration ermöglichen.
Allerdings sind diese Dienste oft mit hohen laufenden Kosten verbunden, insbesondere bei großem Datenvolumen oder hoher Nutzungsfrequenz.
Für kleinere Unternehmen oder solche mit begrenztem IT-Budget können diese Kosten prohibitiv sein.
Darüber hinaus sind kommerzielle Cloud-Dienste häufig nicht flexibel genug, um spezifische Anforderungen oder Anpassungen zu erfüllen.
Die Nutzer sind auf die Funktionen und Updates des Anbieters angewiesen, was die Innovationsfähigkeit einschränken kann.

Im Gegensatz dazu ermöglicht ein on-premises RAG-Chatbot die vollständige Kontrolle über die Daten und die Infrastruktur.
Unternehmen können sicherstellen, dass alle Datenschutz- und Sicherheitsanforderungen eingehalten werden.
Zudem können sie die Lösung genau auf ihre Bedürfnisse zuschneiden und haben die Freiheit, neue Funktionen oder Modelle zu integrieren, ohne auf einen Drittanbieter angewiesen zu sein.
Es ist auch ein klares Signal an Kunden und Partner, dass das Unternehmen den Schutz sensibler Informationen ernst nimmt.

Zudem ist hier nicht zu vernachlässigen, dass sowohl die persönliche Motivation dieser Masterarbeit als auch die des Unternehmens darin liegt, auch selbst innovativ tätig zu werden und nicht nur auf fertige Lösungen von Drittanbietern zurückzugreifen.
Es ist ein klares Ziel, Know-how im Bereich moderner KI-Technologien aufzubauen und langfristig eigene Kompetenzen zu entwickeln.
KI soll nicht nur konsumiert, sondern auch gestaltet werden.
Das soll nicht den Alltagsbetrieb stören, sondern als Kooperator fungieren.
Deswegen wird hier explizit auf kommerzielle Cloud-Dienste verzichtet.

Wenn das Ergebnis dieser Arbeit zeigt, dass ein datenschutzkonformer RAG-Chatbot on-premises machbar ist, könnte dies als Grundlage für weitere Entwicklungen und Innovationen im Unternehmen dienen.
Falls nicht, so liefert es wertvolle Erkenntnisse über die Herausforderungen und Grenzen solcher Ansätze und kann als Entscheidungsgrundlage für zukünftige Investitionen in KI-Technologien dienen.

\subsection{Leistungsanforderungen}

Derzeit existiert keine lokale On-Premise Hardware- oder Software-Infrastruktur für einen datenschutzkonformen RAG-Chatbot im Unternehmen.
Die Mitarbeiter greifen auf SharePoint und OneDrive zu, um Dokumente zu speichern und zu verwalten.
Es gibt jedoch keine automatisierten Mechanismen, um diese Dokumente für einen Chatbot zugänglich zu machen.
Die Mitarbeiter müssen manuell nach Informationen suchen, was zeitaufwendig und ineffizient ist.
Es gibt keine bestehende Chatbot-Lösung, die auf internen Dokumenten basiert.
Die IT-Infrastruktur des Unternehmens ist hauptsächlich Windows-basiert, was die Auswahl und Integration von KI-Tools und -Frameworks einschränkt.
Zudem sind Datenschutz- und Sicherheitsrichtlinien vorhanden, die den Umgang mit sensiblen Daten regeln.
Diese Richtlinien müssen bei der Entwicklung des Chatbots strikt eingehalten werden.

Insgesamt besteht eine deutliche Lücke zwischen dem aktuellen Zustand und dem angestrebten Soll-Zustand, die durch die Entwicklung eines datenschutzkonformen RAG-Chatbots geschlossen werden soll.

Aus den in Abschnitt~\ref{subsec:ist-zustand-und-problemstellung} beschriebenen User Stories und Use Cases lassen sich die folgenden funktionalen und nicht-funktionalen Leistungsanforderungen ableiten.

\subsubsection{Funktionale Anforderungen}

Exemplarisch lassen sich die wichtigsten funktionalen Anforderungen wie folgt gruppieren:

\begin{itemize}
    \item \textbf{FA1: Natürlichsprachliche Fragebeantwortung.}
    Das System muss in der Lage sein, natürlichsprachliche Fragen der Nutzer in Deutsch und Englisch zu verstehen und zu beantworten (User Stories US1--US3, Use Cases UC1--UC3).

    \item \textbf{FA2: Anbindung interner Dokumentquellen.}
    Der Chatbot muss Dokumente aus SharePoint und OneDrive automatisiert einlesen, vorverarbeiten und in einer durchsuchbaren Wissensbasis ablegen (Use Cases UC4, UC5).

    \item \textbf{FA3: Kontextsensitive Antwortgenerierung.}
    Antworten müssen explizit auf zuvor aus den Dokumenten extrahiertem Kontext basieren (RAG-Paradigma) und idealerweise Quellenverweise enthalten.

    \item \textbf{FA4: Berechtigungsbewusstes Retrieval.}
    Das System muss bei der Auswahl von Kontextdokumenten die Berechtigungen der jeweiligen Nutzer respektieren und darf keine Informationen ausgeben, auf die im Ursprungssystem kein Zugriff besteht.

    \item \textbf{FA5: Administrations- und Wartungsfunktionen.}
    Es müssen Funktionen zur Verwaltung der Wissensbasis (Hinzufügen, Aktualisieren, Löschen von Dokumenten), zur Konfiguration von Konnektoren und zur Einsicht in grundlegende Betriebsmetriken bereitgestellt werden (Use Cases UC5, UC6).

    \item \textbf{FA6: Schnittstellen zu Frontends.}
    Der Chatbot muss über eine API ansprechbar sein, sodass eine Web-Oberfläche sowie perspektivisch weitere Integrationen (z.\,B.\ Teams, Intranet) realisiert werden können.
\end{itemize}

\subsubsection{Nicht-funktionale Anforderungen}

Neben diesen funktionalen Anforderungen ergeben sich eine Reihe nicht-funktionaler Anforderungen und Randbedingungen:

\begin{itemize}
    \item \textbf{NFA1: Performance.}
    Die Antwortzeit für typische Anfragen soll im Bereich weniger Sekunden liegen.
    Längere Wartezeiten würden die Akzeptanz des Systems deutlich reduzieren.

    \item \textbf{NFA2: Skalierbarkeit.}
    Die Lösung soll mehrere parallele Nutzeranfragen unterstützen und mit wachsender Dokumentenmenge skalieren können, ohne dass Antwortzeiten unvertretbar steigen.

    \item \textbf{NFA3: Datenschutz und Sicherheit.}
    Sämtliche Verarbeitung muss DSGVO-konform erfolgen, Daten sollen die Unternehmensinfrastruktur nicht verlassen, und bestehende Rollen- und Rechtestrukturen müssen respektiert werden.

    \item \textbf{NFA4: Portabilität und On-Premises-Betrieb.}
    Das System soll auf handelsüblicher Unternehmenshardware (z.\,B.\ Windows-Server oder leistungsfähige Laptops mit GPU) lauffähig sein und nach Möglichkeit containerisiert betrieben werden können.

    \item \textbf{NFA5: Wartbarkeit und Erweiterbarkeit.}
    Die Architektur soll modular aufgebaut sein, sodass Komponenten wie Embedding-Modelle, Vektordatenbanken oder LLMs austauschbar sind, ohne das Gesamtsystem neu zu implementieren.

    \item \textbf{NFA6: Benutzbarkeit.}
    Die Benutzeroberfläche soll intuitiv bedienbar sein und sich an etablierten Chat-Interfaces orientieren, um Schulungsaufwand zu minimieren.
\end{itemize}

Aufgrund dessen wurden sich potentielle Kandidaten für die technische Umsetzung angeschaut und bewertet.

\subsubsection{CPU vs GPU}

Ein weiterer wichtiger Aspekt bei der Auswahl der technischen Umsetzung ist die Frage, ob die KI-Modelle auf CPU- oder GPU-Hardware laufen sollen.
GPU-beschleunigte Systeme bieten in der Regel eine deutlich höhere Leistung bei der Verarbeitung großer Modelle und Datenmengen.
Sie sind besonders vorteilhaft, wenn Echtzeit-Antworten und eine hohe Skalierbarkeit erforderlich sind.
Allerdings sind GPUs oft teurer in der Anschaffung und im Betrieb, was für kleinere Unternehmen eine Hürde darstellen kann.

CPU-basierte Systeme sind hingegen kostengünstiger und einfacher zu warten.
Sie können für kleinere Modelle und weniger intensive Workloads ausreichend sein.
Im Rahmen dieser Arbeit wurde zunächst die Entscheidung getroffen, eine CPU-basierte Lösung zu verfolgen, um die Kosten zu minimieren und die Komplexität der Infrastruktur zu reduzieren.
Dies stellt sicher, dass der Chatbot prinzipiell auch auf handelsüblicher Hardware betrieben werden kann, was den Zugang für kleinere Unternehmen erleichtert.

Es stand bisher nur ein Linux-VPS mit 2 vCPUs und 4 GB RAM zur Verfügung, was für die Ausführung größerer Modelle und die Verarbeitung umfangreicher Dokumente nicht ausreicht.
Dieser geriet schnell auf Grund der fehlenden Hardware-Beschleunigung an seine Grenzen.
VPS-Server haben oftmals, wie auch in diesem Fall, Einschränkungen bei der Installation bestimmter Software-Komponenten sowie der Nutzung der Hardware-Beschleunigung (kein Zugriff auf GPU, eingeschränkte CPU-Leistung).

Daher wurde entschieden, die Entwicklung und das Testen der Anwendung auf einem Windows-Server mit folgenden Spezifikationen durchzuführen:

\begin{itemize}
    \item \textbf{Prozessor:} Intel Xeon E3-1230 v6, 4 Kerne, 3{,}5 GHz
    \item \textbf{Arbeitsspeicher:} 16 GB RAM
    \item \textbf{Speicher:} 2 x 480 GB SSD im Software-RAID 1
    \item \textbf{Betriebssystem:} Windows Server
\end{itemize}

Sollte diese Hardwareleistung ausreichen, würden damit größere Kosten für Cloud-Server sowie GPUs vermieden werden können.
Die späteren Experimente (siehe Kapitel~\ref{sec:technische-umsetzung}) zeigen allerdings, dass für eine praktikable Antwortzeit und Skalierbarkeit eine GPU-Unterstützung faktisch notwendig ist, sodass die Hardwarebasis im Laufe der Arbeit angepasst wurde.

\subsubsection{Open-Source Tools und Frameworks}

Für die Umsetzung des datenschutzkonformen RAG-Chatbots werden verschiedene Open-Source-Tools und Frameworks evaluiert.
Die Entscheidung Open-Source-Lösungen zu nutzen, basiert auf mehreren Faktoren.
Erstens ermöglichen Open-Source-Tools eine hohe Flexibilität und Anpassungsfähigkeit.
Der Quellcode ist frei zugänglich, was es ermöglicht, die Software an spezifische Anforderungen anzupassen und zu erweitern.
Zweitens entfallen Lizenzkosten, was insbesondere für kleinere Unternehmen von Vorteil ist.
So entfällt auch eine Abhängigkeit von kommerziellen Anbietern, die möglicherweise nicht alle Datenschutzanforderungen erfüllen oder jederzeitige Änderungen in ihren Diensten vornehmen könnten.

Wichtige Kriterien bei der Auswahl waren die Kompatibilität mit der Windows-basierten IT-Infrastruktur, die Fähigkeit zur Verarbeitung von Dokumenten aus SharePoint und OneDrive, die Unterstützung für RAG-Architekturen und die Einhaltung von Datenschutzanforderungen.
Die Evaluierung dieser Tools ist in folgender Tabelle zusammengefasst:

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lp{2cm}p{2.4cm}p{2.2cm}p{1.1cm}p{2.6cm}}
        \toprule
        \textbf{Framework/Tool} & \textbf{RAG / Zitation} & \textbf{Office \& PDF} & \textbf{Lokale API} & \textbf{Typ. RAM} & \textbf{Wann empfehlenswert} \\
        \midrule
        GPT-4All Desktop & Integriertes RAG, Seiten-Zitate & DOCX / XLSX native; kein OCR & OpenAI-kompatibel (Einstellungen) & \(\sim\)12 GB & Einfache GUI, schnell lauffähig \\
        Open WebUI + Ollama & RAG-Plugin, Quellenangaben & Viele Formate; OCR per Add-on & OpenAI-kompatible Endpunkte & 8--14 GB & Moderne Web-UI mit REST-API \\
        LM Studio & Knowledge-Workspace mit Zitationen & PDF, TXT; Office nachrüstbar & OpenAI-kompatible API (Port 1234) & 10--14 GB & Desktop + LAN-API Kombination \\
        text-generation-webui & File-loader \& Vector-RAG & Office via Add-on; OCR via pytesseract & REST / WebSocket & 12--14 GB & Umfangreiche Plug-in-Architektur \\
        LocalGPT 2.0 & CLI/UI mit Seiten-Zitaten & PDF / TXT nativ; Office vor Konvertierung & FastAPI (Port 5111) & 4--6 GB & Ressourcenschonend, datenschutzfokussiert \\
        Onyx & Integriertes RAG mit Connectoren, optionale Zitationen & PDF, DOCX; OCR optional & OpenAI-kompatible Endpunkte & 8--12 GB & Gut für On-Premises-Einsatz und flexible Integrationen \\
        \bottomrule
    \end{tabular}
    \caption{Vergleich ausgewählter Open-Source RAG-Tools}
    \label{tab:rag-tools}
\end{table}

Wie aus der Tabelle ersichtlich, bieten verschiedene Tools unterschiedliche Stärken und Schwächen.
GPT-4All Desktop punktet mit einer benutzerfreundlichen GUI und integriertem RAG-Support, während Open WebUI durch seine moderne Web-Oberfläche und REST-API hervorstecht.
LM Studio kombiniert Desktop-Chat mit LAN-API-Funktionalität, was für bestimmte Anwendungsfälle vorteilhaft sein kann.
Text-generation-webui besticht durch seine Plug-in-Architektur, die vielfältige Erweiterungen ermöglicht.
LocalGPT 2.0 ist besonders ressourcenschonend und legt den Fokus auf Datenschutz.
Onyx wiederum ist ein vollumfängliches RAG-Tool, inklusive Konnektoren, Zitationsfunktionen und einer klar strukturierten Weboberfläche.

Um eine gewichtete Entscheidung zu treffen, wurden die Tools hinsichtlich ihrer Funktionalität, Anpassungsfähigkeit und Benutzerfreundlichkeit sowie weiterer Kriterien bewertet.
Die ausführliche Begründung der einzelnen Bewertungskriterien wurde bereits dargestellt; sie wird hier der Vollständigkeit halber nochmals aufgeführt, da sie die Problemstellung direkt adressiert:

\begin{itemize}
    \item \textbf{Funktionalität:} Dieses Kriterium bewertet, wie gut ein Tool die Prinzipien von Retrieval-Augmented Generation (RAG) unterstützt und heterogene Dokumentformate verarbeiten kann. RAG-Systeme binden externe Datenquellen (z. B. PDF-Dokumente, Datenbanken oder Websites) in den LLM-Kontext ein, um Antworten faktisch zu untermauern und aktuell zu halten. Dafür muss ein Tool in der Lage sein, gängige Unternehmensdokumente – etwa Office-Dateien wie Word, Excel oder PowerPoint – zu importieren und zu durchsuchen. Tatsächlich bieten fortgeschrittene Open-Source-Frameworks Konverter für eine Vielzahl von Formaten an (PDF, DOCX, PPTX, XLSX u. a.), sodass sämtliche relevanten Informationen aus Berichten, Tabellen oder Präsentationen in einen einheitlichen Dokumentenindex überführt werden können. Fehlt eine solche Funktionalität, blieben wichtige Wissensquellen unerschlossen, was die Qualität der Antworten und den Nutzen der RAG-Architektur erheblich schmälern würde.
    \item \textbf{Anpassungsfähigkeit:} Hierunter fällt die Flexibilität des Tools, sich an spezifische Anforderungen anzupassen – etwa durch modulare Erweiterungen, Plug-ins oder API-Integrationen. In einer wissenschaftlichen Betrachtung von LLM-Architekturen wird Customizability als wichtiges Qualitätsmerkmal genannt: Die Möglichkeit also, domänenspezifische Anpassungen vorzunehmen, ohne die Grundstruktur zu verändern. Dies kann durch eigene Prompt-Vorlagen, das Feintuning von Modellen oder das Einbinden zusätzlicher Komponenten geschehen. Gleichzeitig ist Interoperabilität gefordert – also eine nahtlose Einbindung des Tools in bestehende Datenbanken, Softwareumgebungen und Workflows. Nur eine offene, erweiterbare Architektur verhindert Integrationshürden im praktischen Einsatz. Ein Beispiel ist das Framework LangChain, das durch seinen modularen Aufbau die Verknüpfung verschiedener Vektor-Datenbanken, externer LLM-APIs und Datenquellen erlaubt und so eine hohe Anpassungsfähigkeit demonstriert. Insgesamt stellt Anpassungsfähigkeit sicher, dass der Chatbot präzise auf die jeweilige Organisationsumgebung zugeschnitten werden kann, anstatt umgekehrt die Prozesse an die Limitierungen des Tools anpassen zu müssen.
    \item \textbf{Benutzerfreundlichkeit:} Die Usability eines RAG-gestützten Chatbot-Systems ist sowohl für Entwickler (bei Einrichtung und Wartung) als auch für Endnutzer (bei der täglichen Interaktion) entscheidend. Eine intuitive Bedienung und klare Nutzeroberfläche erhöhen die Akzeptanz und reduzieren Fehler bei der Anwendung. Empirische Leitlinien betonen, dass die Benutzerschnittstelle möglichst nahtlos und intuitiv gestaltet sein sollte, sodass die komplexe KI-Logik nach außen hin verborgen bleibt. Wichtig ist auch, Mechanismen für Nutzer-Feedback und Korrekturmöglichkeiten bereitzustellen, damit Anwender die Kontrolle über automatisierte Antworten behalten. Aus Entwicklersicht fördert eine gute Dokumentation und ein konsistentes API-Design die Benutzerfreundlichkeit, da Integrationen schneller umgesetzt werden können. Ein praktisches Beispiel für gelungene Usability ist die Integration eines Code-Assistenten direkt in die Entwicklungsumgebung (analog zu GitHub Copilot), was zeigt, wie eine KI-Anwendung durch Einbettung in bestehende Tools die Benutzerfreundlichkeit und Produktivität steigert. Insgesamt gilt: Je leichter ein System zu bedienen ist, desto eher wird es im Alltag akzeptiert und korrekt genutzt.
    \item \textbf{Ressourcenbedarf:} RAG-Anwendungen kombinieren rechenintensive Operationen – wie das Einbetten von Dokumenten in Vektoren und das Generieren von Antworten durch große Sprachmodelle – mit potenziell umfangreichen Datenbeständen. Daher ist der typische Ressourcenbedarf (Arbeitsspeicher, Prozessor/GPU-Auslastung, Speicherplatz) ein wichtiges Kriterium. Besonders in On-Premises-Umgebungen sind die verfügbaren Hardware-Ressourcen beschränkt, sodass effiziente Algorithmen und eine sparsame Nutzung von Speicher entscheidend sind. In der Literatur wird betont, dass optimierte Architekturen (etwa durch Caching von Teilergebnissen, Parallelisierung von Aufgaben und Hardware-Beschleunigung) notwendig sind, um trotz großer Datenmengen niedrige Latenzen und eine effiziente Ressourcenauslastung sicherzustellen. Ein gut bewertetes Tool sollte also auch bei wachsender Anzahl von Dokumenten und Anfragen skalieren, ohne unverhältnismäßig mehr Speicher oder Rechenzeit zu beanspruchen. Dies beinhaltet z. B. einen effizienten Vektorsuch-Index und ggf. Mechanismen, unwichtige Daten früh auszufiltern, damit die teuren LLM-Abfragen nur auf wirklich relevante Kontexte angewendet werden. Ein geringer Ressourcenverbrauch ist nicht nur ökonomisch vorteilhaft, sondern oft die Voraussetzung, um die Software überhaupt auf bestehender Firmen-Hardware betreiben zu können.
    \item \textbf{Datenschutzkonformität:} Für den Einsatz in einer On-Premises-Umgebung – insbesondere bei sensiblen Firmendaten – muss das Tool strenge Datenschutz- und Compliance-Anforderungen erfüllen. Es darf keine vertraulichen Informationen unkontrolliert nach außen gelangen und muss mit geltenden Datenschutzgesetzen (wie der DSGVO) im Einklang stehen. Security-by-Design ist hier zentral: So sollten Mechanismen wie Eingabe- und Ausgabevalidierung, Verschlüsselung der gespeicherten Daten sowie abgesicherte Kommunikationskanäle Standard sein, um Risiken wie unbefugten Zugriff oder Datenlecks vorzubeugen. Zudem warnen Sicherheitsexperten vor spezifischen LLM-Gefahren wie z. B. Prompt Injection – hier muss das System Vorkehrungen treffen, etwa durch Filter für Benutzerinputs und strikte Rollenbeschränkungen, damit solche Angriffe ins Leere laufen. Ein datenschutzkonformes RAG-Tool sollte möglichst so konzipiert sein, dass sämtliche Verarbeitung lokal erfolgt. Techniken wie föderiertes Lernen zeigen Wege auf, Modelle mit dezentralen Daten zu nutzen, sodass persönliche Informationen die geschützte Umgebung gar nicht verlassen. Die Einhaltung von Datenschutz und IT-Compliance ist somit kein optionaler Luxus, sondern Voraussetzung für den produktiven Einsatz eines solchen Systems in vielen Branchen (z. B. Gesundheitswesen, Finanzsektor oder öffentlicher Dienst).
    \item \textbf{Community und Support:} Die Vitalität der Entwickler-Community und die Verfügbarkeit von Support sind wesentliche Erfolgfaktoren bei Open-Source-Software. Ein Blick auf Community-Metriken – etwa Anzahl der GitHub-Sterne, Häufigkeit von Commits, aktive Foren oder Discord/Slack-Kanäle – gibt Aufschluss darüber, wie engagiert und groß die Nutzer- und Entwicklerbasis eines Tools ist. Eine aktive Community bedeutet in der Regel: schnelle Behebung von Fehlern, regelmäßige Updates und viele verfügbare Erweiterungen oder Tutorials. In wissenschaftlichen Entscheidungskriterien wird daher die “Community Health” eines Frameworks explizit als Kriterium genannt. Auch die Qualität der Dokumentation spielt hier mit hinein – ein gut dokumentiertes Tool (ggf. mit praxisnahen Beispielen und Tutorials) erleichtert neuen Nutzern den Einstieg und reduziert den Entwicklungsaufwand. Kommerzieller Support (etwa in Form von Enterprise-SLAs oder Hilfe durch die Firmen hinter einem Open-Source-Projekt) kann ebenfalls relevant sein, insbesondere wenn das System geschäftskritisch eingesetzt wird. Letztlich erhöht eine starke Community & Support-Struktur die Nachhaltigkeit des Tools: Es kann langfristig gepflegt und bei Bedarf an neue Anforderungen angepasst werden, ohne dass man als einzelner Nutzer auf sich allein gestellt ist.
    \item \textbf{Integration in bestehende Systeme:} Ein KI-Assistenzsystem entfaltet nur dann vollen Nutzen, wenn es sich nahtlos in die bestehende IT-Infrastruktur eines Unternehmens einfügt. Daher wurde bewertet, wie gut das Tool in vorhandene Systeme integrierbar ist. Dies umfasst die Fähigkeit, an bestehende Datenquellen (Datenbanken, Dateiablagen, Intranet-Seiten etc.) anzudocken, vorhandene Authentifizierungs- und Autorisierungssysteme (z. B. Single Sign-On) zu nutzen sowie Ausgaben an andere Anwendungen weiterzureichen. Laut aktueller Literatur sind Interoperabilität und Anschlussfähigkeit kritische Erfolgsfaktoren: Eine Architektur, die LLM-Funktionen über standardisierte Schnittstellen (z. B. REST APIs, SDKs) bereitstellt, vermeidet Adoptionsbarrieren und erleichtert die Einbettung in bestehende Arbeitsabläufe. Beispielsweise kann ein gutes RAG-Tool als Microservice laufen, der über ein API abgefragt wird, wodurch Frontend-Anwendungen (wie ein Chat-Frontend oder ein Firmenportal) unkompliziert angebunden werden können. Je reibungsloser die Integration, desto geringer der Aufwand bei der Einführung – insbesondere verglichen mit Insellösungen, die separate Infrastruktur erfordern. In der Bewertung wurde also positiv gewertet, wenn ein Tool bekannte Protokolle und Formate unterstützt (z. B. Output als JSON, Konnektoren für SharePoint/Dateiserver etc.), da dies den Implementierungsaufwand reduziert.
    \item \textbf{Zukunftssicherheit:} Angesichts der dynamischen Entwicklungen im Bereich Large Language Models ist die Zukunftssicherheit einer Lösung ein wesentliches Kriterium. Damit ist gemeint, ob das Tool voraussichtlich mit zukünftigen Anforderungen und technischen Fortschritten Schritt halten kann. Der Markt für RAG-Frameworks wächst derzeit mit ~44\% jährlich, was verdeutlicht, dass ständig neue Modelle, Vektor-Datenbanken oder Optimierungsmethoden erscheinen. Ein zukunftssicheres Tool zeichnet sich dadurch aus, dass es regelmäßig Updates und neue Features erhält und eine klare Roadmap besitzt. Im Open-Source-Kontext bedeutet das oft: Eine große Entwicklergemeinschaft oder ein unterstützendes Unternehmen sorgt fortlaufend für Weiterentwicklung. So integriert z. B. ein aktives Projekt kurz nach Erscheinen neuer vielversprechender Modelle (wie GPT-4 oder Llama 2) deren Unterstützung, während ein weniger gepflegtes Tool irgendwann stagniert. Zudem sollte die Architektur flexibel genug sein, um neue Module (etwa bessere Embeddings-Modelle oder veränderte Prompting-Techniken) aufzunehmen, ohne von Grund auf neu entwickelt werden zu müssen. Zukunftssicherheit umfasst auch die Frage der langfristigen Wartbarkeit: Ein klar strukturiertes, modularisiertes System kann eher über Jahre erhalten und verbessert werden als ein monolithisches. Dieses Kriterium schützt die Investition, indem es wahrscheinlich macht, dass das gewählte Tool auch in einigen Jahren noch State-of-the-Art-Anforderungen erfüllen kann, anstatt veraltet zu sein.
    \item \textbf{Kosten:} Open-Source-Tools sind zwar lizenzkostenfrei verfügbar, doch im professionellen Einsatz entstehen Kosten auf verschiedenen Ebenen. In der Analyse wurde daher betrachtet, welche Aufwände der Betrieb des jeweiligen Tools verursacht – und wie diese im Vergleich zu kommerziellen Alternativen stehen. Ein Vorteil vieler Open-Source-RAG-Frameworks ist die fehlende Lizenzgebühr: Typischerweise stehen sie unter permissiven Lizenzen (z. B. Apache 2.0 oder MIT), was eine kommerzielle Nutzung ohne direkte Softwarekosten erlaubt. Allerdings dürfen die Infrastrukturkosten nicht übersehen werden: Die Notwendigkeit eigener Server (oder Cloud-Ressourcen), Speicher für den Vektorspeicher sowie ausreichend GPU-Leistung für Inferenz kann erheblich zu Buche schlagen. Beispielsweise können beim Betrieb eines Vektorindex in der Cloud Gebühren von etwa \$0.05–\$0.12 pro einer Million Vektoren anfallen. Kommerzielle Lösungen wiederum verlagern diese Kosten oft in Nutzungsentgelte (etwa pro 1000 API-Calls), was abhängig vom Anfragevolumen teuer werden kann. Auch indirekte Kosten wie der Aufwand für Wartung und Updates (bei Open-Source intern zu stemmen, bei kommerziellen Produkten teils im Preis inbegriffen) wurden in die Bewertung einbezogen. Insgesamt musste das Kosten-Nutzen-Verhältnis stimmen: Ein Open-Source-Tool rechtfertigt sich dann, wenn die eingesparten Lizenzkosten die betrieblichen Aufwendungen ausgleichen und es gegenüber einer kommerziellen Lösung langfristig wirtschaftlich bleibt. Andernfalls könnte eine kommerzielle All-in-one-Lösung trotz Lizenzgebühr günstiger sein, etwa wenn sie dafür niedrigere Infrastruktur- oder Personalkosten verursacht.
    \item \textbf{Skalierbarkeit:} Dieses Kriterium untersucht, wie gut das System mit wachsenden Anforderungen mitwächst. In einem Enterprise-Szenario können sowohl die Anzahl der Nutzeranfragen (z. B. parallel arbeitende Benutzer oder Chatbot-Sessions) als auch das Volumen der zu durchsuchenden Dokumente stark ansteigen. Ein skalierbares Tool sollte durch geeignete Architektur darauf vorbereitet sein – etwa indem es Lastverteilung über mehrere Knoten unterstützt oder effizient mit großen Datenbanken interagiert. Forschungsergebnisse zeigen, dass im LLM-Kontext horizontale Skalierung (z. B. das Clustern von Modellen und Datenbanken) nötig ist, um sehr hohe Anfragevolumina zu bewältigen; OpenAI etwa nutzt verteilte Systeme, um Millionen von Requests pro Tag mit GPT-4 verarbeiten zu können. Ähnliches gilt für den Vektor-Index: Bei Millionen von Dokumenten muss die Einfügung neuer Vektoren und die Ähnlichkeitssuche immer noch performant bleiben. Das Kriterium Skalierbarkeit bewertet demnach positiv, wenn ein Tool bereits für Enterprise-Größenordnungen ausgelegt ist – z. B. durch Unterstützung von Kubernetes/Docker-Deployment, Anbindung an skalierbare Cloud-Dienste (wie verteilte Vektordatenbanken) oder bewährte Optimierungen für große Datenmengen. Ein skalierbares System kann mit den Anforderungen wachsen, ohne dass ein kompletter Plattformwechsel nötig wird, sobald mehr Benutzer oder Daten hinzukommen.
    \item \textbf{Performance:} Eng verknüpft mit Skalierbarkeit ist die Performance (Leistungsfähigkeit) des Systems, insbesondere die Antwortzeit auf Nutzeranfragen. Gerade bei großen Dokumentenbeständen ist es wichtig, dass der Chatbot dennoch in Echtzeit oder zumindest wenigen Sekunden sinnvolle Antworten liefert. Nutzerakzeptanzstudien legen nahe, dass zu lange Wartezeiten die Gebrauchstauglichkeit erheblich mindern – daher sind performante Antwortzeiten ein Muss. Technisch wird dies durch optimierte Pipeline-Schritte erreicht: Eine Quelle betont den Einsatz von Caching, Parallelisierung und Hardware-Beschleunigung, um niedrige Latenzen sicherzustellen. Zudem muss die Vektor-Suche in großen Indizes effizient sein; ein häufiger Stolperstein ist es, die Latenz von Vektor-Datenbanken bei sehr vielen Dokumenten zu unterschätzen. Die Bewertung berücksichtigte daher, ob das Tool bekannte Performance-Techniken implementiert (z. B. Approximate Nearest Neighbor Search, Index-Kompression, Vorberechnung von Embeddings) und wie es in Tests auf große Datenmengen reagiert. Ein performantes System liefert konsistente Antwortzeiten, was für die Benutzerzufriedenheit entscheidend ist – insbesondere, wenn der Chatbot produktiv von vielen Nutzern gleichzeitig befragt wird. Schließlich spiegelt Performance auch die Effizienz der KI-Antwortgenerierung wider: Durch cleveres Prompt-Design (etwa Begrenzung des eingefügten Kontexts auf hochrelevante Ausschnitte) kann die Generationszeit und -qualität ebenfalls optimiert werden.
    \item \textbf{Sicherheitsfunktionen:} Neben Datenschutz im engeren Sinne (s. o.) sind weitere Sicherheitsaspekte zu beachten, insbesondere Zugriffs- und Betriebssicherheit. Ein robustes Tool bietet eingebaute Mechanismen, um sensible Daten zu schützen und unbefugten Zugriff zu verhindern. Dazu zählt zunächst ein Rollen- und Rechtesystem (Benutzerverwaltung), das sicherstellt, dass nur autorisierte Personen bestimmte Informationen abfragen können – etwa umgesetzt via Role-Based Access Control (RBAC) in einigen Enterprise-RAG-Frameworks. Weiterhin ist eine sichere Authentifizierung und Autorisierung beim Zugriff auf den Chatbot oder dessen Admin-Oberfläche essenziell. Moderne Implementierungen integrieren häufig OAuth2 oder JWT-basierte Authentifizierung, um den Zugang abzusichern. Darüber hinaus sollten Protokollierung und Intrusion Detection vorhanden sein, um verdächtige Aktivitäten zu erkennen. Schutz gegen spezifische LLM-Angriffe (wie bereits erwähnt Prompt Injection oder auch Data Poisoning des Dokumentenbestands) wird in neueren Architekturempfehlungen hervorgehoben – z. B. diskutiert OWASP entsprechende Sicherheitsrichtlinien für LLMs. Ein gutes RAG-Tool wird deshalb mit Guardrails geliefert oder ist kompatibel mit Zusatztools, die z. B. die Einhaltung bestimmter Antwort-Richtlinien erzwingen und Ausgaben filtern. Die Sicherheitsfunktionen eines solchen Systems sind insgesamt auf mehreren Ebenen verortet: von der Netzwerk-/Infrastruktur-Sicherheit (Verschlüsselung, Firewalling) über Anwendungssicherheit (Eingabefilter, Zugriffskontrollen) bis hin zur ML-spezifischen Sicherheit (Schutz vor Manipulation der KI). Ein hochbewertetes Tool deckt diese Bereiche zumindest dem Grunde nach ab oder lässt sich leicht mit bestehenden Sicherheitslösungen integrieren.
    \item \textbf{Flexibilität bei der Modellauswahl:} Dieses Kriterium betrachtet, inwieweit verschiedene Large-Language-Models in das System eingebunden werden können. Da sich Anforderungen und Rahmenbedingungen ändern können – etwa bezüglich Kosten, Performance oder Lizenzierung –, ist es vorteilhaft, wenn ein RAG-Tool nicht auf ein bestimmtes KI-Modell festgeschrieben ist. Modellagnostische Frameworks ermöglichen es, wahlweise Open-Source-Modelle oder kommerzielle APIs anzuschließen. In der Praxis bedeutet das z. B., dass ein Unternehmen zunächst auf lokale Modelle (wie Llama 2) setzt, bei steigendem Anspruch aber ohne großen Aufwand zu einem stärkeren Cloud-Modell wechseln kann. Viele der aktuellen Open-Source-RAG-Frameworks werben explizit mit dieser Flexibilität: Sie integrieren sich gleichermaßen mit APIs von OpenAI, Cohere, Anthropic und selbstgehosteten Modellen, sodass stets das am besten geeignete Modell verwendet werden kann. Diese Flexibilität in der Modellauswahl erlaubt auch Hybridansätze – etwa leichte Anfragen an ein günstigeres Modell zu schicken, komplexe aber an ein hochqualitatives Modell. Im wissenschaftlichen bzw. industriellen Kontext reduziert eine solche Offenheit zudem die Gefahr von Vendor Lock-in: Man bleibt nicht an einen einzelnen Anbieter gebunden, sondern kann die KI-Komponente austauschen, falls ein alternatives Modell bessere Ergebnisse liefert oder regulatorisch bevorzugt ist. Bei der Bewertung der Tools floss positiv ein, wenn Schnittstellen zu mehreren Modelltypen vorhanden waren (z. B. HuggingFace-Integration, Unterstützung von Modell-Hubs oder Multi-Backend-Support). Damit ist das System zukunfts- und anpassungsfähig gegenüber der schnellen Evolution im LLM-Bereich.
    \item \textbf{Einfache Wartung:} Die Wartungsfreundlichkeit eines Systems bestimmt, wie aufwendig es ist, den Betrieb über längere Zeit aufrechtzuerhalten und Änderungen vorzunehmen. In produktiven Umgebungen ändern sich Anforderungen: neue Dokumente müssen hinzugefügt, veraltete entfernt, Sicherheitsupdates eingespielt oder die KI-Modelle verbessert werden. Ein wartungsarmes Tool zeichnet sich zunächst durch eine klare Struktur und Modularität aus. Wenn einzelne Komponenten (Datenbank, Vektorsuche, LLM-Modul) getrennt und austauschbar sind, können Updates oder Fehlerbehebungen gezielt dort ansetzen, ohne das Gesamtsystem zu gefährden. Die Literatur zu LLM-Architekturen betont, dass Modularität Updates vereinfacht und die Skalierbarkeit erhöht, indem Funktionen in wiederverwendbare Dienste aufgeteilt werden. So kann z. B. ein Wechsel des Embedding-Modells oder ein Upgrade auf einen neuen Vektorindex erfolgen, indem nur die betreffende Komponente ersetzt wird, während der Rest der Pipeline unberührt bleibt. Auch automatische Tests und Monitoring (siehe nächster Punkt) tragen zur Wartungsfreundlichkeit bei, da Probleme früh erkannt und lokalisiert werden können. Schließlich spielt die Qualität der Dokumentation wieder eine Rolle: Gut gewartete Projekte haben häufig ausführliche Release Notes, Migrationsanleitungen und aktive Communities, die bei Problemen helfen. Ein einfach wartbares System minimiert die Total Cost of Ownership, da weniger manuelle Eingriffe und geringere Ausfallzeiten anfallen. In der gewichteten Entscheidung wurde also positiv berücksichtigt, wenn ein Tool als ausgereift und unkompliziert im Betrieb beschrieben wurde – etwa durch Erwähnung von Docker/Kubernetes-Support, Logging-Funktionen für den Betrieb oder „One-click“-Upgradeprozessen.
    \item \textbf{Benutzerverwaltung:} In Mehrnutzerumgebungen braucht es Mechanismen, um verschiedene Benutzer und Rechte zu verwalten. Ein professionelles RAG-System sollte daher eine Benutzer- und Rollenverwaltung mitbringen. Dies ermöglicht es, feingranular zu steuern, welcher Nutzer welche Aktionen durchführen darf und auf welche Dokumente er zugreifen kann. Beispielsweise könnte man definieren, dass nur Mitarbeiter der Rechtsabteilung auf juristische Dokumente zugreifen dürfen, während andere Nutzer davon ausgeschlossen sind. Solche Features sind nicht nur aus Sicherheitsgründen relevant, sondern oft auch aus rechtlichen (Stichwort: Need-to-know-Prinzip bei vertraulichen Informationen). Einige Open-Source-Tools haben bereits integrierte Lösungen: Kernel Memory (KM) etwa implementiert eine rollenbasierte Zugriffskontrolle, um den Zugriff auf Daten konform mit Unternehmensrichtlinien zu regeln. Dabei werden Datenzugriffe protokolliert und technisch erzwingt das System, dass nur authentifizierte Nutzer mit gültigen Rechten Antworten auf bestimmte Inhalte erhalten. Neben der Zugriffskontrolle gehört zur Benutzerverwaltung oft auch die Integration mit bestehenden Verzeichnisdiensten (LDAP, Active Directory) – so etwas wurde in der Bewertung ebenfalls positiv gesehen, da Unternehmen in der Regel ein zentrales Identity Management bevorzugen. Insgesamt gewährleistet eine robuste Benutzerverwaltung, dass der Einsatz des Chatbot-Systems skalierbar bleibt: Auch hunderte Benutzer lassen sich übersichtlich managen, und das System erfüllt Anforderungen an Auditierbarkeit (Nachvollziehbarkeit, wer was angefragt hat) und Compliance (Einhaltung von Berechtigungsstrukturen).
    \item \textbf{Dokumentenmanagement:} Ein RAG-System ist nur so gut wie die Wissensbasis, auf die es zugreift. Daher war ein weiterer Bewertungsaspekt, wie gut das Tool das Management der eingebundenen Dokumente unterstützt. In Unternehmenskontexten ändern sich Dokumentbestände ständig – es kommen neue Dateien hinzu, bestehende werden aktualisiert, manche Informationen veralten und müssen ersetzt oder entfernt werden. Ein effektives Dokumentenmanagement innerhalb des RAG-Tools ermöglicht es, solche Änderungen einfach durchzuführen. Idealerweise verfügt das System über Funktionen zum Hinzufügen, Aktualisieren und Löschen von Dokumenten und indexiert neue Daten automatisch. Dadurch bleibt der Knowledge-Base-Inhalt stets aktuell. Die Wichtigkeit dessen zeigt sich darin, dass RAG-Systeme ihre Stärke aus aktueller Evidenz ziehen – wird ein neues Dokument eingebunden, steht dessen Wissen sofort für die Antworten bereit. Umgekehrt würden ohne gutes Dokumentenmanagement die Antworten mit der Zeit veralten oder es müsste erheblicher manueller Aufwand betrieben werden, um die Wissensbasis aktuell zu halten. Bewertet wurde also z. B., ob ein Tool eine grafische Oberfläche oder API für Dokumentenimporte bietet, wie es mit Dubletten und Versionierung umgeht und ob Metadaten (wie Dokumenttyp, Erstellungsdatum, Zugriffsrechte) verwaltet werden können. Auch die Unterstützung einer Vielzahl von Dokumentformaten (siehe Funktionalität) spielt hier hinein – denn ein breites Spektrum an Formatkonvertern vereinfacht das Einpflegen neuer Quellen. Insgesamt erhöht ein gutes Dokumentenmanagement die Zuverlässigkeit der RAG-Lösung, da die Antworten stets auf dem aktuellen, kuratierten Wissensstand basieren.
    \item \textbf{Mehrsprachigkeit:} In einem mehrsprachigen Land wie Deutschland – und generell in international agierenden Unternehmen – ist die Fähigkeit des Tools, verschiedene Sprachen zu verstehen und zu verarbeiten, von großer Bedeutung. Konkret bedeutet Mehrsprachigkeit, dass der Chatbot sowohl Anfragen auf Deutsch (bzw. anderen Sprachen) korrekt interpretieren kann, als auch Dokumente in verschiedenen Sprachen durchsuchen und als Kontext einbeziehen kann. Einige RAG-Frameworks sind primär für Englisch optimiert, was dazu führen kann, dass die Leistung bei deutschsprachigen Eingaben geringer ist. Studien zeigen beispielsweise, dass mehrsprachige Embedding-Modelle auf Nicht-Englisch oft geringere Genauigkeit erreichen – für Deutsch wurde dies explizit beobachtet. Ein weiteres bekanntes Phänomen ist der sogenannte Output Language Drift in mehrsprachigen RAG-Systemen: Wenn z. B. die Nutzerfrage auf Deutsch gestellt wird, aber hauptsächlich englische Dokumente im Index liegen, neigen manche Modelle dazu, die Antwort unerwünschterweise auf Englisch zu geben. Solche Inkonsistenzen gilt es zu vermeiden. Ein gutes Tool unterstützt daher multilinguale Pipelines, etwa indem es mehrsprachige Vektordatenbanken nutzt oder on-the-fly-Übersetzungen von Fragen und Kontexten ermöglicht. Wichtig ist zudem, dass das zugrundeliegende LLM entweder mehrsprachig trainiert ist oder austauschbar (siehe Modellauswahl) – damit man ein deutschkompetentes Modell verwenden kann. In der Bewertung wirkten sich vorhandene Features wie Language Detection, separate Indexe je Sprache oder die Erwähnung der Unterstützung von Deutsch positiv aus. Mehrsprachigkeit ist letztlich ein Muss, wenn der Chatbot in einem Umfeld eingesetzt wird, in dem nicht alle Informationen und Fragen in einer Sprache vorliegen; sie trägt wesentlich zur Universalität und Nutzbarkeit des Systems bei.
    \item \textbf{Offline-Fähigkeit:} Dieses Kriterium prüft, ob das Tool vollständig ohne Internetverbindung betrieben werden kann. Für viele Unternehmen – gerade mit hohen Sicherheitsanforderungen – ist es obligatorisch, dass keine Daten an externe Server geschickt werden und die gesamte Verarbeitung on-premises erfolgt. Eine ausgeprägte Offline-Fähigkeit bedeutet, dass sämtliche Komponenten (vom Vektorenspeicher bis zum LLM) lokal installiert werden können und keine Abhängigkeiten von Cloud-Diensten bestehen. Einige Anbieter von LLM-Technologie haben erkannt, dass Kunden aus dem Bankensektor, dem Gesundheitswesen oder dem behördlichen Umfeld solche Anforderungen haben, und bieten daher on-premise-Optionen an. Cohere beispielsweise ermöglicht den Betrieb seiner Modelle in der eigenen Infrastruktur oder privaten Cloud, um datenschutzkonforme Lösungen umzusetzen. In unserem Kontext (Auswahl eines Open-Source-RAG-Tools) wurde positiv bewertet, wenn das System standardmäßig offline läuft oder zumindest die Option bietet, statt einer externen API ein lokales Modell einzubinden. Außerdem wichtig: Das Tool sollte ohne ständige Internetzugriffe auskommen – etwa sollten etwaige Telemetrie-Funktionen abschaltbar sein. Ein weiterer Aspekt ist die Aktualisierbarkeit in Offline-Umgebungen (Stichwort Updates per Offline-Pakete). Die Offline-Fähigkeit trägt nicht nur zum Datenschutz bei, sondern erhöht auch die Ausfallsicherheit: Das System ist unabhängig von Cloud-Ausfällen oder -Latenzen. Letztlich ermöglicht ein vollständig offline betreibbares Tool den Einsatz in isolierten Netzwerken und erfüllt höchste Ansprüche an die Kontrolle über die Daten.
    \item \textbf{Anpassbare Antwortformate:} Je nach Anwendungsszenario kann es erforderlich sein, dass der Chatbot Antworten in einem bestimmten Format oder Stil liefert. In einer Wissensdatenbank könnten z. B. Markdown oder HTML-Formatierungen erwartet sein; in anderen Fällen sollen Antworten als JSON-Struktur zurückgegeben werden, damit sie von einem weiteren System weiterverarbeitet werden können. Die Flexibilität bei den Antwortformaten ist daher ein Bewertungspunkt. Von fortgeschrittenen LLM-Integrationen wird erwartet, dass sie entweder durch Prompting-Techniken oder durch Nachverarbeitung die Ausgaben in gewünschter Form ausgeben können. Guardrail-Ansätze für LLMs zeigen, dass man Modelle zwingen kann, sich an Vorgaben (z. B. eine JSON-Schema) zu halten, indem man Regeln im Generierungsprozess durchsetzt. Einige RAG-Frameworks bieten sogar eingebaute Features: So lassen sich beispielsweise bei RAGatouille eigene Prompt-Templates definieren, um den Stil der Antworten zu steuern. Dies kann genutzt werden, um z. B. immer in vollständigen Sätzen zu antworten, Bullet-Point-Listen zu erstellen oder bestimmte Fachterminologie zu verwenden. Die Möglichkeit, Antworten formatmäßig anzupassen, ist besonders in Unternehmenskontexten relevant, wo der Output evtl. in Berichte einfließen oder gewissen Formvorgaben genügen muss. In der gewichteten Entscheidung wurde daher positiv berücksichtigt, wenn das Tool entweder native Unterstützung für strukturierte Ausgabe besaß oder aber bekanntermaßen gut mit entsprechenden Prompt-Techniken harmoniert (etwa durch Unterstützung von Plugins wie Microsoft Guidance oder Guardrails für präzise Ausgabeformate).
    \item \textbf{Logging und Monitoring:} Um einen Chatbot im Unternehmenseinsatz betreiben zu können, sind Überwachungs- und Protokollierungsfunktionen unabdingbar. Logging bedeutet, dass alle Anfragen, die vom Nutzer gestellt werden, sowie die daraufhin abgerufenen Dokumente und generierten Antworten aufgezeichnet werden. Dies ist wichtig aus Debugging-Sicht (bei falschen oder problematischen Antworten muss nachverfolgbar sein, auf welcher Grundlage sie entstanden) und aus Compliance-Sicht (Nachweis, was der Assistent wann ausgegeben hat). Best Practices betonen, dass man ohne Logging im Fehlerfall im Dunkeln tappt – es sollte daher der gesamte RAG-Flow protokolliert werden, inkl. Nutzerprompt, gefundener Kontextdokumente und letztlichem Modell-Output. Monitoring geht noch einen Schritt weiter und umfasst das laufende Überwachen von Systemkennzahlen (Antwortzeiten, Auslastung) sowie der inhaltlichen Qualität. Ein gutes Tool stellt Metriken oder Dashboards bereit, um z. B. die Nutzungshäufigkeit, die Einhaltung von Antwortzeiten oder auftretende Fehler zu verfolgen. In einer Referenzarchitektur für LLM-Systeme werden explizit Monitoring-Komponenten vorgeschlagen, die über alle Schichten verteilt Metriken sammeln und Feedback-Schleifen ermöglichen. Solche Komponenten können etwa die Frequenz bestimmter Warnmeldungen (wie zensierter Inhalte) beobachten oder die Zufriedenheitsbewertung der Nutzer erfassen. Überdies hilft intensives Monitoring, problematische Outputs zu erkennen – beispielsweise könnten Logs ausgewertet werden, um Anzeichen von Bias oder unangemessenen Inhalten zu entdecken. Insgesamt trägt Logging & Monitoring zur kontinuierlichen Verbesserung des Systems bei (durch Auswertung der Protokolle kann man die Retrieval- oder Antwortqualität iterativ optimieren) und ist oft auch aus Revisionsgründen vorgeschrieben. In der Bewertung wurden daher Tools bevorzugt, die von Haus aus Logging-/Monitoring-Funktionen mitbringen oder zumindest einfache Anbindungen an vorhandene Monitoring-Infrastruktur (z. B. Elastic Stack, Prometheus) ermöglichen.
\end{itemize}

% HINWEIS: An dieser Stelle fügst du einfach deinen kompletten bereits vorhandenen Kriterien-Text ein,
% also alle Punkte von Funktionalität, Anpassungsfähigkeit, Benutzerfreundlichkeit, … bis Logging und Monitoring,
% so wie du ihn oben schon stehen hattest. Inhaltlich muss dort nichts geändert werden.

\noindent
Die aufgeführten Kriterien bilden bewusst ein fein granuliertes Raster, das die unterschiedlichen Anforderungen an einen datenschutzkonformen, on-premises betriebenen RAG-Chatbot systematisch abdeckt.
Für die eigentliche Werkzeugauswahl mussten diese Einzelkriterien jedoch in ein kompaktes Bewertungsmodell überführt werden, das eine vergleichende Beurteilung der in Tabelle~\ref{tab:rag-tools} aufgeführten Tools erlaubt.

\subsubsection{Gewichtung der Kriterien und Bewertungsverfahren}

Um eine nachvollziehbare Entscheidung zu ermöglichen, wurden die detaillierten Kriterien zu vier Bewertungsdimensionen zusammengefasst, die den in Abschnitt~\ref{subsec:zielbild} formulierten Qualitätszielen und der praktischen Problemstellung entsprechen:

\begin{itemize}
    \item \textbf{Kernfunktionalität und RAG-Fähigkeiten:} In dieser Dimension bündeln sich insbesondere die Kriterien Funktionalität, Dokumentenmanagement, Mehrsprachigkeit, anpassbare Antwortformate sowie Flexibilität bei der Modellauswahl. Bewertet wird, wie gut ein Tool RAG-typische Pipelines unterstützt, heterogene Dokumente verarbeiten kann, Zitationen bzw.\ Kontextverweise anbietet und inwieweit es fachlich brauchbare, überprüfbare Antworten generiert.
    \item \textbf{Betrieb, Datenschutz und Sicherheit:} Hierzu zählen die Kriterien Datenschutzkonformität, Sicherheitsfunktionen, Benutzerverwaltung, Offline-Fähigkeit und Integration in bestehende Systeme. Diese Dimension spiegelt die Anforderung wider, dass der Chatbot ausschließlich on-premises betrieben, in vorhandene Berechtigungskonzepte eingebettet und revisionssicher überwacht werden können muss.
    \item \textbf{Ressourcenbedarf, Skalierbarkeit und Wartbarkeit:} In dieser Dimension sind insbesondere Ressourcenbedarf, Skalierbarkeit, Performance, einfache Wartung sowie Logging und Monitoring zusammengefasst. Bewertet wird, ob ein Tool auf typischer Unternehmenshardware betrieben werden kann, mit wachsenden Datenmengen und Nutzerzahlen umgehen kann und zugleich im Alltag betreibbar bleibt.
    \item \textbf{Usability, Community und Zukunftssicherheit:} Diese Dimension bündelt Benutzerfreundlichkeit, Community und Support sowie Zukunftssicherheit und Kosten. Sie adressiert die Frage, ob ein System von Endanwenderinnen und -anwendern sowie von der IT-Abteilung akzeptiert wird, ob es voraussichtlich gepflegt und weiterentwickelt wird und ob der Gesamtaufwand im Verhältnis zum Nutzen steht.
\end{itemize}

Die vier Dimensionen wurden gemäß ihrer Relevanz für die Problemstellung gewichtet.
Im Vordergrund stehen die Fähigkeit, einen qualitativ hochwertigen RAG-Chatbot auf Basis interner Dokumente zu realisieren, sowie ein datenschutzkonformer, sicherer Betrieb in einer Windows-dominierten Umgebung.
Entsprechend ergibt sich folgende Gewichtung:

\begin{center}
    \begin{tabular}{ll}
        \toprule
        \textbf{Bewertungsdimension} & \textbf{Gewicht} \\
        \midrule
        Kernfunktionalität und RAG-Fähigkeiten          & 0{,}30 \\
        Betrieb, Datenschutz und Sicherheit             & 0{,}30 \\
        Ressourcenbedarf, Skalierbarkeit, Wartbarkeit   & 0{,}20 \\
        Usability, Community und Zukunftssicherheit     & 0{,}20 \\
        \bottomrule
    \end{tabular}
\end{center}

Die in Tabelle~\ref{tab:rag-tools} aufgeführten Tools wurden entlang dieser vier Dimensionen jeweils auf einer Skala von 1 (sehr geringe Eignung) bis 5 (sehr hohe Eignung) bewertet.
Die Einzelbewertungen basieren auf Dokumentation, eigenen Tests im Rahmen der Arbeit sowie auf Erfahrungsberichten aus der Community.
Aus den Bewertungen wurde anschließend ein gewichteter Gesamtwert berechnet.
Tabelle~\ref{tab:rag-tools-gewichtung} fasst das Ergebnis zusammen.

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Tool} &
        \makecell{\textbf{Kern-}\\\textbf{funktion}} &
        \makecell{\textbf{Betrieb/}\\\textbf{Datenschutz}} &
        \makecell{\textbf{Ressourcen/}\\\textbf{Skalierung}} &
        \makecell{\textbf{Usability/}\\\textbf{Zukunft}} &
        \textbf{Gesamt} \\
        \midrule
        GPT-4All Desktop        & 2{,}5 & 2{,}0 & 2{,}5 & 3{,}0 & 2{,}6 \\
        Open WebUI + Ollama     & 4{,}0 & 3{,}0 & 3{,}0 & 4{,}0 & 3{,}5 \\
        LM Studio               & 3{,}0 & 2{,}5 & 3{,}0 & 3{,}5 & 3{,}0 \\
        text-generation-webui   & 3{,}0 & 2{,}5 & 3{,}0 & 2{,}5 & 2{,}9 \\
        LocalGPT 2.0            & 3{,}0 & 4{,}0 & 3{,}5 & 2{,}5 & 3{,}2 \\
        Onyx                    & 4{,}5 & 4{,}5 & 3{,}5 & 4{,}0 & 4{,}1 \\
        \bottomrule
    \end{tabular}
    \caption{Gewichtete, projektspezifische Bewertung ausgewählter RAG-Tools}
    \label{tab:rag-tools-gewichtung}
\end{table}


Die Zahlenwerte sind nicht als allgemeingültiges Ranking zu verstehen, sondern als projektspezifische Verdichtung der zuvor beschriebenen Kriterien vor dem Hintergrund der konkreten Rahmenbedingungen (On-Premises-Betrieb, Windows-basierte Infrastruktur, Fokus auf SharePoint/OneDrive, begrenzte Hardware-Ressourcen zu Beginn der Arbeit).

\subsubsection{Diskussion der Bewertung und Begründung für Onyx}

Die Tabelle zeigt, dass mehrere Tools grundsätzlich geeignet sind, einen RAG-basierten Chatbot auf internen Dokumenten aufzubauen.
Eine genauere Betrachtung im Lichte der Problemstellung verdeutlicht jedoch, weshalb Onyx im Rahmen dieser Arbeit den höchsten Gesamtwert erreicht und als primäre Plattform ausgewählt wurde.

GPT-4All Desktop erzielt in der Dimension \enquote{Kernfunktionalität} nur mittlere Werte.
Obwohl ein integriertes RAG und Seitenzitationen existieren, ist das Tool primär als Single-User-Desktopanwendung konzipiert.
Es fehlen eine robuste, serverseitige API, Mehrbenutzerfähigkeit, eine granulare Benutzerverwaltung und ein eigenständiges Admin-Interface für das Dokumenten- und Rechtemanagement.
Hinzu kommt die fehlende OCR-Unterstützung für gescannte Dokumente, die in Unternehmensumgebungen häufig vorkommen.
Damit kollidiert GPT-4All insbesondere mit den Kriterien Dokumentenmanagement, Benutzerverwaltung, Integration in bestehende Systeme und Offline-Fähigkeit im produktiven Mehrbenutzerbetrieb.

Open WebUI in Kombination mit Ollama schneidet deutlich besser ab.
In der Dimension \enquote{Kernfunktionalität} werden gute Werte erreicht: RAG-Plugins, Quellenangaben und die Anbindung verschiedenster lokaler Modelle erfüllen zentrale Anforderungen aus den Kriterien Funktionalität, Mehrsprachigkeit und Flexibilität bei der Modellauswahl.
Auch die moderne Weboberfläche adressiert die Benutzerfreundlichkeit.
Gleichzeitig zeigt sich jedoch, dass Open WebUI eher als generische LLM-Oberfläche mit optionalem RAG verstanden werden muss und weniger als vollständig integrierte Wissensplattform.
Insbesondere in den Kriterien Benutzerverwaltung, Sicherheitsfunktionen und Dokumentenmanagement sind die Möglichkeiten begrenzt oder nur mit zusätzlicher Eigenentwicklung zu realisieren.
Für einen produktiven Unternehmensbetrieb müssten daher mehrere zentrale Bausteine (Rollen- und Rechtekonzept, administrative Oberfläche für die Wissensbasis, revisionssicheres Logging) selbst implementiert werden.

LM Studio und text-generation-webui sind ähnlich gelagert:
Beide Tools bieten eine leistungsfähige Lokalisierung und Steuerung von LLMs, inklusive API-Funktionalität und RAG-Ansätzen, konzentrieren sich aber primär auf die Modellinferenz.
Sie erfüllen damit einzelne Kriterien – etwa Flexibilität bei der Modellauswahl, Performance und in gewissem Umfang auch Benutzerfreundlichkeit – sehr gut, adressieren aber zentrale Anforderungen dieser Arbeit nur unvollständig.
Insbesondere robuste Integrationspunkte in Richtung SharePoint/OneDrive, ein durchgängiges Berechtigungsmodell und eine integrierte Administrationsoberfläche für Dokumente und Nutzer fehlen oder müssten extern ergänzt werden.
Der Aufwand, diese Lücken zu schließen, wäre im Rahmen der Arbeit schwer zu rechtfertigen.

LocalGPT 2.0 positioniert sich explizit als datenschutzfreundliche, ressourcenschonende On-Premises-Lösung und erzielt entsprechend hohe Werte in den Dimensionen Datenschutz, Offline-Fähigkeit und Ressourcenbedarf.
Die CLI- bzw.\ UI-orientierte Ausrichtung zeigt jedoch, dass LocalGPT eher als Baustein in einer eigenen Architektur zu verstehen ist, nicht als vollständige Plattform.
Gerade in den Kriterien Benutzerfreundlichkeit (für Endnutzer), Benutzerverwaltung, Dokumentenmanagement und Integration in bestehende Systeme bestehen erhebliche Lücken.
Für die Zielsetzung dieser Arbeit – einen prototypischen, aber realistisch betreibbaren Unternehmens-Chatbot zu entwerfen – wäre LocalGPT damit nur die Grundlage einer umfangreicheren Eigenentwicklung, nicht jedoch eine fertige, integrierte Lösung.

Onyx unterscheidet sich an dieser Stelle deutlich von den übrigen Kandidaten.
Bereits auf Ebene der Kernfunktionalität werden zentrale Kriterien in einer Plattform vereint:
Onyx bietet ein integriertes RAG mit Connectoren zu typischen Unternehmensquellen (einschließlich SharePoint-ähnlicher Dokumentablagen), unterstützt die Verarbeitung gängiger Office- und PDF-Formate und kann wahlweise Zitationen bzw.\ Quellenverweise in den Antworten einblenden.
Dadurch werden Funktionalität, Dokumentenmanagement, anpassbare Antwortformate und Mehrsprachigkeit in einer Weise adressiert, die unmittelbar an den Bedarf der in Abschnitt~\ref{subsec:ist-zustand-und-problemstellung} beschriebenen Nutzergruppen anschließt.

Noch ausschlaggebender ist die Bewertung in der Dimension \enquote{Betrieb, Datenschutz und Sicherheit}.
Onyx ist explizit für On-Premises-Szenarien konzipiert und bietet eine Weboberfläche für Endnutzerinnen und Endnutzer sowie ein separates Admin-Interface.
Dort lassen sich Datenquellen verwalten, Reindizierungen anstoßen, Berechtigungen konfigurieren und grundlegende Betriebsmetriken einsehen.
Die integrierte Benutzer- und Rollenverwaltung ermöglicht es, Zugriffsrechte entlang bestehender Organisationsstrukturen abzubilden und so das Need-to-know-Prinzip technisch zu unterfüttern.
In Verbindung mit der Möglichkeit, sämtliche Komponenten innerhalb der Unternehmensinfrastruktur zu betreiben, adressiert Onyx damit zentrale Kriterien wie Datenschutzkonformität, Sicherheitsfunktionen, Benutzerverwaltung und Integration in bestehende Systeme in einer Weise, die den Anforderungen dieser Masterarbeit besonders nahekommt.

Auch in der Dimension \enquote{Ressourcenbedarf, Skalierbarkeit und Wartbarkeit} zeigt sich ein pragmatisches Bild:
Onyx kann containerisiert betrieben und in eine Docker-Compose-Umgebung integriert werden, was den Einsatz auf vorhandener Unternehmenshardware erleichtert.
Die Möglichkeit, ein externes LLM-Backend (hier: Ollama) anzubinden, erlaubt es zudem, das System schrittweise zu skalieren und bei Bedarf das zugrundeliegende Modell zu wechseln, ohne die restliche Architektur zu verändern.
Damit werden Kriterien wie einfache Wartung, Flexibilität bei der Modellauswahl, Logging und Monitoring sowie Skalierbarkeit adressiert, ohne unmittelbar eine komplexe Microservice-Landschaft einführen zu müssen.

Schließlich erzielt Onyx in der Dimension \enquote{Usability, Community und Zukunftssicherheit} gute Werte.
Die Weboberfläche orientiert sich an etablierten Chat-Interfaces, was die Einstiegshürde für die Mitarbeitenden senkt und die in Abschnitt~\ref{subsec:zielbild} formulierte Anforderung der Benutzerakzeptanz unterstützt.
Gleichzeitig befindet sich Onyx als Open-Source-Projekt in aktiver Weiterentwicklung und bietet durch seine OpenAI-kompatiblen Schnittstellen eine Anschlussfähigkeit an künftige Modellgenerationen.

Zusammenfassend lässt sich festhalten, dass Onyx nicht deshalb ausgewählt wurde, weil andere Tools grundsätzlich ungeeignet wären, sondern weil es im Rahmen der zuvor definierten Kriterien die beste Balance aus Kernfunktionalität, datenschutzkonformem Betrieb, Integrationsfähigkeit und praktischer Wartbarkeit bietet.
Im weiteren Verlauf der Arbeit (vgl.\ Kapitel~\ref{sec:technische-umsetzung}) wird Onyx daher als primäre RAG-Plattform eingesetzt, während Ollama als flexible Laufzeitumgebung für lokale Sprachmodelle dient.
Diese Kombination erlaubt es, die in der Problemanalyse identifizierten Herausforderungen – insbesondere heterogene Datenquellen, Datenschutzanforderungen und die technische Realisierbarkeit auf unternehmensüblicher Hardware – prototypisch, aber realitätsnah zu adressieren.

\subsection{Kommunikationsschnittstellen}\label{subsec:kommunikationsschnittstellen}

Für die Interaktion mit dem RAG-Chatbot kommen grundsätzlich verschiedene Kommunikationsschnittstellen in Frage.
Aus Sicht der Mitarbeitenden ist vor allem wichtig, dass der Zugang niedrigschwellig, plattformunabhängig und ohne zusätzliche lokale Installation möglich ist.
Zudem sollte der Chatbot sich möglichst nahtlos in bestehende Arbeitsabläufe integrieren lassen, etwa in Form eines Web-Frontends im Intranet oder als Bot innerhalb von Kollaborationsplattformen wie Microsoft Teams.

Im Rahmen dieser Arbeit wurde zunächst eine Web-basierte Oberfläche gewählt.
Diese Entscheidung folgt mehreren praktischen und architektonischen Überlegungen:
\begin{itemize}
    \item Ein Web-Interface ist von praktisch allen Endgeräten (Desktop, Laptop, ggf.\ Tablet) aus mit einem aktuellen Browser nutzbar, ohne dass Client-Software ausgerollt werden muss.
    \item Die Präsentationsschicht lässt sich klar von der RAG-Logik im Backend trennen, was eine saubere Schichtung der Architektur unterstützt.
    \item Weitere Kanäle (z.\,B.\ Teams-Bot, REST-API für Fachanwendungen) können später ergänzt werden, ohne die grundlegende Struktur des Systems zu verändern.
\end{itemize}

Die geplante Kommunikation zwischen Nutzer, Web-Frontend und Backend ist in Abbildung~\ref{fig:rag-chatbot-architektur} schematisch dargestellt.

\FloatBarrier
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Bilder/masterarbeit/on-premise-rag-chatbot-uml-architektur-u6uu2d}
    \caption{Geplante Kommunikationsschnittstellen des RAG-Chatbots}
    \label{fig:rag-chatbot-architektur}
\end{figure}
\FloatBarrier

Der Nutzer formuliert seine Anfrage in natürlicher Sprache im Web-Interface.
Diese Anfrage wird an ein Backend gesendet, das die RAG-Pipeline ausführt:
Zunächst wird die Anfrage in ein Embedding überführt und gegen den Vektorindex abgeglichen; aus der Vektordatenbank werden die semantisch ähnlichsten Dokumentenabschnitte abgerufen.
Anschließend kombiniert das Backend die Anfrage mit dem gefundenen Kontext zu einem Prompt für das Sprachmodell und ruft das LLM (hier über Ollama) zur Antwortgenerierung auf.
Die erzeugte Antwort wird schließlich an das Web-Frontend zurückgegeben und dort angezeigt.

\subsubsection{Architekturüberlegungen zu Modularität und Entkopplung}

Die hier eingesetzte Trennung zwischen Web-Interface, Retrieval-Komponenten (Embeddings, Vektorsuche, Dokumentenquellen) und LLM-Inferenz folgt etablierten Prinzipien der Softwarearchitektur.
Parnas betont bereits früh die Bedeutung einer wohldefinierten Zerlegung von Systemen in Module, die intern ihre Implementierungsdetails verbergen (\emph{information hiding}) und nach außen nur wohldefinierte Schnittstellen anbieten~\cite{Parnas1972}.
Ziel ist dabei eine geringe Kopplung zwischen den Modulen bei gleichzeitig hoher Kohäsion innerhalb der einzelnen Bausteine.

Übertragen auf ein RAG-System bedeutet dies konkret, dass die folgenden Verantwortlichkeiten voneinander entkoppelt werden:

\begin{itemize}
    \item das Web-Interface als Präsentations- und Einstiegsschicht für Nutzeranfragen,
    \item das Retrieval- und Embedding-Modul (Konfiguration der Dokumentenquellen, Parsing, Vektorisierung und Vektorsuche),
    \item das Generierungsmodul, also die eigentliche LLM-Inferenz,
    \item optionale Zwischenstufen wie Re-Ranking, Antwort-Postprocessing oder inhaltliche Guardrails.
\end{itemize}

Aktuelle Übersichtsarbeiten zu Retrieval-Augmented Generation weisen darauf hin, dass ein solcher modularer Aufbau gegenüber monolithischen \enquote{retrieve-then-generate}-Systemen klare Vorteile bietet~\cite{Gao2023RAGSurvey}.
Gao et al.\ führen das Konzept in \emph{Modular RAG} weiter und beschreiben RAG-Pipelines explizit als \glqq LEGO-artige\grqq{} Konstruktionen, in denen einzelne Bausteine (Retriever, Reranker, Generator, zusätzliche Tools) gezielt kombiniert, ausgetauscht oder erweitert werden können, ohne das Gesamtsystem neu entwerfen zu müssen~\cite{Gao2024ModularRAG}.
Ähnliche Argumente finden sich in praxisorientierten Beiträgen, die modulare RAG-Architekturen für Unternehmensszenarien skizzieren~\cite{Markov2025ModularRAG,RidgeRun2024OnPremRAG}.

Für die vorliegende Arbeit ergeben sich daraus drei zentrale Vorteile:

\begin{itemize}
    \item \textbf{Austauschbarkeit und Weiterentwicklung:}
    Verbesserungen an der RAG-Pipeline können lokal vorgenommen werden, etwa durch den Austausch des Embedding-Modells oder des LLMs, ohne die übrige Infrastruktur (Web-Interface, Datenquellen, Vektordatenbank) anzupassen.
    Dies ist insbesondere vor dem Hintergrund der schnellen Modellentwicklung bei LLMs und Embeddings wichtig.

    \item \textbf{Gezielte Optimierung:}
    Retrieval-Strategien (z.\,B.\ Wahl von Top-$k$, Hybrid-Suche, Filterregeln) und Generierungsstrategien (z.\,B.\ Temperature, maximale Kontextlänge, Antwortstil) lassen sich getrennt voneinander evaluieren und optimieren.
    Schwächen können so punktgenau adressiert werden, anstatt das System als Ganzes verändern zu müssen.

    \item \textbf{Testbarkeit und Diagnose:}
    Da jede Komponente klar definierte Ein- und Ausgaben besitzt, können Tests auf Modul-Ebene durchgeführt werden.
    Treten fehlerhafte oder unplausible Antworten auf, lässt sich vergleichsweise gut unterscheiden, ob das Problem im Retrieval (irrelevante Chunks), im Prompting oder im LLM-Verhalten liegt.
\end{itemize}

\subsubsection{Rolle des Web-Interfaces als Orchestrierungsschicht}

Das Web-Interface übernimmt in dieser Architektur eine Doppelfunktion:
Zum einen fungiert es als Benutzerschnittstelle, über die Anfragen eingegeben und Antworten dargestellt werden.
Zum anderen spielt es die Rolle einer Orchestrierungsschicht, die die einzelnen Schritte der RAG-Pipeline koordiniert.

Konzeptionell entspricht dies einer Kombination aus \emph{Fassade}- und \emph{Mediator}-Muster:
Das Frontend (bzw.\ das zugehörige Backend im Web-Service) bietet eine einheitliche Schnittstelle nach außen, hinter der die interne Komplexität der RAG-Verarbeitung verborgen bleibt.
Gleichzeitig werden die beteiligten Komponenten (Vektordatenbank, LLM-Laufzeit, ggf.\ zusätzliche Services) nicht direkt miteinander gekoppelt, sondern über eine zentrale Stelle koordiniert.

Aus Sicht moderner Architekturansätze wie Clean Architecture oder Hexagonaler Architektur ist dies konsistent:
Die eigentliche Domänelogik (\enquote{beantworte eine Frage auf Basis interner Dokumente}) bildet den Kern, während technische Details wie konkrete LLM-Engines, Datenbanken oder Connectoren als austauschbare Adapter an standardisierte Ports angebunden werden~\cite{Martin2012CleanArchitecture,Khadapkar2025Hexagonal}.
Die Abhängigkeiten zeigen dabei nach außen: Die Kernlogik hängt nur von Abstraktionen ab, nicht von konkreten Bibliotheken oder Diensten.

Für das hier betrachtete System hat dies mehrere Konsequenzen:

\begin{itemize}
    \item Eine spätere Erweiterung um zusätzliche Frontends (z.\,B.\ ein Teams-Bot oder eine Integration in ein bestehendes Intranet-Portal) kann erfolgen, ohne die RAG-Kernlogik zu verändern.
    \item Die Anbindung neuer Datenquellen (z.\,B.\ weiterer SharePoint-Sites oder Wikis) kann über zusätzliche Connectoren umgesetzt werden, die in die bestehende Pipeline eingehängt werden.
    \item Der Wechsel des LLM-Backends (z.\,B.\ auf ein anderes lokal gehostetes Modell) bleibt möglich, solange die Schnittstelle aus Sicht der Orchestrierungsschicht gleich bleibt.
\end{itemize}

\subsubsection{Skalierbarkeit und Betriebsmodelle: modularer Monolith vs.\ Microservices}

Die beschriebene Modulgrenzen erlauben unterschiedliche Betriebsmodelle:
Zum einen kann das System als sogenannter \emph{modularer Monolith} implementiert werden, bei dem alle Komponenten in einem Deployment laufen, intern aber die beschriebenen Schnittstellen und Schichten strikt eingehalten werden.
Zum anderen ist eine Weiterentwicklung hin zu einer Microservice-Architektur denkbar, in der einzelne Komponenten (z.\,B.\ Embedding-Service, Vektorsuche, LLM-Service) als separate Dienste betrieben werden.

Lewis und Fowler beschreiben Microservices als Ansatz, eine Anwendung in eine Menge kleiner, unabhängiger Dienste zu zerlegen, die jeweils einen abgegrenzten fachlichen Ausschnitt abdecken und über leichtgewichtige Protokolle kommunizieren~\cite{LewisFowler2014Microservices}.
Praxisberichte zu On-Premises-RAG-Systemen zeigen, dass sich insbesondere die LLM-Inferenz und die Vektorsuche gut als eigenständige Services betreiben lassen, da sie eigene Skalierungsprofile besitzen und unterschiedliche Hardwareanforderungen stellen~\cite{RidgeRun2024OnPremRAG}.

Für diese Masterarbeit wurde bewusst ein pragmatischer Ansatz gewählt:
Die Implementierung orientiert sich an einer klar modularisierten Struktur, wird aber in einer gemeinsamen, containerisierten Umgebung (Docker-Compose) betrieben.
Damit werden viele Vorteile einer modularen Architektur (bessere Wartbarkeit, Austauschbarkeit, Testbarkeit) realisiert, ohne den vollen Betriebsaufwand einer verteilten Microservice-Landschaft zu tragen.
Sollte der Einsatz in Zukunft wachsen (mehr Nutzer, größere Dokumentenbasis, strengere Verfügbarkeitsanforderungen), könnten einzelne Komponenten entlang der bereits bestehenden Modulgrenzen in separate Services ausgelagert werden.

\subsubsection{Optionale Erweiterung der Kommunikationskanäle}

Obwohl diese Arbeit den Fokus auf ein Web-Interface legt, sind weitere Kommunikationswege explizit als Teil des Sollbildes vorgesehen:

\begin{itemize}
    \item \textbf{Integration in Microsoft Teams:}
    Ein Bot-Endpunkt könnte dieselbe Backend-Logik nutzen, um Anfragen direkt aus Teams-Chats entgegenzunehmen.
    Damit würde der Chatbot näher an den alltäglichen Kommunikationsfluss der Mitarbeitenden rücken.

    \item \textbf{Programmierschnittstellen (REST/gRPC):}
    Über eine standardisierte API könnten andere Anwendungen den Chatbot nutzen, etwa um Textbausteine zu generieren, Dokumente automatisch zusammenzufassen oder spezifische Auswertungen auf Basis der Wissensbasis anzufordern.

    \item \textbf{Einbettung in bestehende Portale:}
    Das Web-Interface kann in ein Intranet-Portal oder ein Self-Service-Portal eingebettet werden, um den Chatbot als zentrale Wissensschnittstelle des Unternehmens bereitzustellen.
\end{itemize}

Diese Erweiterungen werden im Rahmen der Masterarbeit nicht vollständig umgesetzt, sind jedoch durch die gewählte Architektur vorbereitet:
Da das Web-Interface und das dahinterliegende Backend als zentrale Orchestrierungs- und Domänenschicht fungieren, können zusätzliche Kanäle auf dieselbe fachliche Logik zugreifen, ohne dass die RAG-Pipeline selbst neu gebaut werden müsste.
