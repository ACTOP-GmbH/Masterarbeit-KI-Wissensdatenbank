\pagebreak

\section{Problemanalyse}\label{sec:problemanalyse}

\subsection{Ist-Zustand}\label{subsec:ist-zustand-und-problemstellung}


In modernen Organisationen wächst die Menge an digital verfügbaren Informationen rasant.
%TODO Quelle?
Wissen liegt oft in verschiedenen Dokumenten über beispielsweise SharePoint-Sites, OneDrive-Verzeichnisse oder in Datenbanksystemen verteilt vor.
Mitarbeiter, die Antworten auf spezifische Fragen suchen (etwa Richtlinien, Projektberichte, technische Dokumentationen), müssen derzeit entweder manuelle Suchen durchführen oder sich durch lange Dokumente arbeiten.
Dies ist zeitaufwendig und ineffizient.
Ein intelligenter Chatbot, der Fragen in natürlicher Sprache beantwortet und direkt auf relevante interne Informationen zugreift, könnte die Informationsbeschaffung erheblich erleichtern.

\par
Herkömmliche Chatbots oder Suchfunktionen stoßen hierbei an Grenzen: Ein einfacher Keyword-Suchlauf liefert oft unstrukturierte Trefferlisten, anstatt konkrete Antworten.
Moderne generative KI-Systeme wie GPT-4 besitzen zwar beeindruckende sprachliche Fähigkeiten, haben aber keinen Zugang zu unternehmensspezifischen Daten und Wissen, das nicht in ihrem Trainingsdatensatz enthalten war\href{https://en.wikipedia.org/wiki/Retrieval-augmented_generation#:~:text=Retrieval,responses%20based%20on%20authoritative%20sources}{en.wikipedia.org}.
Zudem wäre ein Einsatz von Cloud-basierten LLM-Diensten in vielen Fällen aus Datenschutz- und Compliance-Sicht problematisch – vertrauliche Unternehmensdaten dürfen nicht unkontrolliert an externe Dienste gesendet werden\href{https://punctuations.ai/ai-agents-workflows/your-private-gpt-the-case-for-secure-on-premise-llms/#:~:text=pharmaceutical%20companies%2C%20manufacturers%2C%20government%20contractors,keeping%20AI%20close%20to%20home}{punctuations.ai}.
In streng regulierten Branchen oder bei sensiblen Informationen (etwa personenbezogene Daten, Geschäftsgeheimnisse) überwiegen die Risiken gegenüber dem Nutzen, wenn man öffentliche KI-APIs nutzt.
Zusätzlich fehlt es generischen LLMs an Aktualität und Domänenwissen: Sie könnten halluzinierende Antworten geben, also sachlich falsche Auskünfte erteilen, wenn die Anfrage sich auf interne Details bezieht\href{https://en.wikipedia.org/wiki/Retrieval-augmented_generation#:~:text=RAG%20improves%20large%20language%20models,4}{en.wikipedia.org}.
Dieses Risiko ist in professionellen Anwendungsfällen nicht akzeptabel.
\par
Ein typischer Nutzer des geplanten Chatbots ist ein Mitarbeiter im Unternehmen, der regelmäßig auf interne Dokumente zugreifen muss, um seine Aufgaben zu erfüllen.
Beispielsweise könnte dies ein Projektmanager sein, der Informationen zu aktuellen Projektrichtlinien, technischen Spezifikationen oder Berichten benötigt.
Meistens ist dies aber ein Anwendungsberater, der Kundenfragen zu Produkten oder Dienstleistungen beantworten muss.
Zu guter Letzt, in der zweiten Prioritätsstufe, sind es die technischen Berater, die intern auf technische Dokumentationen und Anleitungen zugreifen müssen.
Diese Nutzergruppe profitiert besonders von einem Chatbot, der ihnen schnell präzise Antworten liefert, ohne dass sie lange technische Dokumente für Infor LN oder Infor OS durchsuchen müssen.
Diesen Dokumenten fehlt es heutzutage oftmals an Qualität in Bezug auf Struktur, Formatierung und Verständlichkeit, da sie oft aus anderen Sprachen übersetzt wurden oder von verschiedenen Autoren mit unterschiedlichen Stilen verfasst sind.
Die Nutzer erwarten eine einfache, intuitive Bedienung, ähnlich wie bei modernen Chat-Anwendungen.

\subsubsection{User Stories}\label{subsec:user-stories}
Um dies klar hervorzuheben entstehen im folgenden User Stories, die typische Anwendungsfälle und Anforderungen der Nutzer beschreiben:

\begin{itemize}
    \item \textbf{Als Projektmanager} möchte ich schnell Antworten auf Fragen zu internen Richtlinien und Verfahren erhalten, damit ich meine Projekte effizient planen und durchführen kann.
    \item \textbf{Als Anwendungsberater} möchte ich in der Lage sein, Kundenanfragen zu Produkten und Dienstleistungen zügig zu beantworten, indem ich auf relevante interne Dokumente zugreife.
    \item \textbf{Als technischer Berater} möchte ich technische Anleitungen und Dokumentationen schnell finden können, um Supportanfragen effektiv zu bearbeiten.
\end{itemize}



Die zentrale Problemstellung liegt somit darin, einen Chatbot zu konzipieren, der auf firmeninternes Wissen zurückgreift, ohne dieses Wissen nach außen preiszugeben.
Hierbei treten mehrere Herausforderungen auf.
Erstens muss die Integration der diversen Datenquellen bewältigt werden.
SharePoint und OneDrive enthalten Dateien in unterschiedlichen Formaten (PDF, Word, E-Mails, etc.), die automatisiert ausgelesen werden sollen.
Dies erfordert robuste Mechanismen zur Dateiverarbeitung und -indexierung.
Außerdem sind diese Plattformen oft in komplexe Authentifizierungs- und Berechtigungssysteme eingebunden.
So kann es beispielsweise passieren, dass \textit{automatisierte Zugriffe} von Sicherheitsmechanismen blockiert werden – ein Umstand, der in ersten Tests tatsächlich beobachtet wurde\href{https://alain-airom.medium.com/populating-a-rag-with-data-from-enterprise-documents-repositories-for-generative-ai-4ded82952c67#:~:text=W%20riting%20the%20main%20application,activities%20on%20my%20account}{alain-airom.medium.com}.
Zweitens stellt sich die Datenschutzfrage: Selbst wenn die Daten intern bleiben, muss gewährleistet sein, dass nur berechtigte Informationen im jeweiligen Kontext ausgegeben werden und keine sensiblen Inhalte unkontrolliert verbreitet werden.
Drittens spielt die technische Realisierbarkeit im vorhandenen IT-Umfeld eine Rolle.
Viele KI-Frameworks und -Tools sind primär auf Linux-basierte Umgebungen ausgerichtet, während im Unternehmen möglicherweise ein Windows Server als Plattform vorgesehen ist.
Tatsächlich traten bei Tests auf einem Windows Server einige Einschränkungen auf – gewisse Software-Komponenten funktionierten nur eingeschränkt, und Performance-Probleme mussten adressiert werden (siehe \textit{Technische Umsetzung}).



\subsection{Soll-Zustand}
Der angestrebte Soll-Zustand ist ein datenschutzkonformer RAG-Chatbot, der einige Eigenschaften erfüllen muss.
Es muss auf kommerzieller Hardware innerhalb der Unternehmensinfrastruktur betrieben werden können (On-Premises).
Konnektoren zu SharePoint und OneDrive müssen im besten Fall automatisiert Dokumente einlesen können.
Der Chatbot soll in der Lage sein, natürlichsprachliche Fragen der Nutzer zu verstehen und präzise Antworten zu generieren, indem er relevante Informationen aus den internen Dokumenten extrahiert
Dafür muss der Chatbot in der Lage sein Deutsch und Englisch zu verstehen und auch in beiden Sprachen, unabhängig von der Sprache der Anfrage, zu antworten.
Die Antwortqualität soll so hoch sein, dass die Nutzer den Chatbot als verlässliche Informationsquelle wahrnehmen.
Das System muss sicherstellen, dass keine sensiblen Daten unkontrolliert nach außen gelangen.
Zudem soll der Chatbot in eine benutzerfreundliche Oberfläche integriert werden.

Der Chatbot muss komplett on-premises betrieben werden können, um Datenschutzanforderungen zu erfüllen.

\subsubsection{Abgrenzung zu kommerziellen Cloud-Diensten}\label{subsec:abgrenzung-zu-kommerziellen-cloud-diensten}
Die Entscheidung, einen datenschutzkonformen RAG-Chatbot zu entwickeln, der ausschließlich on-premises betrieben wird, basiert auf mehreren Überlegungen.
Kommerzielle Cloud-Dienste für KI-Modelle bieten zwar eine einfache und skalierbare Lösung, bringen jedoch erhebliche Datenschutz- und Sicherheitsrisiken mit sich.
Insbesondere in regulierten Branchen oder bei sensiblen Unternehmensdaten ist es oft nicht zulässig, diese Daten an externe Cloud-Anbieter zu übermitteln.
Die Kontrolle über die Datenverarbeitung und -speicherung geht verloren, was zu Compliance-Verstößen führen kann.
Zudem besteht das Risiko, dass vertrauliche Informationen ungewollt offengelegt werden.
Man kann ohne Weiteres gegen ein Entgelt sofort auf vielerlei solcher Anbieter zugreifen, wie etwa OpenAI, Azure OpenAI, Google Cloud AI oder Amazon Bedrock.
Diese Dienste bieten leistungsstarke LLMs und einfache APIs, die eine schnelle Integration ermöglichen.
Allerdings sind diese Dienste oft mit hohen laufenden Kosten verbunden, insbesondere bei großem Datenvolumen oder hoher Nutzungsfrequenz.
Für kleinere Unternehmen oder solche mit begrenztem IT-Budget können diese Kosten prohibitiv sein.
Darüber hinaus sind kommerzielle Cloud-Dienste häufig nicht flexibel genug, um spezifische Anforderungen oder Anpassungen zu erfüllen.
Die Nutzer sind auf die Funktionen und Updates des Anbieters angewiesen, was die Innovationsfähigkeit einschränken kann.
Im Gegensatz dazu ermöglicht ein on-premises RAG-Chatbot die vollständige Kontrolle über die Daten und die Infrastruktur.
Unternehmen können sicherstellen, dass alle Datenschutz- und Sicherheitsanforderungen eingehalten werden.
Zudem können sie die Lösung genau auf ihre Bedürfnisse zuschneiden und haben die Freiheit, neue Funktionen oder Modelle zu integrieren, ohne auf einen Drittanbieter angewiesen zu sein.
Es ist auch ein klares Signal an Kunden und Partner, dass das Unternehmen den Schutz sensibler Informationen ernst nimmt.
Zudem ist hier nicht zu vernachlässigen, dass sowohl die persönliche Motivation dieser Masterarbeit als auch die des Unternehmens darin liegt auch selbst innovativ tätig zu werden und nicht nur auf fertige Lösungen von Drittanbietern zurückzugreifen.
Es ist ein klares Ziel, Know-how im Bereich moderner KI-Technologien aufzubauen und langfristig eigene Kompetenzen zu entwickeln.
KI soll nicht nur konsumiert, sondern auch gestaltet werden.
Das soll nicht den Alltagsbetrieb stören, sondern als Kooperator fungieren.
Deswegen wird hier explizit auf kommerzielle Cloud-Dienste verzichtet.
Wenn das Ergebnis dieser Arbeit zeigt, dass ein datenschutzkonformer RAG-Chatbot on-premises machbar ist, könnte dies als Grundlage für weitere Entwicklungen und Innovationen im Unternehmen dienen.
Falls nicht, so liefert es wertvolle Erkenntnisse über die Herausforderungen und Grenzen solcher Ansätze und kann als Entscheidungsgrundlage für zukünftige Investitionen in KI-Technologien dienen.


\subsection{Leistungsanforderungen}
Derzeit existiert keine lokale On-Premise Hardware- oder Software-Infrastruktur für einen datenschutzkonformen RAG-Chatbot im Unternehmen.
Die Mitarbeiter greifen auf SharePoint und OneDrive zu, um Dokumente zu speichern und zu verwalten.
Es gibt jedoch keine automatisierten Mechanismen, um diese Dokumente für einen Chatbot zugänglich zu machen.
Die Mitarbeiter müssen manuell nach Informationen suchen, was zeitaufwendig und ineffizient ist.
Es gibt keine bestehende Chatbot-Lösung, die auf internen Dokumenten basiert.
Die IT-Infrastruktur des Unternehmens ist hauptsächlich Windows-basiert, was die Auswahl und Integration von KI-Tools und -Frameworks einschränkt.
Zudem sind Datenschutz- und Sicherheitsrichtlinien vorhanden, die den Umgang mit sensiblen Daten regeln.
Diese Richtlinien müssen bei der Entwicklung des Chatbots strikt eingehalten werden.

Insgesamt besteht eine deutliche Lücke zwischen dem aktuellen Zustand und dem angestrebten Soll-Zustand, die durch die Entwicklung eines datenschutzkonformen RAG-Chatbots geschlossen werden soll.

Aufgrund dessen wurden sich potentielle Kandidaten für die technische Umsetzung angeschaut und bewertet.

\subsubsection{CPU vs GPU}
Ein weiterer wichtiger Aspekt bei der Auswahl der technischen Umsetzung ist die Frage, ob die KI-Modelle auf CPU- oder GPU-Hardware laufen sollen.
GPU-beschleunigte Systeme bieten in der Regel eine deutlich höhere Leistung bei der Verarbeitung großer Modelle und Datenmengen.
Sie sind besonders vorteilhaft, wenn Echtzeit-Antworten und eine hohe Skalierbarkeit erforderlich sind.
Allerdings sind GPUs oft teurer in der Anschaffung und im Betrieb, was für kleinere Unternehmen eine Hürde darstellen kann.
CPU-basierte Systeme sind hingegen kostengünstiger und einfacher zu warten.
Sie können für kleinere Modelle und weniger intensive Workloads ausreichend sein.
Im Rahmen dieser Arbeit wurde die Entscheidung getroffen, eine CPU-basierte Lösung zu verfolgen, um die Kosten zu minimieren und die Komplexität der Infrastruktur zu reduzieren.
Dies stellt sicher, dass der Chatbot auch auf handelsüblicher Hardware betrieben werden kann, was den Zugang für kleinere Unternehmen erleichtert.
Insgesamt zeigt die Analyse der Ist-Situation und der verfügbaren Tools, dass die Entwicklung eines datenschutzkonformen RAG-Chatbots technisch machbar ist.
Die Wahl der richtigen Tools und Frameworks sowie die Berücksichtigung von Hardware-Anforderungen sind entscheidend, um die angestrebten Ziele zu erreichen.
\par
Es stand bisher nur ein Linux-VPS mit 2 vCPUs und 4 GB RAM zur Verfügung, was für die Ausführung größerer Modelle und die Verarbeitung umfangreicher Dokumente nicht ausreicht.
Dieser geriet schnell auf Grund der fehlenden Hardware-Beschleunigung an seine Grenzen.
VPS-Server haben oftmals, wie auch in diesem Fall, Einschränkungen bei der Installation bestimmter Software-Komponenten sowie der Nutzung der Hardware-Beschleunigung (kein Zugriff auf GPU, eingeschränkte CPU-Leistung).
Daher wurde entschieden, die Entwicklung und das Testen der Anwendung auf einem Windows-Server mit folgenden Spezifikationen durchzuführen:
\par
%
%Typ:Dedicated Server L-16CPU:4 Core x 3.5 GHz (Intel Xeon E3-1230 v6)RAM:16 GBSSD:2 x 480 GB Software RAID 1

\begin{itemize}
    \item \textbf{Prozessor:} Intel Xeon E3-1230 v6, 4 Kerne, 3.5 GHz
    \item \textbf{Arbeitsspeicher:} 16 GB RAM
    \item \textbf{Speicher:} 2 x 480 GB SSD im Software-RAID 1
    \item \textbf{Betriebssystem:} Windows Server
\end{itemize}

Sollte diese Hardwareleistung ausreichen würden damit größere Kosten für Cloud-Server sowie GPUs vermieden werden können.

\subsubsection{Open-Source Tools und Frameworks}
Für die Umsetzung des datenschutzkonformen RAG-Chatbots werden verschiedene Open-Source-Tools und Frameworks evaluiert.
Die Entscheidung Open-Source-Lösungen zu nutzen, basiert auf mehreren Faktoren.
Erstens ermöglichen Open-Source-Tools eine hohe Flexibilität und Anpassungsfähigkeit.
Der Quellcode ist frei zugänglich, was es ermöglicht, die Software an spezifische Anforderungen anzupassen und zu erweitern.
Zweitens entfallen Lizenzkosten, was insbesondere für kleinere Unternehmen von Vorteil ist.
So entfällt auch eine Abhängigkeit von kommerziellen Anbietern, die möglicherweise nicht alle Datenschutzanforderungen erfüllen oder jederzeitige Änderungen in ihren Diensten vornehmen könnten.
Wichtige Kriterien bei der Auswahl waren die Kompatibilität mit der Windows-basierten IT-Infrastruktur, die Fähigkeit zur Verarbeitung von Dokumenten aus SharePoint und OneDrive, die Unterstützung für RAG-Architekturen und die Einhaltung von Datenschutzanforderungen.
Die Evaluierung dieser Tools ist in folgender Tabelle zusammengefasst:

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lp{2cm}p{2.4cm}p{2.2cm}p{1.1cm}p{2.6cm}}
        \toprule
        \textbf{Framework/Tool)} & \textbf{RAG / Zitation} & \textbf{Office \& PDF} & \textbf{Lokale API} & \textbf{Typ. RAM} & \textbf{Wann empfehlenswert} \\
        \midrule
        GPT-4All Desktop & Integriertes RAG, Seiten-Zitate & DOCX / XLSX native; kein OCR & OpenAI-kompatibel (Einstellungen) & \(\sim\)12 GB & Einfache GUI, schnell lauffähig \\
        Open WebUI + Ollama & RAG-Plugin, Quellenangaben & Viele Formate; OCR per Add-on & OpenAI-kompatible Endpunkte & 8–14 GB & Moderne Web-UI mit REST-API \\
        LM Studio & Knowledge-Workspace mit Zitationen & PDF, TXT; Office nachrüstbar & OpenAI-kompatible API (Port 1234) & 10–14 GB & Desktop + LAN-API Kombination \\
        text-generation-webui & File-loader \& Vector-RAG & Office via Add-on; OCR via pytesseract & REST / WebSocket & 12–14 GB & Umfangreiche Plug-in‑Architektur \\
        LocalGPT 2.0 & CLI/UI mit Seiten-Zitaten & PDF / TXT nativ; Office vor Konvertierung & FastAPI (Port 5111) & 4–6 GB & Ressourcenschonend, datenschutzfokussiert \\
        Onyx & RAG-Plugin, optionale Zitationen & PDF, DOCX; OCR optional & OpenAI-kompatible Endpunkte & 8–12 GB & Gut für On\-Premises-Einsatz und flexible Integrationen \\
        \bottomrule
    \end{tabular}
    \caption{Vergleich ausgewählter Open\-Source RAG-Tools}
    \label{tab:rag-tools}
\end{table}

Wie aus der Tabelle ersichtlich bieten verschiedene Tools unterschiedliche Stärken und Schwächen.
GPT-4All Desktop punktet mit einer benutzerfreundlichen GUI und integriertem RAG-Support, während Open WebUI durch seine moderne Web-Oberfläche und REST-API hervorstecht.
LM Studio kombiniert Desktop-Chat mit LAN-API-Funktionalität, was für bestimmte Anwendungsfälle vorteilhaft sein kann.
Text-generation-webui besticht durch seine Plug-in-Architektur, die vielfältige Erweiterungen ermöglicht.
LocalGPT 2.0 ist besonders ressourcenschonend und legt den Fokus auf Datenschutz.
Onyx wiederum ist ein vollumfängliches RAG-Tool, inklusive Konnektoren und Zitationsfunktionen.


Um eine gewichtete Entscheidung zu treffen, wurden die Tools hinsichtlich ihrer Funktionalität, Anpassungsfähigkeit und Benutzerfreundlichkeit bewertet.

\begin{ itemize}
    \item \textbf{Funktionalität:} Wie gut unterstützt das Tool RAG-Architekturen und die Verarbeitung von Office-Dokumenten?
    \item \textbf{Anpassungsfähigkeit:} Inwieweit lässt sich das Tool an spezifische Anforderungen anpassen, etwa durch Plug-ins oder API-Integration?
    \item \textbf{Benutzerfreundlichkeit:} Wie intuitiv ist die Bedienung des Tools, sowohl für Entwickler als auch für Endnutzer?
    \item \textbf{Ressourcenbedarf:} Wie viel Arbeitsspeicher wird typischerweise benötigt, um das Tool effizient zu betreiben?
    \item \textbf{Datenschutzkonformität:} Inwieweit erfüllt das Tool die Anforderungen an den Datenschutz im On-Premises-Betrieb?
    \item \textbf{Community und Support:} Wie aktiv ist die Entwickler-Community und wie gut ist die Dokumentation?
    \item \textbf{Integration in bestehende Systeme:} Wie gut lässt sich das Tool in die vorhandene IT-Infrastruktur integrieren?
    \item \textbf{Zukunftssicherheit:} Wie regelmäßig werden Updates und neue Features bereitgestellt?
    \item \textbf{Kosten:} Welche Kosten entstehen durch den Betrieb des Tools, insbesondere im Vergleich zu kommerziellen Lösungen?
    \item \textbf{Skalierbarkeit:} Wie gut lässt sich das Tool an wachsende Anforderungen anpassen, etwa durch Hinzufügen von mehr Nutzern oder Datenquellen?
    \item \textbf{Performance:} Wie schnell reagiert das Tool auf Nutzeranfragen, insbesondere bei großen Dokumentenbeständen?
    \item \textbf{Sicherheitsfunktionen:} Welche Mechanismen bietet das Tool, um sensible Daten zu schützen und unbefugten Zugriff zu verhindern?
    \item \textbf{Flexibilität bei der Modellauswahl:} Inwieweit können verschiedene LLMs genutzt werden, um unterschiedliche Anforderungen zu erfüllen?
    \item \textbf{Einfache Wartung:} Wie einfach ist die Wartung und Aktualisierung des Tools im laufenden Betrieb?
    \item \textbf{Benutzerverwaltung:} Welche Möglichkeiten bietet das Tool zur Verwaltung von Nutzerrollen und -rechten?
    \item \textbf{Dokumentenmanagement:} Wie gut unterstützt das Tool die Organisation und Verwaltung der eingebundenen Dokumente?
    \item \textbf{Mehrsprachigkeit:} Inwieweit unterstützt das Tool verschiedene Sprachen, insbesondere Deutsch und Englisch?
    \item \textbf{Offline-Fähigkeit:} Kann das Tool vollständig ohne Internetverbindung betrieben werden?
    \item \textbf{Anpassbare Antwortformate :} Inwieweit können die Antworten des Chatbots an spezifische Formate oder Stile angepasst werden?
    \item \textbf{Logging und Monitoring:} Welche Möglichkeiten bietet das Tool zur Überwachung und Protokollierung von Nutzerinteraktionen und Systemaktivitäten?
\end{ itemize}

All diese Punkte flossen in die Entscheidungsfindung
Die Auswahl des geeigneten Tools hängt maßgeblich von den zu folgenden Tests und Anforderungen ab.
GPT-4All fällt durch seine fehlenden OCR-Fähigkeiten für gescannte Dokumente und die eingeschränkte API-Integration aus.
Im Grunde sind alle Tools in der Lage, RAG-Architekturen zu unterstützen und Dokumente aus Office-Formaten zu verarbeiten.
Der Mehraufwand, das Design und die Funktionalität der Tools an die eigenen Bedürfnisse anzupassen, variiert jedoch stark.
Basierend auf diesen Überlegungen wurde entschieden, Open WebUI oder Onyx in Kombination mit Ollama für die prototypische Umsetzung zu verwenden.
Beide Tools bieten eine gute Balance zwischen Funktionalität, Anpassungsfähigkeit und Benutzerfreundlichkeit und erinnern in ihrem Design an bekannte Chat-Interfaces wie ChatGPT.


\sub{Kommunikationsschnittstellen}\label{subsec:kommunikationsschnittstellen}
Für die Interaktion mit dem RAG-Chatbot sind verschiedene Kommunikationsschnittstellen denkbar.
Eine Web-basierte Oberfläche bietet den Vorteil der plattformunabhängigen Nutzung über verschiedene Endgeräte (z.B. Desktop, Laptop, Tablet).
Alternativ könnte eine Integration in bestehende Kollaborationsplattformen wie Microsoft Teams erfolgen, was den Zugang für Mitarbeiter erleichtert.
Zudem sind APIs denkbar, die eine Anbindung an andere Systeme oder Anwendungen ermöglichen.
Im Rahmen dieser Arbeit wurde entschieden, eine Web-basierte Oberfläche zu implementieren.
Alles Weitere wird optional gehalten, um die Flexibilität zu maximieren und zukünftige Erweiterungen zu ermöglichen.
\par
Die geplante Kommunikation sieht wie folgt aus:

\FloatBarrier
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Bilder/masterarbeit/on-premise-rag-chatbot-uml-architektur-u6uu2d}
    \caption{Geplante Kommunikationsschnittstellen des RAG-Chatbots}
    \label{fig:rag-chatbot-architektur}
\end{figure}
\FloatBarrier

Der Nutzer interagiert über ein Web-Interface mit dem Chatbot.
Die Anfragen werden an das RAG-Modul weitergeleitet, das relevante Dokumentenabschnitte aus der Vektordatenbank abruft.
Diese Informationen werden dann an das LLM übergeben, das die Antwort generiert und zurück an das Web-Interface sendet.
Diese Architektur ermöglicht eine klare Trennung der Komponenten und erleichtert die Wartung und Erweiterung des Systems.
\par
\begin{itemize}

    \item \textbf{Modularität und Entkopplung als Architekturprinzip}\\
    Ein zentrales Prinzip moderner Softwarearchitektur ist die \textit{Modularisierung} eines Systems. David Parnas zeigte bereits 1972, dass eine Zerlegung in Module die Flexibilität und Verständlichkeit eines Systems erhöht und gleichzeitig die Entwicklungszeit verkürzen kann[1]. Dieses Prinzip beruht auf der \textit{Trennung von Verantwortlichkeiten} (\textit{Separation of Concerns}) und der \textit{informationellen Abschirmung} einzelner Bausteine. Jedes Modul kapselt einen klar abgegrenzten Aspekt der Funktionalität und verbirgt Implementierungsdetails vor anderen Modulen. Dadurch entsteht eine geringere Kopplung zwischen den Teilen des Systems bei gleichzeitig hoher Kohäsion innerhalb der Module. Ein solches Design – oft umschrieben mit “low coupling, high cohesion” – gilt als Voraussetzung für wartbaren Code, da Änderungen an einer Komponente minimalen Einfluss auf andere Komponenten haben[2][3]. Zudem lässt sich jedes Modul unabhängig verstehen, testen und weiterentwickeln, was der Komplexitätsbeherrschung großer Softwaresysteme dient. Insgesamt schafft Modularität also die Grundlage für ein System, das anpassungsfähig und robust gegenüber Änderungen ist (ein wesentliches Qualitätsmerkmal gemäß gängiger Software-Engineering-Literatur).

    \item \textbf{Trennung der RAG-Komponenten}\\
    Die Aufteilung eines Retrieval-Augmented-Generation (RAG) Chatbot-Systems in getrennte Kernkomponenten – insbesondere in ein LLM-Modul, ein Embedding-Modul (Retriever) samt Dokumentenquelle sowie einen Reranker – folgt dem oben genannten Modularitätsprinzip in der konkreten Anwendung. Jede dieser Komponenten erfüllt eine \textit{spezifische Aufgabe} im Gesamtsystem und kann somit als eigenständiges Modul betrachtet werden[4]. So ist das Embedding-Modul dafür zuständig, semantische Vektor-Repräsentationen der Benutzeranfrage (und ggf. der Dokumente) zu berechnen und über die Dokumentenquelle relevante Wissensinhalte zu suchen. Der Reranker wiederum verfeinert die Ergebnisauswahl durch Neuanordnung oder Filterung der abgerufenen Dokumente, bevor das LLM-Modul (etwa ein generatives Sprachmodell wie Ollama) diese Kontexteingaben nutzt, um eine finale Antwort zu formulieren. Diese Aufgabentrennung entspricht auch der in wissenschaftlichen Übersichtsarbeiten beschriebenen Dreiteilung von RAG-Systemen in Retrieval-, Generierungs- und Augmentationskomponenten[5]. Indem man Retrieval und Generierung – und optional weitere Schritte wie Re-Ranking oder Zusammenfassen – als getrennte Module behandelt, erhält man feingranulare Kontrolle über jeden Verarbeitungsschritt[6].

    Die sinnvolle Trennung dieser RAG-Bausteine bringt mehrere Vorteile: Erstens kann jede Komponente unabhängig optimiert oder ausgetauscht werden, ohne das Gesamtsystem neu entwerfen zu müssen. Beispielsweise ließe sich ein Embedding-Modell durch ein genaueres oder effizienteres ersetzen, ohne Änderungen am LLM oder am Reranker vorzunehmen. Ebenso könnte man das LLM-Modul austauschen (z.\,B. ein größeres Modell oder ein spezialisiertes Domain-Modell einsetzen), während Retriever und Reranker unverändert bleiben. Diese Austauschbarkeit fördert schnelle Iteration und Anpassung an neue Anforderungen oder bessere Algorithmen[7][8]. Zweitens ermöglicht die klare Abgrenzung der Verantwortlichkeiten eine gezielte Weiterentwicklung: So kann man etwa den Reranker weglassen oder hinzufügen, je nach gewünschter Antwortqualität, oder zusätzliche Augmentationsmodule (z.\,B. ein Kontext-Filter oder ein Memory-Modul) zwischenfügen, ohne die Struktur der übrigen Pipeline aufzubrechen[9][10]. In aktuellen Forschungsarbeiten wird ein solcher \textit{„Lego-artiger“ modularer Aufbau} von RAG-Systemen als zukunftsweisend beschrieben, da er über den starren “retrieve-then-generate”-Prozess hinausgeht und komplexere Ablaufstrukturen wie bedingtes Routing oder iterative Schleifen erlaubt[6]. Die Trennung von LLM, Embeddingsuche und Reranking ist demnach nicht nur technisch machbar, sondern auch wissenschaftlich motiviert, um Flexibilität und Anpassungsfähigkeit des Systems zu maximieren (vgl. Gao et al. 2024 und Gao et al. 2023). Schließlich verringert die Entkopplung auch das Risiko von Fehlerfortpflanzung: Ein Defekt oder eine Qualitätsminderung in einem Modul (etwa unzureichende Einbettungsqualität) kann isoliert betrachtet und behoben werden, ohne dass die anderen Module direkt betroffen sind – was zur Zuverlässigkeit des Gesamtsystems beiträgt[11].

    \item \textbf{Zentrale Koordination durch das Web-Interface}\\
    Das Web-Interface fungiert in dieser Architektur als \textit{zentrale Orchestrierungskomponente}. Es nimmt Benutzeranfragen entgegen, koordiniert die Abfolge der Verarbeitungsschritte über die genannten Module hinweg und liefert letztlich die generierte Antwort zurück an den Nutzer. Diese zentrale “Steuerung” bringt mehrere Vorteile mit sich. Zum einen stellt das Web-Interface eine einheitliche Schnittstelle nach außen bereit (ähnlich einem \textit{Fassade}-Entwurfsmuster), sodass die internen Abläufe der RAG-Pipeline vor dem Nutzer verborgen bleiben. Für den Anwender erscheint das System als eine homogene Einheit, obwohl im Hintergrund mehrere spezialisierte Dienste zusammenwirken. Zum anderen implementiert das Web-Interface die Ablauflogik an einem Ort, was die Komplexität beherrschbar macht: Anstatt dass jedes Modul wissen muss, wann und wie es die anderen aufrufen soll, übernimmt das Web-Interface diese Vermittlerrolle (ähnlich dem \textit{Mediator}-Pattern der Entwurfsmusterliteratur). Dadurch bleiben die Kernmodule voneinander entkoppelt – sie kommunizieren ausschließlich über wohl definierte Schnittstellen mit dem Koordinator, jedoch nicht direkt untereinander. Die Module müssen folglich nichts über die Existenz oder Implementierungsdetails der anderen wissen, was wiederum genau dem oben genannten Prinzip der lose gekoppelten Architektur entspricht.

    Aus Sicht der Softwaretechnik erhöht diese Trennung von UI/Koordination und Fachlogik die Wartbarkeit und Austauschbarkeit der Komponenten erheblich. Ändert sich beispielsweise die Art der Benutzerschnittstelle (etwa von einem Web-Frontend zu einer Sprachassistenten-Oberfläche), kann diese Änderung erfolgen, ohne die inneren Module (Retriever, LLM etc.) anzupassen – das System ist UI-unabhängig[12]. Ebenso können im Web-Interface bereichsübergreifende Aspekte konzentriert behandelt werden, z.\,B. Authentifizierung, Protokollierung (Logging) oder Fehlerbehandlung, ohne die Fachlogik in den Modulen damit zu belasten. Insgesamt bietet ein zentrales Koordinationsmodul also eine Kapselung der Ablaufsteuerung, was konsistente Abläufe sicherstellt und die Module vor unnötiger Komplexität schützt. Diese Architektur folgt dem Grundsatz der \textit{Schichtentrennung}, wie er in Clean Architecture und ähnlichen Architekturstilen empfohlen wird: Die Präsentations- und Steuerungsschicht (hier das Web-Interface) ist klar getrennt von den Geschäftslogik-Komponenten, welche wiederum getrennt sind von niedrigeren technischen Schichten (Datenhaltung usw.)[3]. Dadurch entsteht ein leicht austausch- und erweiterbares System, in dem jede Schicht sich auf ihren Zweck konzentriert.

    \item \textbf{Skalierbarkeit, Wartbarkeit und Erweiterbarkeit des Designs}\\
    Eine modulare RAG-Architektur unterstützt explizit die Qualitätsmerkmale Skalierbarkeit, Wartbarkeit und Erweiterbarkeit. Dies wird sowohl in der Praxis als auch in der Literatur bestätigt. Durch die Entkopplung der Komponenten kann jede Einheit bei Bedarf \textit{individuell skaliert} werden. Sollte z.\,B. das LLM-Modul besonders rechenintensiv sein, lässt es sich unabhängig von den anderen Komponenten auf mehreren Servern oder mit größerer Hardware-Power betreiben, ohne dass dazu das Gesamtsystem dupliziert werden müsste. Ebenso kann die Embeddingsuche (DocumentSource+EmbeddingModel) auf einen leistungsfähigeren Vektordatenbank-Server ausgelagert oder parallelisiert werden, falls das Anfragevolumen steigt. Industrielle Berichte zeigen, dass ein solcher service-orientierter Aufbau die Skalierbarkeit deutlich erhöht – Komponenten können als eigenständige Dienste “hochgefahren” werden, was gezielte Lastverteilung ermöglicht[13]. Auch aus Architektursicht (z.\,B. in der Hexagonalen Architektur) wird betont, dass die Isolierung der Kernlogik eine effiziente Ressourcenoptimierung und einfaches Hochskalieren bestimmter Funktionalitäten erlaubt[14].

    Die Wartbarkeit profitiert gleichermaßen: Änderungen oder Fehlerbehebungen können lokal in einem Modul vorgenommen werden, ohne Seiteneffekte auf andere Teile. Dies reduziert die Komplexität von Deployments und Regressionstests erheblich – ein neues Embedding-Modell einzuspielen betrifft nur das Embedding-Modul, während die restliche Pipeline unverändert bleibt. Entwicklerteams können zudem \textit{parallel} an unterschiedlichen Modulen arbeiten (z.\,B. ein Team verbessert den Reranker, während ein anderes das Web-Interface erweitert), was die Entwicklungszeit verkürzt[8]. Sollte ein Bug auftreten, lässt sich durch die klare Verantwortlichkeitszuordnung schneller ermitteln, in welcher Komponente die Ursache liegt. Modularität erhöht auch die Zuverlässigkeit im Betrieb: Fällt eine Komponente aus oder liefert fehlerhafte Ergebnisse, bleibt der Fehler isoliert und zieht nicht unmittelbar das ganze System in Mitleidenschaft – oft kann eine Ersatzinstanz des betreffenden Dienstes gestartet werden, während andere Komponenten weiterlaufen. Dieses Prinzip macht das System robuster gegen Teilausfälle (Fehlertoleranz durch Kapselung).

    Schließlich erleichtert die Modularisierung die Erweiterbarkeit des Systems. Neue Funktionen können als zusätzliche Module implementiert und in die bestehende Orchestrierung eingebunden werden, statt monolithische Codeteile aufwendig ändern zu müssen. Beispielsweise könnte man ein Modul zur Wissensaktualisierung (z.\,B. periodischer Neucrawl der Dokumentenquelle) oder ein Modul zur Ergebniserklärung (das die Antwort des LLM mit Quellverweisen anreichert) hinzufügen, indem das Web-Interface um einen entsprechenden Aufruf erweitert wird. Die übrigen Komponenten bleiben unberührt, solange die Schnittstellen konsistent bleiben. Dieses “Plug-and-Play”-Konzept wurde etwa von Ilia Markov (2025) als Schlüsselaspekt von modularen RAG-Systemen hervorgehoben – verschiedene Branchen können gezielt Module hinzufügen, um das System an ihre Bedürfnisse anzupassen (z.\,B. juristische Chatbots fügen ein Zitations-Modul hinzu, E-Commerce-Assistenten ein Empfehlungs-Modul)[8][15]. Kurzum: Die vorgestellte Architektur kann mit den Anforderungen mitwachsen. Sie ermöglicht es, neue Technologien oder Algorithmen einzubinden, ohne das Gesamtsystem neu zu erfinden, und unterstützt damit ein evolutionäres Vorgehen in der Systementwicklung.

    \item \textbf{Relevante Architekturstile und Entwurfsmuster}\\
    Die beschriebenen Konstruktionsentscheidungen lassen sich in den Kontext bekannter Architekturstile einordnen. Insbesondere zeigen sich Parallelen zu \textbf{Clean Architecture} bzw. dem \textbf{Ports-and-Adapters-Prinzip} (Hexagonale Architektur nach Alistair Cockburn). Beide Ansätze verfolgen das Ziel, Geschäftslogik von technischen Details strikt zu trennen und Abhängigkeiten umzukehren: Die Kernlogik (“innere Schicht”) definiert abstrakte Schnittstellen (\textit{Ports}), die von äußeren Schichten (\textit{Adapters}) implementiert werden. Übertragen auf unsere RAG-Architektur bedeutet dies: Das Web-Interface und ggf. eine zentrale Steuerungskomponente bilden die \textit{Domänenschicht}, welche z.B. eine abstrakte Schnittstelle für “Embeddingsuche” oder “LLM-Antwortgenerierung” vorgibt. Die konkreten Implementierungen – sei es ein Aufruf an ein bestimmtes Embedding-Modell oder an ein API einer LLM-Engine – erfolgen in getrennten Modulen, die diese Schnittstellen erfüllen. Die Abhängigkeit zeigt also \textit{von der Kernlogik nach außen}: Das System ist entwerfbar, ohne sich auf spezifische Bibliotheken oder Modelle festzulegen, da diese erst als Plugins angehängt werden. Laut Robert C. Martin resultiert daraus Software, die unabhängig von Frameworks, UI, Datenbanken etc. agiert und die Geschäftslogik nicht von technischen Details beeinflussen lässt[16][12]. Dies trifft den Kern unseres Designs: die LLM- und Embedding-Module könnten z.B. leicht von lokalen Bibliotheken auf Web-Services umgestellt werden, ohne dass sich an der Logik im Web-Interface etwas ändert – man müsste lediglich den Adapter austauschen. Clean Architecture fördert zudem die Testbarkeit, da man die Kernlogik mit Mock-Implementierungen der Schnittstellen isoliert testen kann[17]. Genauso kann in unserer Architektur beispielsweise die Antwortgenerierung getestet werden, indem man ein Dummy-LLM-Modul einspeist. Insgesamt bestätigen diese Entwurfsmuster die Sinnhaftigkeit eines entkoppelten, schnittstellenbasierten Designs für Wartbarkeit und Flexibilität.

    Ein weiterer relevanter Aspekt ist die Frage nach \textbf{Microservices vs. Monolith}. Die modulare Aufteilung begünstigt prinzipiell eine Microservice-Architektur, bei der jedes Modul als eigener (netzwerkgekoppelter) Service deployt wird. Tatsächlich beschreiben Lewis und Fowler (2014) Microservices als Ansatz, eine Anwendung als Sammlung kleiner, unabhängig deploybarer Dienste zu implementieren[18]. Im Kontext unseres Systems könnte man z.B. einen eigenständigen “Embedding-Service”, einen “Reranking-Service” und einen “LLM-Service” betreiben, die über leichte Kommunikationsmechanismen (etwa HTTP-APIs) vom Web-Interface aus angesprochen werden. Dies hätte den Vorteil, dass jede Komponente separat skaliert, ausgetauscht oder neu gestartet werden kann, ohne die anderen zu beeinflussen – ganz im Sinne der vorher erwähnten Skalierbarkeits- und Wartbarkeitsvorteile. Die Firma RidgeRun berichtet etwa, dass sie ihr RAG-System als Suite von Microservices umgesetzt hat, um genau diese Unabhängigkeit der Komponenten und leichte Austauschbarkeit von Open-Source-LLMs zu erreichen[13]. Microservices fördern auch technologische Diversität: so könnte die Dokumentenquelle in einer anderen Programmiersprache oder mit einer speziellen Datenbank realisiert sein, während das Web-Interface weiterhin z.B. in Python läuft.

    Allerdings ist zu beachten, dass Microservices auch zusätzliche Komplexität mit sich bringen – etwa durch den Overhead verteilter Systeme, Netzwerkkommunikation, Monitoring vieler Deployments etc. Daher folgt man häufig dem Pragmatismus, zunächst einen gut modularisierten Monolithen zu bauen und bei Wachstumsbedarf ausgewählte Module in separate Dienste auszugliedern. Wichtig ist, dass die Architektur diese Trennlinien bereits vorsieht, was in unserem Design der Fall ist. Ein modularer Monolith, der die beschriebenen entkoppelten Schnittstellen intern einhält, kann viele Vorteile der Clean Architecture bereits liefern (klare Struktur, lokale Austauschbarkeit, Testbarkeit), ohne die Betriebskomplexität von verteilten Services. Sollte das System jedoch an Grenzen stoßen – etwa weil bestimmte Teile skalieren müssen oder unterschiedliche Deployment-Zyklen verlangen – kann man immer noch zum Microservice-Stil übergehen, da die Modulgrenzen ja klar definiert sind. Fowler weist darauf hin, dass monolithische Anwendungen mit zunehmendem Umfang häufig Erosionserscheinungen in der Modularität zeigen und Änderungen dann das gesamte System neu deployen lassen müssen[19]. Um dies zu vermeiden, ist unser Architekturentwurf von Anfang an auf Entkopplung ausgelegt.

\end{itemize}



