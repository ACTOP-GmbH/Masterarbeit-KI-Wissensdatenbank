\pagebreak

\section{Problemanalyse}\label{sec:problemanalyse}

\subsection{Ist-Zustand}\label{subsec:ist-zustand-und-problemstellung}


In modernen Organisationen wächst die Menge an digital verfügbaren Informationen rasant.
%TODO Quelle?
Wissen liegt oft in verschiedenen Dokumenten über beispielsweise SharePoint-Sites, OneDrive-Verzeichnisse oder in Datenbanksystemen verteilt vor.
Mitarbeiter, die Antworten auf spezifische Fragen suchen (etwa Richtlinien, Projektberichte, technische Dokumentationen), müssen derzeit entweder manuelle Suchen durchführen oder sich durch lange Dokumente arbeiten.
Dies ist zeitaufwendig und ineffizient.
Ein intelligenter Chatbot, der Fragen in natürlicher Sprache beantwortet und direkt auf relevante interne Informationen zugreift, könnte die Informationsbeschaffung erheblich erleichtern.

\par
Herkömmliche Chatbots oder Suchfunktionen stoßen hierbei an Grenzen: Ein einfacher Keyword-Suchlauf liefert oft unstrukturierte Trefferlisten, anstatt konkrete Antworten.
Moderne generative KI-Systeme wie GPT-4 besitzen zwar beeindruckende sprachliche Fähigkeiten, haben aber keinen Zugang zu unternehmensspezifischen Daten und Wissen, das nicht in ihrem Trainingsdatensatz enthalten war\href{https://en.wikipedia.org/wiki/Retrieval-augmented_generation#:~:text=Retrieval,responses%20based%20on%20authoritative%20sources}{en.wikipedia.org}.
Zudem wäre ein Einsatz von Cloud-basierten LLM-Diensten in vielen Fällen aus Datenschutz- und Compliance-Sicht problematisch – vertrauliche Unternehmensdaten dürfen nicht unkontrolliert an externe Dienste gesendet werden\href{https://punctuations.ai/ai-agents-workflows/your-private-gpt-the-case-for-secure-on-premise-llms/#:~:text=pharmaceutical%20companies%2C%20manufacturers%2C%20government%20contractors,keeping%20AI%20close%20to%20home}{punctuations.ai}.
In streng regulierten Branchen oder bei sensiblen Informationen (etwa personenbezogene Daten, Geschäftsgeheimnisse) überwiegen die Risiken gegenüber dem Nutzen, wenn man öffentliche KI-APIs nutzt.
Zusätzlich fehlt es generischen LLMs an Aktualität und Domänenwissen: Sie könnten halluzinierende Antworten geben, also sachlich falsche Auskünfte erteilen, wenn die Anfrage sich auf interne Details bezieht\href{https://en.wikipedia.org/wiki/Retrieval-augmented_generation#:~:text=RAG%20improves%20large%20language%20models,4}{en.wikipedia.org}.
Dieses Risiko ist in professionellen Anwendungsfällen nicht akzeptabel.
\par
Ein typischer Nutzer des geplanten Chatbots ist ein Mitarbeiter im Unternehmen, der regelmäßig auf interne Dokumente zugreifen muss, um seine Aufgaben zu erfüllen.
Beispielsweise könnte dies ein Projektmanager sein, der Informationen zu aktuellen Projektrichtlinien, technischen Spezifikationen oder Berichten benötigt.
Meistens ist dies aber ein Anwendungsberater, der Kundenfragen zu Produkten oder Dienstleistungen beantworten muss.
Zu guter Letzt, in der zweiten Prioritätsstufe, sind es die technischen Berater, die intern auf technische Dokumentationen und Anleitungen zugreifen müssen.
Diese Nutzergruppe profitiert besonders von einem Chatbot, der ihnen schnell präzise Antworten liefert, ohne dass sie lange technische Dokumente für Infor LN oder Infor OS durchsuchen müssen.
Diesen Dokumenten fehlt es heutzutage oftmals an Qualität in Bezug auf Struktur, Formatierung und Verständlichkeit, da sie oft aus anderen Sprachen übersetzt wurden oder von verschiedenen Autoren mit unterschiedlichen Stilen verfasst sind.
Die Nutzer erwarten eine einfache, intuitive Bedienung, ähnlich wie bei modernen Chat-Anwendungen.

\subsubsection{User Stories}\label{subsec:user-stories}
Um dies klar hervorzuheben entstehen im folgenden User Stories, die typische Anwendungsfälle und Anforderungen der Nutzer beschreiben:

\begin{itemize}
    \item \textbf{Als Projektmanager} möchte ich schnell Antworten auf Fragen zu internen Richtlinien und Verfahren erhalten, damit ich meine Projekte effizient planen und durchführen kann.
    \item \textbf{Als Anwendungsberater} möchte ich in der Lage sein, Kundenanfragen zu Produkten und Dienstleistungen zügig zu beantworten, indem ich auf relevante interne Dokumente zugreife.
    \item \textbf{Als technischer Berater} möchte ich technische Anleitungen und Dokumentationen schnell finden können, um Supportanfragen effektiv zu bearbeiten.
\end{itemize}



Die zentrale Problemstellung liegt somit darin, einen Chatbot zu konzipieren, der auf firmeninternes Wissen zurückgreift, ohne dieses Wissen nach außen preiszugeben.
Hierbei treten mehrere Herausforderungen auf.
Erstens muss die Integration der diversen Datenquellen bewältigt werden.
SharePoint und OneDrive enthalten Dateien in unterschiedlichen Formaten (PDF, Word, E-Mails, etc.), die automatisiert ausgelesen werden sollen.
Dies erfordert robuste Mechanismen zur Dateiverarbeitung und -indexierung.
Außerdem sind diese Plattformen oft in komplexe Authentifizierungs- und Berechtigungssysteme eingebunden.
So kann es beispielsweise passieren, dass \textit{automatisierte Zugriffe} von Sicherheitsmechanismen blockiert werden – ein Umstand, der in ersten Tests tatsächlich beobachtet wurde\href{https://alain-airom.medium.com/populating-a-rag-with-data-from-enterprise-documents-repositories-for-generative-ai-4ded82952c67#:~:text=W%20riting%20the%20main%20application,activities%20on%20my%20account}{alain-airom.medium.com}.
Zweitens stellt sich die Datenschutzfrage: Selbst wenn die Daten intern bleiben, muss gewährleistet sein, dass nur berechtigte Informationen im jeweiligen Kontext ausgegeben werden und keine sensiblen Inhalte unkontrolliert verbreitet werden.
Drittens spielt die technische Realisierbarkeit im vorhandenen IT-Umfeld eine Rolle.
Viele KI-Frameworks und -Tools sind primär auf Linux-basierte Umgebungen ausgerichtet, während im Unternehmen möglicherweise ein Windows Server als Plattform vorgesehen ist.
Tatsächlich traten bei Tests auf einem Windows Server einige Einschränkungen auf – gewisse Software-Komponenten funktionierten nur eingeschränkt, und Performance-Probleme mussten adressiert werden (siehe \textit{Technische Umsetzung}).



\subsection{Soll-Zustand}
Der angestrebte Soll-Zustand ist ein datenschutzkonformer RAG-Chatbot, der einige Eigenschaften erfüllen muss.
Es muss auf kommerzieller Hardware innerhalb der Unternehmensinfrastruktur betrieben werden können (On-Premises).
Konnektoren zu SharePoint und OneDrive müssen im besten Fall automatisiert Dokumente einlesen können.
Der Chatbot soll in der Lage sein, natürlichsprachliche Fragen der Nutzer zu verstehen und präzise Antworten zu generieren, indem er relevante Informationen aus den internen Dokumenten extrahiert
Dafür muss der Chatbot in der Lage sein Deutsch und Englisch zu verstehen und auch in beiden Sprachen, unabhängig von der Sprache der Anfrage, zu antworten.
Die Antwortqualität soll so hoch sein, dass die Nutzer den Chatbot als verlässliche Informationsquelle wahrnehmen.
Das System muss sicherstellen, dass keine sensiblen Daten unkontrolliert nach außen gelangen.
Zudem soll der Chatbot in eine benutzerfreundliche Oberfläche integriert werden.

Der Chatbot muss komplett on-premises betrieben werden können, um Datenschutzanforderungen zu erfüllen.

\subsubsection{Abgrenzung zu kommerziellen Cloud-Diensten}\label{subsec:abgrenzung-zu-kommerziellen-cloud-diensten}
Die Entscheidung, einen datenschutzkonformen RAG-Chatbot zu entwickeln, der ausschließlich on-premises betrieben wird, basiert auf mehreren Überlegungen.
Kommerzielle Cloud-Dienste für KI-Modelle bieten zwar eine einfache und skalierbare Lösung, bringen jedoch erhebliche Datenschutz- und Sicherheitsrisiken mit sich.
Insbesondere in regulierten Branchen oder bei sensiblen Unternehmensdaten ist es oft nicht zulässig, diese Daten an externe Cloud-Anbieter zu übermitteln.
Die Kontrolle über die Datenverarbeitung und -speicherung geht verloren, was zu Compliance-Verstößen führen kann.
Zudem besteht das Risiko, dass vertrauliche Informationen ungewollt offengelegt werden.
Man kann ohne Weiteres gegen ein Entgelt sofort auf vielerlei solcher Anbieter zugreifen, wie etwa OpenAI, Azure OpenAI, Google Cloud AI oder Amazon Bedrock.
Diese Dienste bieten leistungsstarke LLMs und einfache APIs, die eine schnelle Integration ermöglichen.
Allerdings sind diese Dienste oft mit hohen laufenden Kosten verbunden, insbesondere bei großem Datenvolumen oder hoher Nutzungsfrequenz.
Für kleinere Unternehmen oder solche mit begrenztem IT-Budget können diese Kosten prohibitiv sein.
Darüber hinaus sind kommerzielle Cloud-Dienste häufig nicht flexibel genug, um spezifische Anforderungen oder Anpassungen zu erfüllen.
Die Nutzer sind auf die Funktionen und Updates des Anbieters angewiesen, was die Innovationsfähigkeit einschränken kann.
Im Gegensatz dazu ermöglicht ein on-premises RAG-Chatbot die vollständige Kontrolle über die Daten und die Infrastruktur.
Unternehmen können sicherstellen, dass alle Datenschutz- und Sicherheitsanforderungen eingehalten werden.
Zudem können sie die Lösung genau auf ihre Bedürfnisse zuschneiden und haben die Freiheit, neue Funktionen oder Modelle zu integrieren, ohne auf einen Drittanbieter angewiesen zu sein.
Es ist auch ein klares Signal an Kunden und Partner, dass das Unternehmen den Schutz sensibler Informationen ernst nimmt.
Zudem ist hier nicht zu vernachlässigen, dass sowohl die persönliche Motivation dieser Masterarbeit als auch die des Unternehmens darin liegt auch selbst innovativ tätig zu werden und nicht nur auf fertige Lösungen von Drittanbietern zurückzugreifen.
Es ist ein klares Ziel, Know-how im Bereich moderner KI-Technologien aufzubauen und langfristig eigene Kompetenzen zu entwickeln.
KI soll nicht nur konsumiert, sondern auch gestaltet werden.
Das soll nicht den Alltagsbetrieb stören, sondern als Kooperator fungieren.
Deswegen wird hier explizit auf kommerzielle Cloud-Dienste verzichtet.
Wenn das Ergebnis dieser Arbeit zeigt, dass ein datenschutzkonformer RAG-Chatbot on-premises machbar ist, könnte dies als Grundlage für weitere Entwicklungen und Innovationen im Unternehmen dienen.
Falls nicht, so liefert es wertvolle Erkenntnisse über die Herausforderungen und Grenzen solcher Ansätze und kann als Entscheidungsgrundlage für zukünftige Investitionen in KI-Technologien dienen.


\subsection{Leistungsanforderungen}
Derzeit existiert keine lokale On-Premise Hardware- oder Software-Infrastruktur für einen datenschutzkonformen RAG-Chatbot im Unternehmen.
Die Mitarbeiter greifen auf SharePoint und OneDrive zu, um Dokumente zu speichern und zu verwalten.
Es gibt jedoch keine automatisierten Mechanismen, um diese Dokumente für einen Chatbot zugänglich zu machen.
Die Mitarbeiter müssen manuell nach Informationen suchen, was zeitaufwendig und ineffizient ist.
Es gibt keine bestehende Chatbot-Lösung, die auf internen Dokumenten basiert.
Die IT-Infrastruktur des Unternehmens ist hauptsächlich Windows-basiert, was die Auswahl und Integration von KI-Tools und -Frameworks einschränkt.
Zudem sind Datenschutz- und Sicherheitsrichtlinien vorhanden, die den Umgang mit sensiblen Daten regeln.
Diese Richtlinien müssen bei der Entwicklung des Chatbots strikt eingehalten werden.

Insgesamt besteht eine deutliche Lücke zwischen dem aktuellen Zustand und dem angestrebten Soll-Zustand, die durch die Entwicklung eines datenschutzkonformen RAG-Chatbots geschlossen werden soll.

Aufgrund dessen wurden sich potentielle Kandidaten für die technische Umsetzung angeschaut und bewertet.

\subsubsection{CPU vs GPU}
Ein weiterer wichtiger Aspekt bei der Auswahl der technischen Umsetzung ist die Frage, ob die KI-Modelle auf CPU- oder GPU-Hardware laufen sollen.
GPU-beschleunigte Systeme bieten in der Regel eine deutlich höhere Leistung bei der Verarbeitung großer Modelle und Datenmengen.
Sie sind besonders vorteilhaft, wenn Echtzeit-Antworten und eine hohe Skalierbarkeit erforderlich sind.
Allerdings sind GPUs oft teurer in der Anschaffung und im Betrieb, was für kleinere Unternehmen eine Hürde darstellen kann.
CPU-basierte Systeme sind hingegen kostengünstiger und einfacher zu warten.
Sie können für kleinere Modelle und weniger intensive Workloads ausreichend sein.
Im Rahmen dieser Arbeit wurde die Entscheidung getroffen, eine CPU-basierte Lösung zu verfolgen, um die Kosten zu minimieren und die Komplexität der Infrastruktur zu reduzieren.
Dies stellt sicher, dass der Chatbot auch auf handelsüblicher Hardware betrieben werden kann, was den Zugang für kleinere Unternehmen erleichtert.
Insgesamt zeigt die Analyse der Ist-Situation und der verfügbaren Tools, dass die Entwicklung eines datenschutzkonformen RAG-Chatbots technisch machbar ist.
Die Wahl der richtigen Tools und Frameworks sowie die Berücksichtigung von Hardware-Anforderungen sind entscheidend, um die angestrebten Ziele zu erreichen.
\par
Es stand bisher nur ein Linux-VPS mit 2 vCPUs und 4 GB RAM zur Verfügung, was für die Ausführung größerer Modelle und die Verarbeitung umfangreicher Dokumente nicht ausreicht.
Dieser geriet schnell auf Grund der fehlenden Hardware-Beschleunigung an seine Grenzen.
VPS-Server haben oftmals, wie auch in diesem Fall, Einschränkungen bei der Installation bestimmter Software-Komponenten sowie der Nutzung der Hardware-Beschleunigung (kein Zugriff auf GPU, eingeschränkte CPU-Leistung).
Daher wurde entschieden, die Entwicklung und das Testen der Anwendung auf einem Windows-Server mit folgenden Spezifikationen durchzuführen:
\par
%
%Typ:Dedicated Server L-16CPU:4 Core x 3.5 GHz (Intel Xeon E3-1230 v6)RAM:16 GBSSD:2 x 480 GB Software RAID 1

\begin{itemize}
    \item \textbf{Prozessor:} Intel Xeon E3-1230 v6, 4 Kerne, 3.5 GHz
    \item \textbf{Arbeitsspeicher:} 16 GB RAM
    \item \textbf{Speicher:} 2 x 480 GB SSD im Software-RAID 1
    \item \textbf{Betriebssystem:} Windows Server
\end{itemize}

Sollte diese Hardwareleistung ausreichen würden damit größere Kosten für Cloud-Server sowie GPUs vermieden werden können.

\subsubsection{Open-Source Tools und Frameworks}
Für die Umsetzung des datenschutzkonformen RAG-Chatbots werden verschiedene Open-Source-Tools und Frameworks evaluiert.
Die Entscheidung Open-Source-Lösungen zu nutzen, basiert auf mehreren Faktoren.
Erstens ermöglichen Open-Source-Tools eine hohe Flexibilität und Anpassungsfähigkeit.
Der Quellcode ist frei zugänglich, was es ermöglicht, die Software an spezifische Anforderungen anzupassen und zu erweitern.
Zweitens entfallen Lizenzkosten, was insbesondere für kleinere Unternehmen von Vorteil ist.
So entfällt auch eine Abhängigkeit von kommerziellen Anbietern, die möglicherweise nicht alle Datenschutzanforderungen erfüllen oder jederzeitige Änderungen in ihren Diensten vornehmen könnten.
Wichtige Kriterien bei der Auswahl waren die Kompatibilität mit der Windows-basierten IT-Infrastruktur, die Fähigkeit zur Verarbeitung von Dokumenten aus SharePoint und OneDrive, die Unterstützung für RAG-Architekturen und die Einhaltung von Datenschutzanforderungen.
Die Evaluierung dieser Tools ist in folgender Tabelle zusammengefasst:

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lp{2cm}p{2.4cm}p{2.2cm}p{1.1cm}p{2.6cm}}
        \toprule
        \textbf{Framework/Tool)} & \textbf{RAG / Zitation} & \textbf{Office \& PDF} & \textbf{Lokale API} & \textbf{Typ. RAM} & \textbf{Wann empfehlenswert} \\
        \midrule
        GPT-4All Desktop & Integriertes RAG, Seiten-Zitate & DOCX / XLSX native; kein OCR & OpenAI-kompatibel (Einstellungen) & \(\sim\)12 GB & Einfache GUI, schnell lauffähig \\
        Open WebUI + Ollama & RAG-Plugin, Quellenangaben & Viele Formate; OCR per Add-on & OpenAI-kompatible Endpunkte & 8–14 GB & Moderne Web-UI mit REST-API \\
        LM Studio & Knowledge-Workspace mit Zitationen & PDF, TXT; Office nachrüstbar & OpenAI-kompatible API (Port 1234) & 10–14 GB & Desktop + LAN-API Kombination \\
        text-generation-webui & File-loader \& Vector-RAG & Office via Add-on; OCR via pytesseract & REST / WebSocket & 12–14 GB & Umfangreiche Plug-in‑Architektur \\
        LocalGPT 2.0 & CLI/UI mit Seiten-Zitaten & PDF / TXT nativ; Office vor Konvertierung & FastAPI (Port 5111) & 4–6 GB & Ressourcenschonend, datenschutzfokussiert \\
        Onyx & RAG-Plugin, optionale Zitationen & PDF, DOCX; OCR optional & OpenAI-kompatible Endpunkte & 8–12 GB & Gut für On\-Premises-Einsatz und flexible Integrationen \\
        \bottomrule
    \end{tabular}
    \caption{Vergleich ausgewählter Open\-Source RAG-Tools}
    \label{tab:rag-tools}
\end{table}

Wie aus der Tabelle ersichtlich bieten verschiedene Tools unterschiedliche Stärken und Schwächen.
GPT-4All Desktop punktet mit einer benutzerfreundlichen GUI und integriertem RAG-Support, während Open WebUI durch seine moderne Web-Oberfläche und REST-API hervorstecht.
LM Studio kombiniert Desktop-Chat mit LAN-API-Funktionalität, was für bestimmte Anwendungsfälle vorteilhaft sein kann.
Text-generation-webui besticht durch seine Plug-in-Architektur, die vielfältige Erweiterungen ermöglicht.
LocalGPT 2.0 ist besonders ressourcenschonend und legt den Fokus auf Datenschutz.
Onyx wiederum ist ein vollumfängliches RAG-Tool, inklusive Konnektoren und Zitationsfunktionen.


Um eine gewichtete Entscheidung zu treffen, wurden die Tools hinsichtlich ihrer Funktionalität, Anpassungsfähigkeit und Benutzerfreundlichkeit bewertet.

\begin{itemize}
    \item \textbf{Funktionalität:} Dieses Kriterium bewertet, wie gut ein Tool die Prinzipien von Retrieval-Augmented Generation (RAG) unterstützt und heterogene Dokumentformate verarbeiten kann. RAG-Systeme binden externe Datenquellen (z. B. PDF-Dokumente, Datenbanken oder Websites) in den LLM-Kontext ein, um Antworten faktisch zu untermauern und aktuell zu halten. Dafür muss ein Tool in der Lage sein, gängige Unternehmensdokumente – etwa Office-Dateien wie Word, Excel oder PowerPoint – zu importieren und zu durchsuchen. Tatsächlich bieten fortgeschrittene Open-Source-Frameworks Konverter für eine Vielzahl von Formaten an (PDF, DOCX, PPTX, XLSX u. a.), sodass sämtliche relevanten Informationen aus Berichten, Tabellen oder Präsentationen in einen einheitlichen Dokumentenindex überführt werden können. Fehlt eine solche Funktionalität, blieben wichtige Wissensquellen unerschlossen, was die Qualität der Antworten und den Nutzen der RAG-Architektur erheblich schmälern würde.
    \item \textbf{Anpassungsfähigkeit:} Hierunter fällt die Flexibilität des Tools, sich an spezifische Anforderungen anzupassen – etwa durch modulare Erweiterungen, Plug-ins oder API-Integrationen. In einer wissenschaftlichen Betrachtung von LLM-Architekturen wird Customizability als wichtiges Qualitätsmerkmal genannt: Die Möglichkeit also, domänenspezifische Anpassungen vorzunehmen, ohne die Grundstruktur zu verändern. Dies kann durch eigene Prompt-Vorlagen, das Feintuning von Modellen oder das Einbinden zusätzlicher Komponenten geschehen. Gleichzeitig ist Interoperabilität gefordert – also eine nahtlose Einbindung des Tools in bestehende Datenbanken, Softwareumgebungen und Workflows. Nur eine offene, erweiterbare Architektur verhindert Integrationshürden im praktischen Einsatz. Ein Beispiel ist das Framework LangChain, das durch seinen modularen Aufbau die Verknüpfung verschiedener Vektor-Datenbanken, externer LLM-APIs und Datenquellen erlaubt und so eine hohe Anpassungsfähigkeit demonstriert. Insgesamt stellt Anpassungsfähigkeit sicher, dass der Chatbot präzise auf die jeweilige Organisationsumgebung zugeschnitten werden kann, anstatt umgekehrt die Prozesse an die Limitierungen des Tools anpassen zu müssen.
    \item \textbf{Benutzerfreundlichkeit:} Die Usability eines RAG-gestützten Chatbot-Systems ist sowohl für Entwickler (bei Einrichtung und Wartung) als auch für Endnutzer (bei der täglichen Interaktion) entscheidend. Eine intuitive Bedienung und klare Nutzeroberfläche erhöhen die Akzeptanz und reduzieren Fehler bei der Anwendung. Empirische Leitlinien betonen, dass die Benutzerschnittstelle möglichst nahtlos und intuitiv gestaltet sein sollte, sodass die komplexe KI-Logik nach außen hin verborgen bleibt. Wichtig ist auch, Mechanismen für Nutzer-Feedback und Korrekturmöglichkeiten bereitzustellen, damit Anwender die Kontrolle über automatisierte Antworten behalten. Aus Entwicklersicht fördert eine gute Dokumentation und ein konsistentes API-Design die Benutzerfreundlichkeit, da Integrationen schneller umgesetzt werden können. Ein praktisches Beispiel für gelungene Usability ist die Integration eines Code-Assistenten direkt in die Entwicklungsumgebung (analog zu GitHub Copilot), was zeigt, wie eine KI-Anwendung durch Einbettung in bestehende Tools die Benutzerfreundlichkeit und Produktivität steigert. Insgesamt gilt: Je leichter ein System zu bedienen ist, desto eher wird es im Alltag akzeptiert und korrekt genutzt.
    \item \textbf{Ressourcenbedarf:} RAG-Anwendungen kombinieren rechenintensive Operationen – wie das Einbetten von Dokumenten in Vektoren und das Generieren von Antworten durch große Sprachmodelle – mit potenziell umfangreichen Datenbeständen. Daher ist der typische Ressourcenbedarf (Arbeitsspeicher, Prozessor/GPU-Auslastung, Speicherplatz) ein wichtiges Kriterium. Besonders in On-Premises-Umgebungen sind die verfügbaren Hardware-Ressourcen beschränkt, sodass effiziente Algorithmen und eine sparsame Nutzung von Speicher entscheidend sind. In der Literatur wird betont, dass optimierte Architekturen (etwa durch Caching von Teilergebnissen, Parallelisierung von Aufgaben und Hardware-Beschleunigung) notwendig sind, um trotz großer Datenmengen niedrige Latenzen und eine effiziente Ressourcenauslastung sicherzustellen. Ein gut bewertetes Tool sollte also auch bei wachsender Anzahl von Dokumenten und Anfragen skalieren, ohne unverhältnismäßig mehr Speicher oder Rechenzeit zu beanspruchen. Dies beinhaltet z. B. einen effizienten Vektorsuch-Index und ggf. Mechanismen, unwichtige Daten früh auszufiltern, damit die teuren LLM-Abfragen nur auf wirklich relevante Kontexte angewendet werden. Ein geringer Ressourcenverbrauch ist nicht nur ökonomisch vorteilhaft, sondern oft die Voraussetzung, um die Software überhaupt auf bestehender Firmen-Hardware betreiben zu können.
    \item \textbf{Datenschutzkonformität:} Für den Einsatz in einer On-Premises-Umgebung – insbesondere bei sensiblen Firmendaten – muss das Tool strenge Datenschutz- und Compliance-Anforderungen erfüllen. Es darf keine vertraulichen Informationen unkontrolliert nach außen gelangen und muss mit geltenden Datenschutzgesetzen (wie der DSGVO) im Einklang stehen. Security-by-Design ist hier zentral: So sollten Mechanismen wie Eingabe- und Ausgabevalidierung, Verschlüsselung der gespeicherten Daten sowie abgesicherte Kommunikationskanäle Standard sein, um Risiken wie unbefugten Zugriff oder Datenlecks vorzubeugen. Zudem warnen Sicherheitsexperten vor spezifischen LLM-Gefahren wie z. B. Prompt Injection – hier muss das System Vorkehrungen treffen, etwa durch Filter für Benutzerinputs und strikte Rollenbeschränkungen, damit solche Angriffe ins Leere laufen. Ein datenschutzkonformes RAG-Tool sollte möglichst so konzipiert sein, dass sämtliche Verarbeitung lokal erfolgt. Techniken wie föderiertes Lernen zeigen Wege auf, Modelle mit dezentralen Daten zu nutzen, sodass persönliche Informationen die geschützte Umgebung gar nicht verlassen. Die Einhaltung von Datenschutz und IT-Compliance ist somit kein optionaler Luxus, sondern Voraussetzung für den produktiven Einsatz eines solchen Systems in vielen Branchen (z. B. Gesundheitswesen, Finanzsektor oder öffentlicher Dienst).
    \item \textbf{Community und Support:} Die Vitalität der Entwickler-Community und die Verfügbarkeit von Support sind wesentliche Erfolgfaktoren bei Open-Source-Software. Ein Blick auf Community-Metriken – etwa Anzahl der GitHub-Sterne, Häufigkeit von Commits, aktive Foren oder Discord/Slack-Kanäle – gibt Aufschluss darüber, wie engagiert und groß die Nutzer- und Entwicklerbasis eines Tools ist. Eine aktive Community bedeutet in der Regel: schnelle Behebung von Fehlern, regelmäßige Updates und viele verfügbare Erweiterungen oder Tutorials. In wissenschaftlichen Entscheidungskriterien wird daher die “Community Health” eines Frameworks explizit als Kriterium genannt. Auch die Qualität der Dokumentation spielt hier mit hinein – ein gut dokumentiertes Tool (ggf. mit praxisnahen Beispielen und Tutorials) erleichtert neuen Nutzern den Einstieg und reduziert den Entwicklungsaufwand. Kommerzieller Support (etwa in Form von Enterprise-SLAs oder Hilfe durch die Firmen hinter einem Open-Source-Projekt) kann ebenfalls relevant sein, insbesondere wenn das System geschäftskritisch eingesetzt wird. Letztlich erhöht eine starke Community & Support-Struktur die Nachhaltigkeit des Tools: Es kann langfristig gepflegt und bei Bedarf an neue Anforderungen angepasst werden, ohne dass man als einzelner Nutzer auf sich allein gestellt ist.
    \item \textbf{Integration in bestehende Systeme:} Ein KI-Assistenzsystem entfaltet nur dann vollen Nutzen, wenn es sich nahtlos in die bestehende IT-Infrastruktur eines Unternehmens einfügt. Daher wurde bewertet, wie gut das Tool in vorhandene Systeme integrierbar ist. Dies umfasst die Fähigkeit, an bestehende Datenquellen (Datenbanken, Dateiablagen, Intranet-Seiten etc.) anzudocken, vorhandene Authentifizierungs- und Autorisierungssysteme (z. B. Single Sign-On) zu nutzen sowie Ausgaben an andere Anwendungen weiterzureichen. Laut aktueller Literatur sind Interoperabilität und Anschlussfähigkeit kritische Erfolgsfaktoren: Eine Architektur, die LLM-Funktionen über standardisierte Schnittstellen (z. B. REST APIs, SDKs) bereitstellt, vermeidet Adoptionsbarrieren und erleichtert die Einbettung in bestehende Arbeitsabläufe. Beispielsweise kann ein gutes RAG-Tool als Microservice laufen, der über ein API abgefragt wird, wodurch Frontend-Anwendungen (wie ein Chat-Frontend oder ein Firmenportal) unkompliziert angebunden werden können. Je reibungsloser die Integration, desto geringer der Aufwand bei der Einführung – insbesondere verglichen mit Insellösungen, die separate Infrastruktur erfordern. In der Bewertung wurde also positiv gewertet, wenn ein Tool bekannte Protokolle und Formate unterstützt (z. B. Output als JSON, Konnektoren für SharePoint/Dateiserver etc.), da dies den Implementierungsaufwand reduziert.
    \item \textbf{Zukunftssicherheit:} Angesichts der dynamischen Entwicklungen im Bereich Large Language Models ist die Zukunftssicherheit einer Lösung ein wesentliches Kriterium. Damit ist gemeint, ob das Tool voraussichtlich mit zukünftigen Anforderungen und technischen Fortschritten Schritt halten kann. Der Markt für RAG-Frameworks wächst derzeit mit ~44\% jährlich, was verdeutlicht, dass ständig neue Modelle, Vektor-Datenbanken oder Optimierungsmethoden erscheinen. Ein zukunftssicheres Tool zeichnet sich dadurch aus, dass es regelmäßig Updates und neue Features erhält und eine klare Roadmap besitzt. Im Open-Source-Kontext bedeutet das oft: Eine große Entwicklergemeinschaft oder ein unterstützendes Unternehmen sorgt fortlaufend für Weiterentwicklung. So integriert z. B. ein aktives Projekt kurz nach Erscheinen neuer vielversprechender Modelle (wie GPT-4 oder Llama 2) deren Unterstützung, während ein weniger gepflegtes Tool irgendwann stagniert. Zudem sollte die Architektur flexibel genug sein, um neue Module (etwa bessere Embeddings-Modelle oder veränderte Prompting-Techniken) aufzunehmen, ohne von Grund auf neu entwickelt werden zu müssen. Zukunftssicherheit umfasst auch die Frage der langfristigen Wartbarkeit: Ein klar strukturiertes, modularisiertes System kann eher über Jahre erhalten und verbessert werden als ein monolithisches. Dieses Kriterium schützt die Investition, indem es wahrscheinlich macht, dass das gewählte Tool auch in einigen Jahren noch State-of-the-Art-Anforderungen erfüllen kann, anstatt veraltet zu sein.
    \item \textbf{Kosten:} Open-Source-Tools sind zwar lizenzkostenfrei verfügbar, doch im professionellen Einsatz entstehen Kosten auf verschiedenen Ebenen. In der Analyse wurde daher betrachtet, welche Aufwände der Betrieb des jeweiligen Tools verursacht – und wie diese im Vergleich zu kommerziellen Alternativen stehen. Ein Vorteil vieler Open-Source-RAG-Frameworks ist die fehlende Lizenzgebühr: Typischerweise stehen sie unter permissiven Lizenzen (z. B. Apache 2.0 oder MIT), was eine kommerzielle Nutzung ohne direkte Softwarekosten erlaubt. Allerdings dürfen die Infrastrukturkosten nicht übersehen werden: Die Notwendigkeit eigener Server (oder Cloud-Ressourcen), Speicher für den Vektorspeicher sowie ausreichend GPU-Leistung für Inferenz kann erheblich zu Buche schlagen. Beispielsweise können beim Betrieb eines Vektorindex in der Cloud Gebühren von etwa \$0.05–\$0.12 pro einer Million Vektoren anfallen. Kommerzielle Lösungen wiederum verlagern diese Kosten oft in Nutzungsentgelte (etwa pro 1000 API-Calls), was abhängig vom Anfragevolumen teuer werden kann. Auch indirekte Kosten wie der Aufwand für Wartung und Updates (bei Open-Source intern zu stemmen, bei kommerziellen Produkten teils im Preis inbegriffen) wurden in die Bewertung einbezogen. Insgesamt musste das Kosten-Nutzen-Verhältnis stimmen: Ein Open-Source-Tool rechtfertigt sich dann, wenn die eingesparten Lizenzkosten die betrieblichen Aufwendungen ausgleichen und es gegenüber einer kommerziellen Lösung langfristig wirtschaftlich bleibt. Andernfalls könnte eine kommerzielle All-in-one-Lösung trotz Lizenzgebühr günstiger sein, etwa wenn sie dafür niedrigere Infrastruktur- oder Personalkosten verursacht.
    \item \textbf{Skalierbarkeit:} Dieses Kriterium untersucht, wie gut das System mit wachsenden Anforderungen mitwächst. In einem Enterprise-Szenario können sowohl die Anzahl der Nutzeranfragen (z. B. parallel arbeitende Benutzer oder Chatbot-Sessions) als auch das Volumen der zu durchsuchenden Dokumente stark ansteigen. Ein skalierbares Tool sollte durch geeignete Architektur darauf vorbereitet sein – etwa indem es Lastverteilung über mehrere Knoten unterstützt oder effizient mit großen Datenbanken interagiert. Forschungsergebnisse zeigen, dass im LLM-Kontext horizontale Skalierung (z. B. das Clustern von Modellen und Datenbanken) nötig ist, um sehr hohe Anfragevolumina zu bewältigen; OpenAI etwa nutzt verteilte Systeme, um Millionen von Requests pro Tag mit GPT-4 verarbeiten zu können. Ähnliches gilt für den Vektor-Index: Bei Millionen von Dokumenten muss die Einfügung neuer Vektoren und die Ähnlichkeitssuche immer noch performant bleiben. Das Kriterium Skalierbarkeit bewertet demnach positiv, wenn ein Tool bereits für Enterprise-Größenordnungen ausgelegt ist – z. B. durch Unterstützung von Kubernetes/Docker-Deployment, Anbindung an skalierbare Cloud-Dienste (wie verteilte Vektordatenbanken) oder bewährte Optimierungen für große Datenmengen. Ein skalierbares System kann mit den Anforderungen wachsen, ohne dass ein kompletter Plattformwechsel nötig wird, sobald mehr Benutzer oder Daten hinzukommen.
    \item \textbf{Performance:} Eng verknüpft mit Skalierbarkeit ist die Performance (Leistungsfähigkeit) des Systems, insbesondere die Antwortzeit auf Nutzeranfragen. Gerade bei großen Dokumentenbeständen ist es wichtig, dass der Chatbot dennoch in Echtzeit oder zumindest wenigen Sekunden sinnvolle Antworten liefert. Nutzerakzeptanzstudien legen nahe, dass zu lange Wartezeiten die Gebrauchstauglichkeit erheblich mindern – daher sind performante Antwortzeiten ein Muss. Technisch wird dies durch optimierte Pipeline-Schritte erreicht: Eine Quelle betont den Einsatz von Caching, Parallelisierung und Hardware-Beschleunigung, um niedrige Latenzen sicherzustellen. Zudem muss die Vektor-Suche in großen Indizes effizient sein; ein häufiger Stolperstein ist es, die Latenz von Vektor-Datenbanken bei sehr vielen Dokumenten zu unterschätzen. Die Bewertung berücksichtigte daher, ob das Tool bekannte Performance-Techniken implementiert (z. B. Approximate Nearest Neighbor Search, Index-Kompression, Vorberechnung von Embeddings) und wie es in Tests auf große Datenmengen reagiert. Ein performantes System liefert konsistente Antwortzeiten, was für die Benutzerzufriedenheit entscheidend ist – insbesondere, wenn der Chatbot produktiv von vielen Nutzern gleichzeitig befragt wird. Schließlich spiegelt Performance auch die Effizienz der KI-Antwortgenerierung wider: Durch cleveres Prompt-Design (etwa Begrenzung des eingefügten Kontexts auf hochrelevante Ausschnitte) kann die Generationszeit und -qualität ebenfalls optimiert werden.
    \item \textbf{Sicherheitsfunktionen:} Neben Datenschutz im engeren Sinne (s. o.) sind weitere Sicherheitsaspekte zu beachten, insbesondere Zugriffs- und Betriebssicherheit. Ein robustes Tool bietet eingebaute Mechanismen, um sensible Daten zu schützen und unbefugten Zugriff zu verhindern. Dazu zählt zunächst ein Rollen- und Rechtesystem (Benutzerverwaltung), das sicherstellt, dass nur autorisierte Personen bestimmte Informationen abfragen können – etwa umgesetzt via Role-Based Access Control (RBAC) in einigen Enterprise-RAG-Frameworks. Weiterhin ist eine sichere Authentifizierung und Autorisierung beim Zugriff auf den Chatbot oder dessen Admin-Oberfläche essenziell. Moderne Implementierungen integrieren häufig OAuth2 oder JWT-basierte Authentifizierung, um den Zugang abzusichern. Darüber hinaus sollten Protokollierung und Intrusion Detection vorhanden sein, um verdächtige Aktivitäten zu erkennen. Schutz gegen spezifische LLM-Angriffe (wie bereits erwähnt Prompt Injection oder auch Data Poisoning des Dokumentenbestands) wird in neueren Architekturempfehlungen hervorgehoben – z. B. diskutiert OWASP entsprechende Sicherheitsrichtlinien für LLMs. Ein gutes RAG-Tool wird deshalb mit Guardrails geliefert oder ist kompatibel mit Zusatztools, die z. B. die Einhaltung bestimmter Antwort-Richtlinien erzwingen und Ausgaben filtern. Die Sicherheitsfunktionen eines solchen Systems sind insgesamt auf mehreren Ebenen verortet: von der Netzwerk-/Infrastruktur-Sicherheit (Verschlüsselung, Firewalling) über Anwendungssicherheit (Eingabefilter, Zugriffskontrollen) bis hin zur ML-spezifischen Sicherheit (Schutz vor Manipulation der KI). Ein hochbewertetes Tool deckt diese Bereiche zumindest dem Grunde nach ab oder lässt sich leicht mit bestehenden Sicherheitslösungen integrieren.
    \item \textbf{Flexibilität bei der Modellauswahl:} Dieses Kriterium betrachtet, inwieweit verschiedene Large-Language-Models in das System eingebunden werden können. Da sich Anforderungen und Rahmenbedingungen ändern können – etwa bezüglich Kosten, Performance oder Lizenzierung –, ist es vorteilhaft, wenn ein RAG-Tool nicht auf ein bestimmtes KI-Modell festgeschrieben ist. Modellagnostische Frameworks ermöglichen es, wahlweise Open-Source-Modelle oder kommerzielle APIs anzuschließen. In der Praxis bedeutet das z. B., dass ein Unternehmen zunächst auf lokale Modelle (wie Llama 2) setzt, bei steigendem Anspruch aber ohne großen Aufwand zu einem stärkeren Cloud-Modell wechseln kann. Viele der aktuellen Open-Source-RAG-Frameworks werben explizit mit dieser Flexibilität: Sie integrieren sich gleichermaßen mit APIs von OpenAI, Cohere, Anthropic und selbstgehosteten Modellen, sodass stets das am besten geeignete Modell verwendet werden kann. Diese Flexibilität in der Modellauswahl erlaubt auch Hybridansätze – etwa leichte Anfragen an ein günstigeres Modell zu schicken, komplexe aber an ein hochqualitatives Modell. Im wissenschaftlichen bzw. industriellen Kontext reduziert eine solche Offenheit zudem die Gefahr von Vendor Lock-in: Man bleibt nicht an einen einzelnen Anbieter gebunden, sondern kann die KI-Komponente austauschen, falls ein alternatives Modell bessere Ergebnisse liefert oder regulatorisch bevorzugt ist. Bei der Bewertung der Tools floss positiv ein, wenn Schnittstellen zu mehreren Modelltypen vorhanden waren (z. B. HuggingFace-Integration, Unterstützung von Modell-Hubs oder Multi-Backend-Support). Damit ist das System zukunfts- und anpassungsfähig gegenüber der schnellen Evolution im LLM-Bereich.
    \item \textbf{Einfache Wartung:} Die Wartungsfreundlichkeit eines Systems bestimmt, wie aufwendig es ist, den Betrieb über längere Zeit aufrechtzuerhalten und Änderungen vorzunehmen. In produktiven Umgebungen ändern sich Anforderungen: neue Dokumente müssen hinzugefügt, veraltete entfernt, Sicherheitsupdates eingespielt oder die KI-Modelle verbessert werden. Ein wartungsarmes Tool zeichnet sich zunächst durch eine klare Struktur und Modularität aus. Wenn einzelne Komponenten (Datenbank, Vektorsuche, LLM-Modul) getrennt und austauschbar sind, können Updates oder Fehlerbehebungen gezielt dort ansetzen, ohne das Gesamtsystem zu gefährden. Die Literatur zu LLM-Architekturen betont, dass Modularität Updates vereinfacht und die Skalierbarkeit erhöht, indem Funktionen in wiederverwendbare Dienste aufgeteilt werden. So kann z. B. ein Wechsel des Embedding-Modells oder ein Upgrade auf einen neuen Vektorindex erfolgen, indem nur die betreffende Komponente ersetzt wird, während der Rest der Pipeline unberührt bleibt. Auch automatische Tests und Monitoring (siehe nächster Punkt) tragen zur Wartungsfreundlichkeit bei, da Probleme früh erkannt und lokalisiert werden können. Schließlich spielt die Qualität der Dokumentation wieder eine Rolle: Gut gewartete Projekte haben häufig ausführliche Release Notes, Migrationsanleitungen und aktive Communities, die bei Problemen helfen. Ein einfach wartbares System minimiert die Total Cost of Ownership, da weniger manuelle Eingriffe und geringere Ausfallzeiten anfallen. In der gewichteten Entscheidung wurde also positiv berücksichtigt, wenn ein Tool als ausgereift und unkompliziert im Betrieb beschrieben wurde – etwa durch Erwähnung von Docker/Kubernetes-Support, Logging-Funktionen für den Betrieb oder „One-click“-Upgradeprozessen.
    \item \textbf{Benutzerverwaltung:} In Mehrnutzerumgebungen braucht es Mechanismen, um verschiedene Benutzer und Rechte zu verwalten. Ein professionelles RAG-System sollte daher eine Benutzer- und Rollenverwaltung mitbringen. Dies ermöglicht es, feingranular zu steuern, welcher Nutzer welche Aktionen durchführen darf und auf welche Dokumente er zugreifen kann. Beispielsweise könnte man definieren, dass nur Mitarbeiter der Rechtsabteilung auf juristische Dokumente zugreifen dürfen, während andere Nutzer davon ausgeschlossen sind. Solche Features sind nicht nur aus Sicherheitsgründen relevant, sondern oft auch aus rechtlichen (Stichwort: Need-to-know-Prinzip bei vertraulichen Informationen). Einige Open-Source-Tools haben bereits integrierte Lösungen: Kernel Memory (KM) etwa implementiert eine rollenbasierte Zugriffskontrolle, um den Zugriff auf Daten konform mit Unternehmensrichtlinien zu regeln. Dabei werden Datenzugriffe protokolliert und technisch erzwingt das System, dass nur authentifizierte Nutzer mit gültigen Rechten Antworten auf bestimmte Inhalte erhalten. Neben der Zugriffskontrolle gehört zur Benutzerverwaltung oft auch die Integration mit bestehenden Verzeichnisdiensten (LDAP, Active Directory) – so etwas wurde in der Bewertung ebenfalls positiv gesehen, da Unternehmen in der Regel ein zentrales Identity Management bevorzugen. Insgesamt gewährleistet eine robuste Benutzerverwaltung, dass der Einsatz des Chatbot-Systems skalierbar bleibt: Auch hunderte Benutzer lassen sich übersichtlich managen, und das System erfüllt Anforderungen an Auditierbarkeit (Nachvollziehbarkeit, wer was angefragt hat) und Compliance (Einhaltung von Berechtigungsstrukturen).
    \item \textbf{Dokumentenmanagement:} Ein RAG-System ist nur so gut wie die Wissensbasis, auf die es zugreift. Daher war ein weiterer Bewertungsaspekt, wie gut das Tool das Management der eingebundenen Dokumente unterstützt. In Unternehmenskontexten ändern sich Dokumentbestände ständig – es kommen neue Dateien hinzu, bestehende werden aktualisiert, manche Informationen veralten und müssen ersetzt oder entfernt werden. Ein effektives Dokumentenmanagement innerhalb des RAG-Tools ermöglicht es, solche Änderungen einfach durchzuführen. Idealerweise verfügt das System über Funktionen zum Hinzufügen, Aktualisieren und Löschen von Dokumenten und indexiert neue Daten automatisch. Dadurch bleibt der Knowledge-Base-Inhalt stets aktuell. Die Wichtigkeit dessen zeigt sich darin, dass RAG-Systeme ihre Stärke aus aktueller Evidenz ziehen – wird ein neues Dokument eingebunden, steht dessen Wissen sofort für die Antworten bereit. Umgekehrt würden ohne gutes Dokumentenmanagement die Antworten mit der Zeit veralten oder es müsste erheblicher manueller Aufwand betrieben werden, um die Wissensbasis aktuell zu halten. Bewertet wurde also z. B., ob ein Tool eine grafische Oberfläche oder API für Dokumentenimporte bietet, wie es mit Dubletten und Versionierung umgeht und ob Metadaten (wie Dokumenttyp, Erstellungsdatum, Zugriffsrechte) verwaltet werden können. Auch die Unterstützung einer Vielzahl von Dokumentformaten (siehe Funktionalität) spielt hier hinein – denn ein breites Spektrum an Formatkonvertern vereinfacht das Einpflegen neuer Quellen. Insgesamt erhöht ein gutes Dokumentenmanagement die Zuverlässigkeit der RAG-Lösung, da die Antworten stets auf dem aktuellen, kuratierten Wissensstand basieren.
    \item \textbf{Mehrsprachigkeit:} In einem mehrsprachigen Land wie Deutschland – und generell in international agierenden Unternehmen – ist die Fähigkeit des Tools, verschiedene Sprachen zu verstehen und zu verarbeiten, von großer Bedeutung. Konkret bedeutet Mehrsprachigkeit, dass der Chatbot sowohl Anfragen auf Deutsch (bzw. anderen Sprachen) korrekt interpretieren kann, als auch Dokumente in verschiedenen Sprachen durchsuchen und als Kontext einbeziehen kann. Einige RAG-Frameworks sind primär für Englisch optimiert, was dazu führen kann, dass die Leistung bei deutschsprachigen Eingaben geringer ist. Studien zeigen beispielsweise, dass mehrsprachige Embedding-Modelle auf Nicht-Englisch oft geringere Genauigkeit erreichen – für Deutsch wurde dies explizit beobachtet. Ein weiteres bekanntes Phänomen ist der sogenannte Output Language Drift in mehrsprachigen RAG-Systemen: Wenn z. B. die Nutzerfrage auf Deutsch gestellt wird, aber hauptsächlich englische Dokumente im Index liegen, neigen manche Modelle dazu, die Antwort unerwünschterweise auf Englisch zu geben. Solche Inkonsistenzen gilt es zu vermeiden. Ein gutes Tool unterstützt daher multilinguale Pipelines, etwa indem es mehrsprachige Vektordatenbanken nutzt oder on-the-fly-Übersetzungen von Fragen und Kontexten ermöglicht. Wichtig ist zudem, dass das zugrundeliegende LLM entweder mehrsprachig trainiert ist oder austauschbar (siehe Modellauswahl) – damit man ein deutschkompetentes Modell verwenden kann. In der Bewertung wirkten sich vorhandene Features wie Language Detection, separate Indexe je Sprache oder die Erwähnung der Unterstützung von Deutsch positiv aus. Mehrsprachigkeit ist letztlich ein Muss, wenn der Chatbot in einem Umfeld eingesetzt wird, in dem nicht alle Informationen und Fragen in einer Sprache vorliegen; sie trägt wesentlich zur Universalität und Nutzbarkeit des Systems bei.
    \item \textbf{Offline-Fähigkeit:} Dieses Kriterium prüft, ob das Tool vollständig ohne Internetverbindung betrieben werden kann. Für viele Unternehmen – gerade mit hohen Sicherheitsanforderungen – ist es obligatorisch, dass keine Daten an externe Server geschickt werden und die gesamte Verarbeitung on-premises erfolgt. Eine ausgeprägte Offline-Fähigkeit bedeutet, dass sämtliche Komponenten (vom Vektorenspeicher bis zum LLM) lokal installiert werden können und keine Abhängigkeiten von Cloud-Diensten bestehen. Einige Anbieter von LLM-Technologie haben erkannt, dass Kunden aus dem Bankensektor, dem Gesundheitswesen oder dem behördlichen Umfeld solche Anforderungen haben, und bieten daher on-premise-Optionen an. Cohere beispielsweise ermöglicht den Betrieb seiner Modelle in der eigenen Infrastruktur oder privaten Cloud, um datenschutzkonforme Lösungen umzusetzen. In unserem Kontext (Auswahl eines Open-Source-RAG-Tools) wurde positiv bewertet, wenn das System standardmäßig offline läuft oder zumindest die Option bietet, statt einer externen API ein lokales Modell einzubinden. Außerdem wichtig: Das Tool sollte ohne ständige Internetzugriffe auskommen – etwa sollten etwaige Telemetrie-Funktionen abschaltbar sein. Ein weiterer Aspekt ist die Aktualisierbarkeit in Offline-Umgebungen (Stichwort Updates per Offline-Pakete). Die Offline-Fähigkeit trägt nicht nur zum Datenschutz bei, sondern erhöht auch die Ausfallsicherheit: Das System ist unabhängig von Cloud-Ausfällen oder -Latenzen. Letztlich ermöglicht ein vollständig offline betreibbares Tool den Einsatz in isolierten Netzwerken und erfüllt höchste Ansprüche an die Kontrolle über die Daten.
    \item \textbf{Anpassbare Antwortformate:} Je nach Anwendungsszenario kann es erforderlich sein, dass der Chatbot Antworten in einem bestimmten Format oder Stil liefert. In einer Wissensdatenbank könnten z. B. Markdown oder HTML-Formatierungen erwartet sein; in anderen Fällen sollen Antworten als JSON-Struktur zurückgegeben werden, damit sie von einem weiteren System weiterverarbeitet werden können. Die Flexibilität bei den Antwortformaten ist daher ein Bewertungspunkt. Von fortgeschrittenen LLM-Integrationen wird erwartet, dass sie entweder durch Prompting-Techniken oder durch Nachverarbeitung die Ausgaben in gewünschter Form ausgeben können. Guardrail-Ansätze für LLMs zeigen, dass man Modelle zwingen kann, sich an Vorgaben (z. B. eine JSON-Schema) zu halten, indem man Regeln im Generierungsprozess durchsetzt. Einige RAG-Frameworks bieten sogar eingebaute Features: So lassen sich beispielsweise bei RAGatouille eigene Prompt-Templates definieren, um den Stil der Antworten zu steuern. Dies kann genutzt werden, um z. B. immer in vollständigen Sätzen zu antworten, Bullet-Point-Listen zu erstellen oder bestimmte Fachterminologie zu verwenden. Die Möglichkeit, Antworten formatmäßig anzupassen, ist besonders in Unternehmenskontexten relevant, wo der Output evtl. in Berichte einfließen oder gewissen Formvorgaben genügen muss. In der gewichteten Entscheidung wurde daher positiv berücksichtigt, wenn das Tool entweder native Unterstützung für strukturierte Ausgabe besaß oder aber bekanntermaßen gut mit entsprechenden Prompt-Techniken harmoniert (etwa durch Unterstützung von Plugins wie Microsoft Guidance oder Guardrails für präzise Ausgabeformate).
    \item \textbf{Logging und Monitoring:} Um einen Chatbot im Unternehmenseinsatz betreiben zu können, sind Überwachungs- und Protokollierungsfunktionen unabdingbar. Logging bedeutet, dass alle Anfragen, die vom Nutzer gestellt werden, sowie die daraufhin abgerufenen Dokumente und generierten Antworten aufgezeichnet werden. Dies ist wichtig aus Debugging-Sicht (bei falschen oder problematischen Antworten muss nachverfolgbar sein, auf welcher Grundlage sie entstanden) und aus Compliance-Sicht (Nachweis, was der Assistent wann ausgegeben hat). Best Practices betonen, dass man ohne Logging im Fehlerfall im Dunkeln tappt – es sollte daher der gesamte RAG-Flow protokolliert werden, inkl. Nutzerprompt, gefundener Kontextdokumente und letztlichem Modell-Output. Monitoring geht noch einen Schritt weiter und umfasst das laufende Überwachen von Systemkennzahlen (Antwortzeiten, Auslastung) sowie der inhaltlichen Qualität. Ein gutes Tool stellt Metriken oder Dashboards bereit, um z. B. die Nutzungshäufigkeit, die Einhaltung von Antwortzeiten oder auftretende Fehler zu verfolgen. In einer Referenzarchitektur für LLM-Systeme werden explizit Monitoring-Komponenten vorgeschlagen, die über alle Schichten verteilt Metriken sammeln und Feedback-Schleifen ermöglichen. Solche Komponenten können etwa die Frequenz bestimmter Warnmeldungen (wie zensierter Inhalte) beobachten oder die Zufriedenheitsbewertung der Nutzer erfassen. Überdies hilft intensives Monitoring, problematische Outputs zu erkennen – beispielsweise könnten Logs ausgewertet werden, um Anzeichen von Bias oder unangemessenen Inhalten zu entdecken. Insgesamt trägt Logging & Monitoring zur kontinuierlichen Verbesserung des Systems bei (durch Auswertung der Protokolle kann man die Retrieval- oder Antwortqualität iterativ optimieren) und ist oft auch aus Revisionsgründen vorgeschrieben. In der Bewertung wurden daher Tools bevorzugt, die von Haus aus Logging-/Monitoring-Funktionen mitbringen oder zumindest einfache Anbindungen an vorhandene Monitoring-Infrastruktur (z. B. Elastic Stack, Prometheus) ermöglichen.
\end{itemize}

\noindent
All diese Punkte flossen mit unterschiedlicher Gewichtung in die Entscheidungsfindung ein. Jeder Aspekt adressiert bestimmte Anforderungen der Problemstellung – von der reinen technischen Machbarkeit (Funktionalität) über Betriebsaspekte (Ressourcen, Wartung, Sicherheit) bis hin zur Benutzerakzeptanz und Regulatorik. Durch die ganzheitliche Bewertung entlang dieser Kriterien konnte eine fundierte Auswahl des geeigneten Tools getroffen werden, die sowohl den fachlichen Anforderungen als auch den praktischen Randbedingungen gerecht wird.


All diese Punkte flossen in die Entscheidungsfindung
Die Auswahl des geeigneten Tools hängt maßgeblich von den zu folgenden Tests und Anforderungen ab.
GPT-4All fällt durch seine fehlenden OCR-Fähigkeiten für gescannte Dokumente und die eingeschränkte API-Integration aus.
Im Grunde sind alle Tools in der Lage, RAG-Architekturen zu unterstützen und Dokumente aus Office-Formaten zu verarbeiten.
Der Mehraufwand, das Design und die Funktionalität der Tools an die eigenen Bedürfnisse anzupassen, variiert jedoch stark.
Basierend auf diesen Überlegungen wurde entschieden, Open WebUI oder Onyx in Kombination mit Ollama für die prototypische Umsetzung zu verwenden.
Beide Tools bieten eine gute Balance zwischen Funktionalität, Anpassungsfähigkeit und Benutzerfreundlichkeit und erinnern in ihrem Design an bekannte Chat-Interfaces wie ChatGPT.


\sub{Kommunikationsschnittstellen}\label{subsec:kommunikationsschnittstellen}
Für die Interaktion mit dem RAG-Chatbot sind verschiedene Kommunikationsschnittstellen denkbar.
Eine Web-basierte Oberfläche bietet den Vorteil der plattformunabhängigen Nutzung über verschiedene Endgeräte (z.B. Desktop, Laptop, Tablet).
Alternativ könnte eine Integration in bestehende Kollaborationsplattformen wie Microsoft Teams erfolgen, was den Zugang für Mitarbeiter erleichtert.
Zudem sind APIs denkbar, die eine Anbindung an andere Systeme oder Anwendungen ermöglichen.
Im Rahmen dieser Arbeit wurde entschieden, eine Web-basierte Oberfläche zu implementieren.
Alles Weitere wird optional gehalten, um die Flexibilität zu maximieren und zukünftige Erweiterungen zu ermöglichen.
\par
Die geplante Kommunikation sieht wie folgt aus:

\FloatBarrier
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Bilder/masterarbeit/on-premise-rag-chatbot-uml-architektur-u6uu2d}
    \caption{Geplante Kommunikationsschnittstellen des RAG-Chatbots}
    \label{fig:rag-chatbot-architektur}
\end{figure}
\FloatBarrier

Der Nutzer interagiert über ein Web-Interface mit dem Chatbot.
Die Anfragen werden an das RAG-Modul weitergeleitet, das relevante Dokumentenabschnitte aus der Vektordatenbank abruft.
Diese Informationen werden dann an das LLM übergeben, das die Antwort generiert und zurück an das Web-Interface sendet.
Diese Architektur ermöglicht eine klare Trennung der Komponenten und erleichtert die Wartung und Erweiterung des Systems.
Dadurch werden folgende Vorteile erzielt, die sowohl aus technischer als auch aus wissenschaftlicher Sicht relevant sind:
\par
\begin{itemize}

    \item \textbf{Modularität und Entkopplung als Architekturprinzip}\\
    Ein zentrales Prinzip moderner Softwarearchitektur ist die \textit{Modularisierung} eines Systems. David Parnas zeigte bereits 1972, dass eine Zerlegung in Module die Flexibilität und Verständlichkeit eines Systems erhöht und gleichzeitig die Entwicklungszeit verkürzen kann \cite{Parnas1972}. Dieses Prinzip beruht auf der \textit{Trennung von Verantwortlichkeiten} (\textit{Separation of Concerns}) und der \textit{informationellen Abschirmung} einzelner Bausteine. Jedes Modul kapselt einen klar abgegrenzten Aspekt der Funktionalität und verbirgt Implementierungsdetails vor anderen Modulen. Dadurch entsteht eine geringere Kopplung zwischen den Teilen des Systems bei gleichzeitig hoher Kohäsion innerhalb der Module. Ein solches Design – oft umschrieben mit “low coupling, high cohesion” – gilt als Voraussetzung für wartbaren Code, da Änderungen an einer Komponente minimalen Einfluss auf andere Komponenten haben \cite{Martin2012CleanArchitecture, Khadapkar2025Hexagonal}. Zudem lässt sich jedes Modul unabhängig verstehen, testen und weiterentwickeln, was der Komplexitätsbeherrschung großer Softwaresysteme dient. Insgesamt schafft Modularität also die Grundlage für ein System, das anpassungsfähig und robust gegenüber Änderungen ist (ein wesentliches Qualitätsmerkmal gemäß gängiger Software-Engineering-Literatur).

    \item \textbf{Trennung der RAG-Komponenten}\\
    Die Aufteilung eines Retrieval-Augmented-Generation (RAG) Chatbot-Systems in getrennte Kernkomponenten – insbesondere in ein LLM-Modul, ein Embedding-Modul (Retriever) samt Dokumentenquelle sowie einen Reranker – folgt dem oben genannten Modularitätsprinzip in der konkreten Anwendung. Jede dieser Komponenten erfüllt eine \textit{spezifische Aufgabe} im Gesamtsystem und kann somit als eigenständiges Modul betrachtet werden \cite{Gao2023RAGSurvey}. So ist das Embedding-Modul dafür zuständig, semantische Vektor-Repräsentationen der Benutzeranfrage (und ggf. der Dokumente) zu berechnen und über die Dokumentenquelle relevante Wissensinhalte zu suchen. Der Reranker wiederum verfeinert die Ergebnisauswahl durch Neuanordnung oder Filterung der abgerufenen Dokumente, bevor das LLM-Modul (etwa ein generatives Sprachmodell wie Ollama) diese Kontexteingaben nutzt, um eine finale Antwort zu formulieren. Diese Aufgabentrennung entspricht auch der in wissenschaftlichen Übersichtsarbeiten beschriebenen Dreiteilung von RAG-Systemen in Retrieval-, Generierungs- und Augmentationskomponenten \cite{Gao2023RAGSurvey}. Indem man Retrieval und Generierung – und optional weitere Schritte wie Re-Ranking oder Zusammenfassen – als getrennte Module behandelt, erhält man feingranulare Kontrolle über jeden Verarbeitungsschritt \cite{Gao2024ModularRAG}.

    Die sinnvolle Trennung dieser RAG-Bausteine bringt mehrere Vorteile: Erstens kann jede Komponente unabhängig optimiert oder ausgetauscht werden, ohne das Gesamtsystem neu entwerfen zu müssen. Beispielsweise ließe sich ein Embedding-Modell durch ein genaueres oder effizienteres ersetzen, ohne Änderungen am LLM oder am Reranker vorzunehmen. Ebenso könnte man das LLM-Modul austauschen (z. B. ein größeres Modell oder ein spezialisiertes Domain-Modell einsetzen), während Retriever und Reranker unverändert bleiben. Diese Austauschbarkeit fördert schnelle Iteration und Anpassung an neue Anforderungen oder bessere Algorithmen \cite{Gao2024ModularRAG, Markov2025ModularRAG}. Zweitens ermöglicht die klare Abgrenzung der Verantwortlichkeiten eine gezielte Weiterentwicklung: So kann man etwa den Reranker weglassen oder hinzufügen, je nach gewünschter Antwortqualität, oder zusätzliche Augmentationsmodule (z. B. ein Kontext-Filter oder ein Memory-Modul) zwischenfügen, ohne die Struktur der übrigen Pipeline aufzubrechen \cite{Gao2024ModularRAG, Markov2025ModularRAG}. In aktuellen Forschungsarbeiten wird ein solcher \textit{„Lego-artiger“ modularer Aufbau} von RAG-Systemen als zukunftsweisend beschrieben, da er über den starren “retrieve-then-generate”-Prozess hinausgeht und komplexere Ablaufstrukturen wie bedingtes Routing oder iterative Schleifen erlaubt \cite{Gao2024ModularRAG}. Die Trennung von LLM, Embeddingsuche und Reranking ist demnach nicht nur technisch machbar, sondern auch wissenschaftlich motiviert, um Flexibilität und Anpassungsfähigkeit des Systems zu maximieren (vgl. Gao et al. 2024 und Gao et al. 2023; \cite{Gao2024ModularRAG,Gao2023RAGSurvey}). Schließlich verringert die Entkopplung auch das Risiko von Fehlerfortpflanzung: Ein Defekt oder eine Qualitätsminderung in einem Modul (etwa unzureichende Einbettungsqualität) kann isoliert betrachtet und behoben werden, ohne dass die anderen Module direkt betroffen sind – was zur Zuverlässigkeit des Gesamtsystems beiträgt \cite{Gao2023RAGSurvey}.

    \item \textbf{Zentrale Koordination durch das Web-Interface}\\
    Das Web-Interface fungiert in dieser Architektur als \textit{zentrale Orchestrierungskomponente}. Es nimmt Benutzeranfragen entgegen, koordiniert die Abfolge der Verarbeitungsschritte über die genannten Module hinweg und liefert letztlich die generierte Antwort zurück an den Nutzer. Diese zentrale “Steuerung” bringt mehrere Vorteile mit sich. Zum einen stellt das Web-Interface eine einheitliche Schnittstelle nach außen bereit (ähnlich einem \textit{Fassade}-Entwurfsmuster), sodass die internen Abläufe der RAG-Pipeline vor dem Nutzer verborgen bleiben. Für den Anwender erscheint das System als eine homogene Einheit, obwohl im Hintergrund mehrere spezialisierte Dienste zusammenwirken. Zum anderen implementiert das Web-Interface die Ablauflogik an einem Ort, was die Komplexität beherrschbar macht: Anstatt dass jedes Modul wissen muss, wann und wie es die anderen aufrufen soll, übernimmt das Web-Interface diese Vermittlerrolle (ähnlich dem \textit{Mediator}-Pattern der Entwurfsmusterliteratur). Dadurch bleiben die Kernmodule voneinander entkoppelt – sie kommunizieren ausschließlich über wohl definierte Schnittstellen mit dem Koordinator, jedoch nicht direkt untereinander. Die Module müssen folglich nichts über die Existenz oder Implementierungsdetails der anderen wissen, was wiederum genau dem oben genannten Prinzip der lose gekoppelten Architektur entspricht.

    Aus Sicht der Softwaretechnik erhöht diese Trennung von UI/Koordination und Fachlogik die Wartbarkeit und Austauschbarkeit der Komponenten erheblich. Ändert sich beispielsweise die Art der Benutzerschnittstelle (etwa von einem Web-Frontend zu einer Sprachassistenten-Oberfläche), kann diese Änderung erfolgen, ohne die inneren Module (Retriever, LLM etc.) anzupassen – das System ist UI-unabhängig \cite{Martin2012CleanArchitecture}. Ebenso können im Web-Interface bereichsübergreifende Aspekte konzentriert behandelt werden, z. B. Authentifizierung, Protokollierung (Logging) oder Fehlerbehandlung, ohne die Fachlogik in den Modulen damit zu belasten. Insgesamt bietet ein zentrales Koordinationsmodul also eine Kapselung der Ablaufsteuerung, was konsistente Abläufe sicherstellt und die Module vor unnötiger Komplexität schützt. Diese Architektur folgt dem Grundsatz der \textit{Schichtentrennung}, wie er in Clean Architecture und ähnlichen Architekturstilen empfohlen wird: Die Präsentations- und Steuerungsschicht (hier das Web-Interface) ist klar getrennt von den Geschäftslogik-Komponenten, welche wiederum getrennt sind von niedrigeren technischen Schichten (Datenhaltung usw.) \cite{Martin2012CleanArchitecture, Khadapkar2025Hexagonal}. Dadurch entsteht ein leicht austausch- und erweiterbares System, in dem jede Schicht sich auf ihren Zweck konzentriert.

    \item \textbf{Skalierbarkeit, Wartbarkeit und Erweiterbarkeit des Designs}\\
    Eine modulare RAG-Architektur unterstützt explizit die Qualitätsmerkmale Skalierbarkeit, Wartbarkeit und Erweiterbarkeit. Dies wird sowohl in der Praxis als auch in der Literatur bestätigt. Durch die Entkopplung der Komponenten kann jede Einheit bei Bedarf \textit{individuell skaliert} werden. Sollte z. B. das LLM-Modul besonders rechenintensiv sein, lässt es sich unabhängig von den anderen Komponenten auf mehreren Servern oder mit größerer Hardware-Power betreiben, ohne dass dazu das Gesamtsystem dupliziert werden müsste. Ebenso kann die Embeddingsuche (DocumentSource+EmbeddingModel) auf einen leistungsfähigeren Vektordatenbank-Server ausgelagert oder parallelisiert werden, falls das Anfragevolumen steigt. Industrielle Berichte zeigen, dass ein solcher service-orientierter Aufbau die Skalierbarkeit deutlich erhöht – Komponenten können als eigenständige Dienste “hochgefahren” werden, was gezielte Lastverteilung ermöglicht \cite{RidgeRun2024OnPremRAG}. Auch aus Architektursicht (z. B. in der Hexagonalen Architektur) wird betont, dass die Isolierung der Kernlogik eine effiziente Ressourcenoptimierung und einfaches Hochskalieren bestimmter Funktionalitäten erlaubt \cite{Khadapkar2025Hexagonal}.

    Die Wartbarkeit profitiert gleichermaßen: Änderungen oder Fehlerbehebungen können lokal in einem Modul vorgenommen werden, ohne Seiteneffekte auf andere Teile. Dies reduziert die Komplexität von Deployments und Regressionstests erheblich – ein neues Embedding-Modell einzuspielen betrifft nur das Embedding-Modul, während die restliche Pipeline unverändert bleibt. Entwicklerteams können zudem \textit{parallel} an unterschiedlichen Modulen arbeiten (z. B. ein Team verbessert den Reranker, während ein anderes das Web-Interface erweitert), was die Entwicklungszeit verkürzt \cite{Gao2024ModularRAG}. Sollte ein Bug auftreten, lässt sich durch die klare Verantwortlichkeitszuordnung schneller ermitteln, in welcher Komponente die Ursache liegt. Modularität erhöht auch die Zuverlässigkeit im Betrieb: Fällt eine Komponente aus oder liefert fehlerhafte Ergebnisse, bleibt der Fehler isoliert und zieht nicht unmittelbar das ganze System in Mitleidenschaft – oft kann eine Ersatzinstanz des betreffenden Dienstes gestartet werden, während andere Komponenten weiterlaufen. Dieses Prinzip macht das System robuster gegen Teilausfälle (Fehlertoleranz durch Kapselung).

    Schließlich erleichtert die Modularisierung die Erweiterbarkeit des Systems. Neue Funktionen können als zusätzliche Module implementiert und in die bestehende Orchestrierung eingebunden werden, statt monolithische Codeteile aufwendig ändern zu müssen. Beispielsweise könnte man ein Modul zur Wissensaktualisierung (z. B. periodischer Neucrawl der Dokumentenquelle) oder ein Modul zur Ergebniserklärung (das die Antwort des LLM mit Quellverweisen anreichert) hinzufügen, indem das Web-Interface um einen entsprechenden Aufruf erweitert wird. Die übrigen Komponenten bleiben unberührt, solange die Schnittstellen konsistent bleiben. Dieses “Plug-and-Play”-Konzept wurde als Schlüsselaspekt modularer RAG-Systeme hervorgehoben – verschiedene Branchen können gezielt Module hinzufügen, um das System an ihre Bedürfnisse anzupassen (z. B. juristische Chatbots fügen ein Zitations-Modul hinzu, E-Commerce-Assistenten ein Empfehlungs-Modul) \cite{Markov2025ModularRAG}. Kurzum: Die vorgestellte Architektur kann mit den Anforderungen mitwachsen. Sie ermöglicht es, neue Technologien oder Algorithmen einzubinden, ohne das Gesamtsystem neu zu erfinden, und unterstützt damit ein evolutionäres Vorgehen in der Systementwicklung.

    \item \textbf{Relevante Architekturstile und Entwurfsmuster}\\
    Die beschriebenen Konstruktionsentscheidungen lassen sich in den Kontext bekannter Architekturstile einordnen. Insbesondere zeigen sich Parallelen zu \textbf{Clean Architecture} bzw. dem \textbf{Ports-and-Adapters-Prinzip} (Hexagonale Architektur nach Alistair Cockburn). Beide Ansätze verfolgen das Ziel, Geschäftslogik von technischen Details strikt zu trennen und Abhängigkeiten umzukehren: Die Kernlogik (“innere Schicht”) definiert abstrakte Schnittstellen (\textit{Ports}), die von äußeren Schichten (\textit{Adapters}) implementiert werden. Übertragen auf unsere RAG-Architektur bedeutet dies: Das Web-Interface und ggf. eine zentrale Steuerungskomponente bilden die \textit{Domänenschicht}, welche z. B. eine abstrakte Schnittstelle für “Embeddingsuche” oder “LLM-Antwortgenerierung” vorgibt. Die konkreten Implementierungen – sei es ein Aufruf an ein bestimmtes Embedding-Modell oder an ein API einer LLM-Engine – erfolgen in getrennten Modulen, die diese Schnittstellen erfüllen. Die Abhängigkeit zeigt also \textit{von der Kernlogik nach außen}: Das System ist entwerfbar, ohne sich auf spezifische Bibliotheken oder Modelle festzulegen, da diese erst als Plugins angehängt werden. Laut Robert C. Martin resultiert daraus Software, die unabhängig von Frameworks, UI, Datenbanken etc. agiert und die Geschäftslogik nicht von technischen Details beeinflussen lässt \cite{Martin2012CleanArchitecture}. Dies trifft den Kern unseres Designs: die LLM- und Embedding-Module könnten z. B. leicht von lokalen Bibliotheken auf Web-Services umgestellt werden, ohne dass sich an der Logik im Web-Interface etwas ändert – man müsste lediglich den Adapter austauschen. Clean Architecture fördert zudem die Testbarkeit, da man die Kernlogik mit Mock-Implementierungen der Schnittstellen isoliert testen kann \cite{Martin2012CleanArchitecture}. Genauso kann in unserer Architektur beispielsweise die Antwortgenerierung getestet werden, indem man ein Dummy-LLM-Modul einspeist. Insgesamt bestätigen diese Entwurfsmuster die Sinnhaftigkeit eines entkoppelten, schnittstellenbasierten Designs für Wartbarkeit und Flexibilität.

    Ein weiterer relevanter Aspekt ist die Frage nach \textbf{Microservices vs. Monolith}. Die modulare Aufteilung begünstigt prinzipiell eine Microservice-Architektur, bei der jedes Modul als eigener (netzwerkgekoppelter) Service deployt wird. Tatsächlich beschreiben Lewis und Fowler Microservices als Ansatz, eine Anwendung als Sammlung kleiner, unabhängig deploybarer Dienste zu implementieren \cite{LewisFowler2014Microservices}. Im Kontext unseres Systems könnte man z. B. einen eigenständigen “Embedding-Service”, einen “Reranking-Service” und einen “LLM-Service” betreiben, die über leichte Kommunikationsmechanismen (etwa HTTP-APIs) vom Web-Interface aus angesprochen werden. Dies hätte den Vorteil, dass jede Komponente separat skaliert, ausgetauscht oder neu gestartet werden kann, ohne die anderen zu beeinflussen – ganz im Sinne der vorher erwähnten Skalierbarkeits- und Wartbarkeitsvorteile. Die Firma RidgeRun berichtet etwa, dass sie ihr RAG-System als Suite von Microservices umgesetzt hat, um genau diese Unabhängigkeit der Komponenten und leichte Austauschbarkeit von Open-Source-LLMs zu erreichen \cite{RidgeRun2024OnPremRAG}. Microservices fördern auch technologische Diversität: so könnte die Dokumentenquelle in einer anderen Programmiersprache oder mit einer speziellen Datenbank realisiert sein, während das Web-Interface weiterhin z. B. in Python läuft.

    Allerdings ist zu beachten, dass Microservices auch zusätzliche Komplexität mit sich bringen – etwa durch den Overhead verteilter Systeme, Netzwerkkommunikation, Monitoring vieler Deployments etc. Daher folgt man häufig dem Pragmatismus, zunächst einen gut modularisierten Monolithen zu bauen und bei Wachstumsbedarf ausgewählte Module in separate Dienste auszugliedern. Wichtig ist, dass die Architektur diese Trennlinien bereits vorsieht, was in unserem Design der Fall ist. Ein modularer Monolith, der die beschriebenen entkoppelten Schnittstellen intern einhält, kann viele Vorteile der Clean Architecture bereits liefern (klare Struktur, lokale Austauschbarkeit, Testbarkeit), ohne die Betriebskomplexität von verteilten Services. Sollte das System jedoch an Grenzen stoßen – etwa weil bestimmte Teile skalieren müssen oder unterschiedliche Deployment-Zyklen verlangen – kann man immer noch zum Microservice-Stil übergehen, da die Modulgrenzen ja klar definiert sind. Fowler weist darauf hin, dass monolithische Anwendungen mit zunehmendem Umfang häufig Erosionserscheinungen in der Modularität zeigen und Änderungen dann das gesamte System neu deployen lassen müssen \cite{LewisFowler2014Microservices}. Um dies zu vermeiden, ist unser Architekturentwurf von Anfang an auf Entkopplung ausgelegt.

\end{itemize}




