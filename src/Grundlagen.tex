\section{Grundlagen}
\label{sec:grundlagen}

Die vorliegende Arbeit bewegt sich im Spannungsfeld zwischen modernen Verfahren der Künstlichen Intelligenz, klassischer Information-Retrieval-Forschung und Fragen des Datenschutzes.
Im Zentrum steht die Frage, wie große Sprachmodelle (\textit{Large Language Models}, LLMs) so mit unternehmensinternen Wissensquellen gekoppelt werden können, dass sie einerseits nützliche, kontextbezogene Antworten liefern, andererseits aber den rechtlichen und organisatorischen Anforderungen an Datenschutz und IT-Sicherheit genügen.
Dieses Kapitel führt daher zunächst die notwendigen Grundlagen zu LLMs ein, erläutert den Architekturansatz der \textit{Retrieval-Augmented Generation} (RAG) und skizziert anschließend die wesentlichen Prinzipien des Datenschutzes und der Zugriffskontrolle im Unternehmenskontext.
Abschließend werden die technischen Bausteine der Dokumentenverarbeitung, Embeddings und semantischen Suche beschrieben, die in späteren Kapiteln konkret in der Onyx-Implementierung zum Einsatz kommen.

\subsection{Large Language Models (LLMs)}
\label{subsec:grundlagen-llms}

Große Sprachmodelle (\textit{Large Language Models}, LLMs) bilden die Grundlage moderner KI-Chatbots und Assistenzsysteme.
Technisch handelt es sich dabei um neuronale Netze auf Basis der Transformer-Architektur, die mit sehr großen Textkorpora vortrainiert werden und anschließend in der Lage sind, Wahrscheinlichkeitsverteilungen über Folge-Tokens zu modellieren~\cite{vaswani2017attention}.
Die Einführung des Transformers durch Vaswani et al.\ markiert einen Paradigmenwechsel, da anstelle rekurrenter Netze (\textit{RNNs}) und LSTMs vollständig auf Selbstaufmerksamkeitsmechanismen (\textit{self-attention}) gesetzt wird, wodurch sich sowohl Training als auch Inferenz besser parallelisieren lassen~\cite{vaswani2017attention}.

\paragraph{Pretraining und In-Context-Lernen.}
Im Pretraining werden LLMs auf allgemeine Sprachaufgaben optimiert, etwa die Vorhersage des nächsten Tokens (\textit{autoregressive Modelle} wie GPT) oder das Maskieren und Rekonstruieren von Tokens (\textit{masked language modeling} wie bei BERT)~\cite{devlin2019bert,brown2020language}.
Die dabei verwendeten Korpora umfassen typischerweise Webtexte, Bücher, Nachrichtenartikel und weitere öffentlich zugängliche Quellen in Milliarden-Token-Größenordnung.
Aus Sicht der Informationstechnik entsteht dadurch ein parametrisierter Wissensspeicher: Ein Großteil des Weltwissens, das in den Trainingsdaten enthalten war, ist implizit in den Modellgewichten kodiert.

Neuere Arbeiten zeigen, dass solche Modelle neben klassischen Supervised-Learning-Fähigkeiten auch \textit{In-Context-Learning} beherrschen: Bereits wenige Beispiel-Paare im Prompt genügen, um das Modell auf neue Aufgaben zu konditionieren, ohne dessen Parameter zu verändern~\cite{brown2020language}.
Dies erklärt, warum LLMs als generische Sprachschnittstellen eingesetzt werden können, die durch geeignetes Prompting an unterschiedliche Aufgaben angepasst werden.

\paragraph{Fähigkeiten und Grenzen im Unternehmenskontext.}
LLMs wie GPT-3/4, LLaMA oder Mistral sind in der Lage, kontextabhängige, natürlichsprachliche Antworten zu erzeugen, Texte zu strukturieren, zusammenzufassen oder in andere Sprachen zu übersetzen.
Sie können zudem Fragen in eigenen Worten beantworten, Programmcode generieren oder bestehende Inhalte umformulieren.

Im Unternehmenskontext treten jedoch mehrere inhärente Limitationen zutage:

\begin{itemize}
    \item \textbf{Statisches Weltwissen:}
    Das in den Modellgewichten gespeicherte Wissen entspricht dem Stand der Trainingsdaten.
    Nach Abschluss des Pretrainings ist das Modell gewissermaßen \glqq eingefroren\grqq{}; neue Informationen (z.\,B.\ geänderte Richtlinien, Produktversionen) werden nicht automatisch berücksichtigt.
    Eine Aktualisierung erfordert aufwändiges Fine-Tuning oder erneutes Training auf den geänderten Daten, was mit hohen Kosten und organisatorischem Aufwand verbunden ist~\cite{lewis2020rag,gao2023retrieval}.

    \item \textbf{Fehlende Kenntnis interner Informationen:}
    Standard-LLMs kennen typischerweise keine vertraulichen oder internen Dokumente eines Unternehmens, da diese aus Datenschutz- und Compliance-Gründen nicht für das Pretraining zur Verfügung standen.
    Ohne zusätzliche Anbindung können Fragen zu internen Prozessen, Kundendokumentationen oder spezifischen Konfigurationen daher nicht beantwortet werden.

    \item \textbf{Halluzinationen:}
    LLMs neigen dazu, fehlende Informationen durch plausible, aber faktisch falsche Aussagen zu ersetzen, ein Phänomen, das in der Literatur als \textit{Hallucination} bezeichnet wird~\cite{ji2023hallucination}.
    Insbesondere bei offenen Fragen ohne klaren Kontext generieren Modelle Antworten, die zwar sprachlich flüssig und glaubwürdig wirken, aber keine Grundlage in den zugrundeliegenden Daten haben.
    Im Unternehmenskontext kann dies zu erheblichen Risiken führen, wenn beispielsweise falsche technische Anweisungen oder rechtlich relevante Aussagen erzeugt werden.

    \item \textbf{Mangelnde Nachvollziehbarkeit und fehlende Quellenangabe:}
    Rein parametrisierte Modelle geben typischerweise keine Auskunft darüber, aus welchen Daten eine bestimmte Aussage abgeleitet wurde.
    Die Nachvollziehbarkeit (\textit{provenance}) und prüfbare Begründbarkeit von Antworten ist dadurch eingeschränkt~\cite{lewis2020rag}.
\end{itemize}

Diese Grenzen motivieren den Einsatz von Architekturen, die LLMs mit expliziten Wissensquellen verknüpfen, anstatt ausschließlich auf statisch parametrisiertes Wissen zu vertrauen.
Ein prominenter Ansatz in diesem Kontext ist die \textit{Retrieval-Augmented Generation} (RAG).

\subsection{Retrieval-Augmented Generation (RAG)}
\label{subsec:grundlagen-rag}

\textit{Retrieval-Augmented Generation} (RAG) bezeichnet einen Architekturansatz, bei dem ein generatives Sprachmodell mit einem vorgeschalteten Retrieval-Modul kombiniert wird~\cite{lewis2020rag,gao2023retrieval}.
Anstatt Antworten ausschließlich aus den eigenen Modellgewichten zu erzeugen, greift das LLM bei jeder Anfrage auf eine externe Wissensbasis zurück, typischerweise in Form eines Vektorindex über Dokumente.

\paragraph{Grundidee und Architektur.}
Die Originalarbeit von Lewis et al.\ beschreibt RAG als Kombination aus zwei Komponenten~\cite{lewis2020rag}:
\begin{enumerate}
    \item einem \textbf{Retriever}, der zu einer Eingabeanfrage passende Dokumentenpassagen aus einem großen Textkorpus findet, und
    \item einem \textbf{Generator}, der auf Basis der Eingabe und der abgerufenen Passagen eine Antwort generiert.
\end{enumerate}
Formal lässt sich der Ansatz als Zusammenspiel eines Retrieval-Modells $R(q)$, das zu einer Anfrage $q$ eine Menge von Kontextpassagen $D = \{d_1, \dots, d_k\}$ liefert, und eines generativen Modells $p_\theta(y \mid q, D)$ beschreiben, das daraus eine Antwort $y$ erzeugt~\cite{lewis2020rag}.

In der Praxis wird Retrieval häufig über dichte Vektorrepräsentationen (\textit{Embeddings}) umgesetzt:
\begin{itemize}
    \item Für jeden Dokumentenabschnitt $d_i$ wird ein Embedding $f(d_i) \in \mathbb{R}^n$ berechnet und in einer Vektordatenbank gespeichert.
    \item Die Nutzeranfrage $q$ wird mit derselben Funktion $f$ ebenfalls in ein Embedding überführt.
    \item Über eine Distanzfunktion (z.\,B.\ Kosinus-Ähnlichkeit) werden diejenigen Embeddings identifiziert, die dem Anfrage-Embedding am ähnlichsten sind.
\end{itemize}
Die so gefundenen Dokumentenabschnitte werden als zusätzlicher Kontext in den Prompt des LLM aufgenommen; das Modell generiert seine Antwort also nicht mehr im luftleeren Raum, sondern \glqq grounded\grqq{} in expliziten Textpassagen~\cite{gao2023retrieval}.

\paragraph{Zwei Phasen: Offline-Indizierung und Online-Antwortgenerierung.}
RAG-Architekturen lassen sich konzeptionell in zwei Phasen unterteilen~\cite{gao2023retrieval}:

\begin{enumerate}
    \item \textbf{Offline-Datenaufbereitung:}
    \begin{itemize}
        \item Dokumente werden aus den zugrundeliegenden Datenquellen (z.\,B.\ SharePoint, Dateisysteme, E-Mail-Archive) eingelesen.
        \item Eine Vorverarbeitung extrahiert den Text (Parsing), bereinigt ihn und zerlegt ihn in kleinere, semantisch sinnvolle Abschnitte (\textit{Chunking}).
        \item Für jeden Chunk werden Embeddings berechnet und zusammen mit Metadaten (Quelle, Zeitstempel, Berechtigungen) in einer Vektordatenbank gespeichert.
    \end{itemize}

    \item \textbf{Online-Antwortgenerierung:}
    \begin{itemize}
        \item Bei einer Nutzeranfrage wird ein Embedding der Anfrage berechnet.
        \item Das Retrieval-Modul ermittelt die $k$ ähnlichsten Dokumentenchunks aus der Vektordatenbank.
        \item Anfrage und Chunks werden zu einem erweiterten Prompt kombiniert, den das LLM als Eingabe erhält.
        \item Das LLM generiert eine Antwort, idealerweise unter expliziter Bezugnahme auf die bereitgestellten Quellen.
    \end{itemize}
\end{enumerate}

Microsoft und andere Anbieter beschreiben RAG in ähnlicher Form als zweistufiges Such- und Generierungsverfahren, bei dem der Suchindex als externer Wissensspeicher dient, während das LLM als generativer Reasoning-Layer fungiert~\cite{gao2023retrieval}.

\paragraph{Vorteile von RAG gegenüber rein parametrischen LLMs.}
Aus Sicht der Information Engineering adressiert RAG mehrere der zuvor beschriebenen LLM-Limitationen:

\begin{itemize}
    \item \textbf{Aktualität und Domänenspezifität:}
    Da die Wissensbasis über den Vektorindex gepflegt wird, können neue oder geänderte Informationen durch erneutes Indizieren der Dokumente berücksichtigt werden, ohne dass das LLM selbst neu trainiert werden muss~\cite{lewis2020rag,gao2023retrieval}.
    Dies ist insbesondere für Unternehmenskontexte attraktiv, in denen sich Richtlinien, Produkte und Prozesse regelmäßig ändern.

    \item \textbf{Reduktion von Halluzinationen:}
    Durch die Bindung an explizite Kontextdokumente sinkt die Wahrscheinlichkeit, dass das Modell frei \glqq halluziniert\grqq{}.
    Studien zeigen, dass RAG-Modelle in wissensintensiven Aufgaben häufig faktentreuere Antworten liefern als reine LLMs, da sie Informationen aus einem expliziten Speicher abrufen, anstatt ausschließlich parametrisiertes Wissen zu nutzen~\cite{lewis2020rag,ji2023hallucination}.

    \item \textbf{Nachvollziehbarkeit und Quellenangaben:}
    Da die verwendeten Kontextpassagen explizit bekannt sind, können Antworten mit Quellenangaben versehen werden.
    Dies verbessert die Transparenz und erlaubt es Nutzenden, Aussagen zu überprüfen, was wiederum zentrale Anforderungen an vertrauenswürdige KI adressiert~\cite{gao2023retrieval}.

    \item \textbf{Trennung von Modell und Wissensbasis:}
    Die Trennung zwischen generativem Modell und Wissensspeicher ermöglicht es, unterschiedliche LLMs (z.\,B.\ kleinere, schnellere Modelle für einfache Anfragen und größere Modelle für komplexe Fragestellungen) auf denselben Dokumentenbestand anzuwenden, ohne die Wissensbasis zu duplizieren.
\end{itemize}

RAG ist damit ein zentraler Baustein für unternehmensinterne Chatbots, die sowohl qualitativ hochwertige Antworten liefern als auch auf vertrauliche Dokumente zugreifen müssen, ohne diese externen Dienstleistern offenzulegen.

\subsection{Datenschutz und IT-Sicherheit}
\label{subsec:grundlagen-datenschutz}

Neben den KI-spezifischen Aspekten sind im Unternehmenskontext insbesondere Fragen des Datenschutzes und der IT-Sicherheit von zentraler Bedeutung.
In Europa bildet die Datenschutz-Grundverordnung (DSGVO, \textit{General Data Protection Regulation}, GDPR) den rechtlichen Rahmen für die Verarbeitung personenbezogener Daten~\cite{gdpr2016}.
Sie definiert grundlegende Prinzipien wie Rechtmäßigkeit, Zweckbindung, Datenminimierung sowie Integrität und Vertraulichkeit der Verarbeitung.

\paragraph{DSGVO-Grundprinzipien im Kontext von LLMs und RAG.}
Übertragen auf RAG-basierte Chatbots lassen sich insbesondere folgende Prinzipien hervorheben:

\begin{itemize}
    \item \textbf{Datenminimierung und Zweckbindung:}
    Es dürfen nur diejenigen Daten verarbeitet werden, die für den jeweiligen Zweck erforderlich sind (Art.~5 Abs.~1 lit.~c DSGVO)~\cite{gdpr2016}.
    Für einen internen Chatbot bedeutet dies, dass nur solche Dokumententeile in die Wissensbasis aufgenommen werden sollten, die tatsächlich für die Beantwortung typischer Nutzeranfragen relevant sind.
    Ebenso sollten Chatverläufe nur in dem Umfang gespeichert werden, wie es für Debugging, Qualitätssicherung oder Nachvollziehbarkeit notwendig ist.

    \item \textbf{Integrität und Vertraulichkeit:}
    Personenbezogene Daten müssen durch geeignete technische und organisatorische Maßnahmen vor unbefugter Offenlegung oder Veränderung geschützt werden (Art.~5 Abs.~1 lit.~f DSGVO)~\cite{gdpr2016}.
    Für ein RAG-System umfasst dies unter anderem Zugriffskontrollen auf die Wissensbasis, Verschlüsselung ruhender Daten (z.\,B.\ Embeddings, Indizes) sowie abgesicherte Kommunikationskanäle.

    \item \textbf{Transparenz:}
    Betroffene Personen müssen darüber informiert werden, in welcher Form ihre Daten verarbeitet werden (Art.~12 ff. DSGVO).
    Im Kontext von LLM-basierten Systemen wird in aktuellen Leitlinien zu KI und Datenschutz darauf hingewiesen, dass die Verwendung großer Modelle zusätzliche Risiken (z.\,B.\ Rekonstruktion sensibler Daten aus Trainingsdaten) mit sich bringt und entsprechend transparent gemacht werden muss~\cite{edpb2024llm}.
\end{itemize}

\paragraph{On-Premises-Verarbeitung versus Cloud-Dienste.}
Ein entscheidendes Architekturmerkmal der in dieser Arbeit betrachteten Lösung ist der On-Premises-Betrieb aller Komponenten.
Werden Anfragen oder Dokumente an externe LLM-APIs in der Cloud übermittelt, besteht grundsätzlich das Risiko, dass personenbezogene oder vertrauliche Unternehmensdaten die eigene Kontrollsphäre verlassen.
Viele Unternehmen unterliegen darüber hinaus regulatorischen Vorgaben zur Datenresidenz, die eine Verarbeitung nur innerhalb bestimmter geographischer oder organisatorischer Grenzen erlauben~\cite{gdpr2016}.

Durch eine On-Premises-Architektur, bei der sowohl die RAG-Komponenten (Vektordatenbank, Retrieval, Chat-Backend) als auch das eigentliche LLM auf eigener Hardware betrieben werden, verbleiben sämtliche Daten im Verantwortungsbereich der Organisation.
Fachliteratur und technische Whitepaper betonen, dass selbst gehostete LLMs insbesondere für stark regulierte Branchen (Finanzwesen, Gesundheitswesen, öffentliche Verwaltung) eine zentrale Option darstellen, um KI-Funktionalität mit strengen Datenschutzanforderungen zu verbinden~\cite{omnifact2024selfhosted}.

\paragraph{Zugriffskontrolle und Rollenmodelle.}
Ein weiterer zentraler Aspekt datenschutzkonformer KI-Systeme ist die Zugriffskontrolle.
In Systemen wie SharePoint oder OneDrive ist der Zugriff auf Dokumente häufig rollen- oder gruppenbasiert geregelt.
Ein interner Chatbot darf daher nur Informationen aus solchen Dokumenten liefern, auf die die anfragende Person auch im Ursprungssystem Zugriff hat.

In der sicherheitstechnischen Literatur hat sich hierfür das Konzept der \textit{Role-Based Access Control} (RBAC) etabliert~\cite{sandhu1996rbac}.
Dabei werden Zugriffsrechte nicht direkt einzelnen Benutzenden, sondern Rollen zugeordnet.
Benutzende werden wiederum Mitglied einer oder mehrerer Rollen, wodurch sich Berechtigungen effizient verwalten lassen.
Übertragen auf RAG-Systeme bedeutet dies:

\begin{itemize}
    \item Die Wissensbasis muss bei der Indizierung oder beim Retrieval die im Quellsystem definierten Berechtigungen berücksichtigen.
    \item Die Identität des Nutzers (z.\,B.\ über Single-Sign-On) muss mit der RAG-Komponente verknüpft werden, sodass nur Dokumentenchunks abgerufen werden, für die der Nutzer leseberechtigt ist.
    \item Der Chatbot darf keine Informationen aus Dokumenten \glqq durchsickern lassen\grqq{}, auf die ein Nutzer regulär keinen Zugriff hat, auch nicht indirekt durch zusammenfassende Antworten.
\end{itemize}

Aktuelle Arbeiten zur sicheren Verwendung von RAG-Systemen heben hervor, dass fehlende oder unzureichend implementierte Zugriffskontrollen eines der größten Risiken für Datenlecks darstellen und daher von Beginn an in das Architekturdesign integriert werden müssen~\cite{omnifact2024selfhosted,gao2023retrieval}.

\paragraph{Privacy by Design und Logging.}
Die DSGVO fordert in Art.~25 das Prinzip \textit{Datenschutz durch Technikgestaltung und durch datenschutzfreundliche Voreinstellungen} (\textit{privacy by design/by default})~\cite{gdpr2016}.
Für RAG-Systeme bedeutet dies u.\,a.:

\begin{itemize}
    \item Standardmäßig sollten nur die unbedingt notwendigen Daten geloggt werden (z.\,B.\ anonymisierte Nutzungsstatistiken statt vollständiger Chatverläufe).
    \item Logs, die für Debugging oder Audit-Zwecke benötigt werden, sind zu schützen (z.\,B.\ durch Zugriffsbeschränkungen, Pseudonymisierung).
    \item Schnittstellen zu Drittsystemen sind so zu gestalten, dass sie keine unnötigen personenbezogenen Daten übertragen.
\end{itemize}

Diese Überlegungen bilden den theoretischen Rahmen für die spätere konkrete Ausgestaltung der Onyx-Implementierung und die dort getroffenen Designentscheidungen.

\subsection{Dokumentenverarbeitung, Embeddings und semantische Suche}
\label{subsec:grundlagen-dokumentenverarbeitung}

Die in Abschnitt~\ref{subsec:grundlagen-rag} beschriebene RAG-Architektur setzt voraus, dass unstrukturierte Dokumente in eine Form überführt werden, die für die semantische Suche geeignet ist.
Dies geschieht in mehreren Schritten: Dokumentenverarbeitung (Parsing und Vorverarbeitung), Vektorisierung durch Embedding-Modelle und effiziente Ähnlichkeitssuche in einer Vektordatenbank.

\paragraph{Dokumenten-Parsing und Vorverarbeitung.}
Unternehmensdokumente liegen meist in heterogenen Formaten vor: PDF-Handbücher, Word-Dokumente (DOCX), PowerPoint-Präsentationen, E-Mail-Archive, HTML-Seiten usw.
Für die Nutzung in einer RAG-Pipeline müssen diese Formate zunächst in reinen Text überführt werden.
Dies geschieht typischerweise durch spezialisierte Parser-Bibliotheken (z.\,B.\ PDF-Parser, \texttt{python-docx}, Apache POI) und, falls erforderlich, ergänzende OCR-Verfahren für gescannte Dokumente.

Wesentliche Aufgaben der Vorverarbeitung sind:

\begin{itemize}
    \item Entfernung von nicht informativen Teilen (z.\,B.\ automatisch generierten Kopf- und Fußzeilen),
    \item Normalisierung von Zeichensätzen, Encodings und Sonderzeichen,
    \item Erhalt oder Rekonstruktion logischer Dokumentstruktur (Überschriften, Absätze, Listen), da diese für das spätere Chunking und die Interpretation der Inhalte relevant ist.
\end{itemize}

Eine saubere Vorverarbeitung ist entscheidend, da Fehler oder Inkonsistenzen in diesem Schritt sich unmittelbar auf die Qualität der Embeddings und damit auf das gesamte Retrieval auswirken.

\paragraph{Chunking: Aufteilung in semantische Einheiten.}
Wie bereits erläutert, werden Dokumente nach der Textextraktion in kleinere Einheiten (\textit{Chunks}) aufgeteilt.
Ziel ist es, Abschnitte zu erhalten, die für sich genommen semantisch kohärent sind und dennoch kurz genug bleiben, um effizient in das Kontextfenster des LLM aufgenommen werden zu können~\cite{gao2023retrieval}.

Chunking-Strategien können grob in zwei Ansätze unterteilt werden:

\begin{itemize}
    \item \textbf{Strukturorientiertes Chunking:}
    Chunks werden entlang der vorhandenen Dokumentstruktur (Überschriften, Absätze, Folien) gebildet.
    Dies ist insbesondere bei gut strukturierten technischen Dokumenten sinnvoll.

    \item \textbf{Token- oder Zeichenbasiertes Chunking:}
    Hier wird unabhängig von der dokumentinternen Struktur anhand einer maximalen Token- oder Zeichenzahl geschnitten.
    Häufig wird dieser Ansatz mit einer Überlappung zwischen Chunks kombiniert, um Kontext nicht abrupt abzuschneiden.
\end{itemize}

In der Praxis werden struktur- und tokenbasierte Verfahren oft kombiniert, indem zunächst strukturelle Einheiten identifiziert und anschließend der Länge nach begrenzt werden.
Die gewählte Chunking-Strategie beeinflusst direkt die Balance zwischen Recall (Auffinden aller relevanten Informationen) und Precision (Vermeidung irrelevanter Kontexte) im Retrieval~\cite{gao2023retrieval}.

\paragraph{Embedding-Modelle für semantische Repräsentationen.}
Um semantische Ähnlichkeit von Texten rechnerisch fassbar zu machen, werden Embedding-Modelle eingesetzt, die Text in hochdimensionale Vektoren abbilden.
Frühe Ansätze wie Word2Vec oder GloVe arbeiten auf Wortebene, sind aber für ganze Sätze oder Absätze nur eingeschränkt geeignet.
Moderne Systeme setzen stattdessen auf Kontextualisierung über Transformer-Architekturen.

Ein prominentes Beispiel ist \textit{Sentence-BERT} (SBERT)~\cite{reimers2019sbert}.
SBERT modifiziert BERT so, dass es mittels Siamese- und Triplet-Netzwerken direkt Vektorrepräsentationen für Sätze erzeugt, die über einfache Distanzmaße wie Kosinus-Ähnlichkeit verglichen werden können.
Dies reduziert die Berechnungskosten für Ähnlichkeitssuchen erheblich und liefert in vielen Retrieval-Tasks state-of-the-art Ergebnisse~\cite{reimers2019sbert}.

In RAG-Systemen können unterschiedlich spezialisierte Embedding-Modelle eingesetzt werden, z.\,B.:

\begin{itemize}
    \item generische, mehrsprachige Modelle für allgemeine Unternehmensdokumente,
    \item domänenspezifische Modelle (z.\,B.\ aus dem juristischen oder medizinischen Bereich),
    \item Modelle mit erweitertem Kontextfenster und besonderer Eignung für lange Dokumente.
\end{itemize}

Die Wahl des Embedding-Modells beeinflusst sowohl die Qualität der semantischen Suche als auch den Rechen- und Speicherbedarf des Systems.

\paragraph{Vektordatenbanken und Approximate Nearest Neighbor Search.}
Die gespeicherten Embeddings werden in einer Vektordatenbank oder einer darauf spezialisierten Bibliothek verwaltet.
Ziel ist es, für ein Anfrage-Embedding schnell diejenigen Vektoren zu finden, die ihm im semantischen Raum am nächsten liegen.
Bei Millionen von Chunks ist eine exakte Suche über alle Vektoren nicht mehr praktikabel, weshalb häufig Approximate-Nearest-Neighbor-Verfahren (ANN) eingesetzt werden.

FAISS (\textit{Facebook AI Similarity Search}) ist eine weit verbreitete Open-Source-Bibliothek, die effiziente Algorithmen zur Ähnlichkeitssuche und Vektorquantisierung bereitstellt~\cite{johnson2017faiss}.
Sie unterstützt sowohl CPU- als auch GPU-basierte Indexstrukturen und erlaubt damit die Skalierung auf sehr große Vektormengen.
Weitere Systeme wie Milvus oder Chroma bauen auf ähnlichen Konzepten auf und integrieren zusätzlich verteilte Speicherung sowie Metadatenfilter.

Die Kombination aus Embeddings und Vektordatenbank ist ein zentrales technisches Fundament von RAG-Systemen:
Sie ermöglicht es, inhaltlich passende Dokumentenpassagen auf Basis ihrer semantischen Ähnlichkeit zur Nutzeranfrage zu identifizieren, anstatt nur exakte Schlüsselwörter zu vergleichen.
Damit bilden diese Komponenten auch die Grundlage für die in Kapitel~\ref{sec:technische-umsetzung} beschriebene technische Umsetzung des datenschutzkonformen RAG-Chatbots.
