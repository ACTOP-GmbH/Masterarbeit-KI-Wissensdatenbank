\newpage
\section{Technische Umsetzung}\label{sec:technische-umsetzung}
Die technische Umsetzung des datenschutzkonformen RAG-Chatbots gliedert sich in mehrere
komponenten, die zusammenarbeiten, um die Anforderungen zu erfüllen.

\subsection{Notwendige Architekturkomponenten}\label{subsec:notwendige-architekturkomponenten}
Die Architektur des RAG-Chatbots umfasst folgende Hauptkomponenten:
\begin{itemize}
    \item \textbf{Dokumenten-Connectoren:} Schnittstellen, um Dokumente aus SharePoint, OneDrive und anderen Quellen zu extrahieren.
    \item \textbf{Dokumenten-Parser:} Tools zur Textextraktion und -bereinigung aus verschiedenen Dateiformaten (PDF, Word, E-Mail etc.).
    \item \textbf{Chunking-Modul:} Logik zur Aufteilung langer Dokumente in kleinere, semantisch sinnvolle Abschnitte.
    \item \textbf{Embedding-Modell:} Vortrainiertes Modell (z.B. Sentence-BERT), das Textabschnitte in Vektor-Repräsentationen umwandelt.
    \item \textbf{Vektordatenbank:} Speicherung und effiziente Suche der Embeddings (z.B. FAISS, Milvus).
    \item \textbf{Retrieval-Modul:} Komponente, die bei Nutzeranfragen relevante Dokumentenabschnitte aus der Vektordatenbank abruft.
    \item \textbf{LLM-Integration:} Anbindung eines Large Language Models (z.B. GPT-4, LLaMA) zur Generierung der Antworten basierend auf den abgerufenen Kontextinformationen.
    \item \textbf{Chat-Interface:} Frontend-Komponente für die Interaktion mit den Nutzern (z.B. Web-App, Teams-Bot).
    \item \textbf{Sicherheits- und Datenschutzmodule:} Mechanismen zur Gewährleistung des On-Premises-Betriebs, Zugriffskontrolle und Datenminimierung.
\end{itemize}

\subsection{Prototypische Implementierung zur Evaluierung der technischen Machbarkeit}\label{subsec:prototypische-implementierung-zur-evaluierung-der-technischen-machbarkeit}

\subsubsection{CPU-Tests und Auswahl der LLM-Modelle}

Um die technische Machbarkeit eines On-Premises RAG-Chatbots zu evaluieren, wurde ein Prototyp entwickelt.
Zunächst wurden verschiedene LLM-Modelle auf ihre Leistungsfähigkeit auf reiner CPU-Hardware hin getestet.
Ziel war es festzustellen, welche Modelle akzeptable Antwortzeiten und ausreichende Genauigkeit bieten können, wenn keine GPU-Beschleunigung zur Verfügung steht.
Getestet wurden dabei unter anderem:

\begin{itemize}
    \item \textbf{GPT-4All:} Ein Framework, das verschiedene offene Sprachmodelle bündelt, die lokal auf CPU ausgeführt werden können. Viele dieser Modelle basieren auf GPT-J oder LLaMA-Derivaten und sind für CPU-Inferenz optimiert (z. B. durch Quantisierung).
    \item \textbf{LLaMA 2 (7B):} Ein Open-Source-Modell von Meta mit 7 Milliarden Parametern. Diese kompakte Modellgröße erlaubt grundsätzlich den Betrieb auf Systemen ohne GPU.
    \item \textbf{Mistral 7B:} Ein modernes 7B-Modell der Firma Mistral AI, das 2023 veröffentlicht wurde und in seiner Parameterklasse als besonders leistungsfähig gilt.
\end{itemize}

\paragraph{Ergebnisse der CPU-Tests}

Die Experimente zeigten, dass selbst kleinere LLM-Modelle auf einer CPU grundsätzlich lauffähig sind, jedoch mit deutlich längeren Antwortzeiten im Vergleich zu GPU-gestützten Umgebungen.
So dauerte auf unserem Windows-Server die Generierung einer Antwort auf eine einfache Begrüßung ("Hallo") etwa 127 Sekunden – also über zwei Minuten Wartezeit für eine einzelne kurze Antwort.
Dieses Ergebnis verdeutlicht die erheblichen Geschwindigkeitsnachteile beim Einsatz von LLMs ohne spezialisierte Hardware.
Ähnliche Verzögerungen wurden auch in anderen Projekten beobachtet: Ein Erfahrungsbericht beschreibt beispielsweise eine Inferenzzeit von über 10 Minuten pro Anfrage für ein GPT4All-Falcon-Modell auf CPU \cite{cpu_reasoning_langchain}.

Die genauen Antwortzeiten hängen stark von der verfügbaren Hardware und vom Optimierungsgrad ab.
Community-Benchmarks zeigen zum Beispiel:

\begin{itemize}
    \item Raspberry Pi 4 (8 GB RAM): etwa 0.75 Token/s \cite{gguf_hardware_specs}
    \item Desktop-CPU Ryzen 5700G: etwa 11 Token/s \cite{gguf_hardware_specs}
    \item Günstige GPU (Radeon RX 6600, 8 GB VRAM): etwa 33 Token/s \cite{gguf_hardware_specs}
\end{itemize}

Diese Werte verdeutlichen die enorme Leistungsdifferenz zwischen CPU- und GPU-Inferenz.

\paragraph{Herausforderungen bei der CPU-Inferenz}

Viele der Schwierigkeiten bei der Nutzung von LLMs auf CPUs ergeben sich aus der Architektur der Modelle und den Limitierungen von CPUs:

\begin{itemize}
    \item \textbf{Rechenintensive Architektur:} Transformermodelle wie GPT oder LLaMA führen eine große Anzahl an Matrixmultiplikationen durch. GPUs sind für solche Operationen optimiert und können diese massiv parallelisieren, während CPUs deutlich langsamer arbeiten \cite{cpu_vs_gpu_transformer}.

    \item \textbf{Begrenzte Parallelisierung:} Zwar unterstützen moderne CPUs Multithreading, dennoch fehlt ihnen die breite Parallelverarbeitungskapazität moderner GPUs. Zusätzlich kann die verfügbare Speicherbandbreite (RAM) sehr schnell zum Flaschenhals werden.

    \item \textbf{Optimierungsbedarf:} Viele CPU-optimierte Modelle nutzen starke Quantisierung (z. B. 4-bit), um Rechenlast und Speicherbedarf zu reduzieren \cite{cpu_reasoning_langchain}. Zusätzlich müssen optimierte BLAS/LAPACK-Bibliotheken oder CPU-Befehlssatzerweiterungen wie AVX2, AVX-512 oder AMX genutzt werden.

    \item \textbf{Hohe Auslastung und Engpässe:} Während der Inferenz war der Server nahezu vollständig ausgelastet. Dadurch standen kaum Ressourcen für andere Prozesse zur Verfügung, etwa den Vektor-Suchdienst oder das Chat-Interface selbst. Die parallele Ausführung mehrerer Prozesse führte zu deutlichen Latenzsteigerungen und einer schlechten Nutzererfahrung.

    \item \textbf{Begrenzte Skalierbarkeit:} In unserem Anwendungsfall sollen im Worst-Case bis zu 20 Nutzer gleichzeitig den Chatbot abfragen. Eine rein CPU-basierte Lösung skaliert hierfür nicht: Bereits eine Anfrage dauert mehrere Minuten, parallele Anfragen überlasten das System vollständig.
\end{itemize}

\paragraph{Fazit: Notwendigkeit von GPU-Unterstützung}

Die Testergebnisse zeigen klar, dass der Chatbot ohne GPU-Unterstützung nicht sinnvoll betrieben werden kann.
Zwar ist die Ausführung moderner Open-Source-LLMs auf CPU technisch möglich, jedoch weder performant noch skalierbar.
Spezialisierte Hardware wie GPUs oder alternative Beschleuniger ist daher unerlässlich, um Inferenzzeiten in akzeptablen Grenzen zu halten und mehrere parallele Nutzeranfragen zuverlässig bedienen zu können.
Daher wurde der ursprüngliche Ansatz eines rein CPU-basierten Systems verworfen und stattdessen auf GPU-Beschleunigung gesetzt.

\subsubsection{GPU Testing und Auswahl der LLM-Modelle}
Um einen lokalen Retrieval-Augmented-Generation (RAG)-Chatbot effizient betreiben zu können, ist eine leistungsfähige GPU-Unterstützung von zentraler Bedeutung.

Die Hardwareanforderungen variieren hierbei erheblich in Abhängigkeit von der Modellgröße.

Kleinere Sprachmodelle lassen sich bereits auf Consumer-Hardware mit 4–8GB VRAM ausführen, wie beispielsweise einige GPT4All-Modelle zeigen \cite{gpt4all_models}.

Dagegen übersteigen große Modelle wie LLaMA~2 mit 70~Milliarden Parametern die Kapazität einzelner Grafikkarten deutlich: Für Inferenz in 16-bit-Darstellung werden über 130GB VRAM benötigt \cite{bacloud_gpu_guide}.

Derartige Modelle erfordern demnach entweder mehrere High-End-GPUs (z.B. zwei oder mehr Karten mit jeweils 24GB VRAM) oder quantisierte Varianten, was für unser On-Premises-Szenario nicht praktikabel war.

Eine akzeptable Leistungsfähigkeit und Antwortzeit auf nur einer GPU ließ sich realistisch nur mit mittelgroßen Modellen wie LLaMA~2~7B oder Mistral~7B erzielen, die mit etwa 7~Milliarden Parametern einen geeigneten Kompromiss zwischen Rechenleistung und Speicherbedarf bieten \cite{bacloud_gpu_guide,obotaiblog}.

Neuere Modelle beweisen, dass Effizienz nicht zwangsläufig mit Modellgröße korreliert.

So wurde im Jahr 2023 das Modell Mistral~7B vorgestellt, das in zahlreichen Benchmarks Modelle mit deutlich höherer Parameteranzahl — wie LLaMA~2~13B oder in Einzelfällen sogar LLaMA~2~70B — in der Leistung übertrifft \cite{obotaiblog}.

Auch andere aktuelle LLMs fokussieren zunehmend auf ressourcenschonende Architekturen, wodurch die Einstiegshürde für On-Premises-Lösungen weiter sinkt \cite{edge_medium}.

Für dieses Projekt wurde ein \textit{Lenovo Legion Pro 5 16IAX10H} Laptop mit einer \textit{NVIDIA RTX~5070~Ti} Grafikeinheit gewählt.

Diese auf NVIDIAs aktueller Architektur basierende GPU verfügt über 12GB GDDR7-VRAM \cite{notebookcheck_legion}.

Damit ist sie in der Lage, gängige 7B-Modelle vollständig im Grafikspeicher zu laden und effizient zu betreiben.

Beispielsweise benötigt LLaMA~2~7B in 16-bit etwa 12GB VRAM, was exakt dem vorhandenen Speicher der RTX~5070~Ti entspricht \cite{bacloud_gpu_guide}.

Durch quantisierte Repräsentationen (8-bit bzw. 4-bit) kann der Speicherbedarf sogar auf rund 6GB bzw. 3GB reduziert werden, wodurch theoretisch auch kleinere GPUs für die Inferenz herangezogen werden könnten.

Die restliche Hardwareausstattung des Laptops trägt ebenfalls zur Gesamtperformance bei:

Der verbaute \textit{Intel Core Ultra i9-275HX} Prozessor basiert auf der Arrow-Lake-Architektur und kombiniert 8 Performance-Kerne mit 16 Effizienz-Kernen bei einer TDP von 55W \cite{notebookcheck_legion}.

Durch Boost-Taktraten von bis zu 5,4GHz erreicht er eine Performance, die der von Desktop-Prozessoren nahekommt.

Ergänzt wird dies durch 32GB DDR5-Arbeitsspeicher im Dual-Channel-Betrieb mit bis zu 6400MT/s, wodurch eine hohe Speicherbandbreite zur Verfügung steht \cite{microcenter_legion}.

Besonders beim Laden großer Modelle oder beim parallelen Zugriff auf eine Vektordatenbank wirkt sich dies positiv auf die Laufzeitleistung aus.

Trotz des Notebook-Formfaktors ist die Leistungsfähigkeit nahezu mit Desktop-Systemen vergleichbar.

Das 300W-Netzteil erlaubt es der RTX~5070~Ti, mit einer Total Graphics Power (TGP) von bis zu 140W zu arbeiten \cite{notebookcheck_legion}.

Leistungsanalysen zeigen, dass sie in diesem Modus eine mit der Desktop-Variante der RTX~4070 vergleichbare Rechenleistung erzielen kann \cite{reddit_rtx_comparison}, welche typischerweise 200–250W benötigt.

Die Energieeffizienz der mobilen Variante ist dabei deutlich höher:

Anwenderberichte verweisen auf eine Reduktion der Leistungsaufnahme um 30–40W bei ähnlicher Taktrate \cite{reddit_rtx_comparison}.

Mit einem Marktpreis von etwa 2.490€ stellt diese Konfiguration eine wirtschaftlich sinnvolle Lösung zur lokalen Ausführung großer Sprachmodelle dar.

Zum Vergleich: Mobile Workstations wie etwa das \textit{HP ZBook 14 G1} bewegen sich im selben Preissegment, sind jedoch meist mit GPUs der Ada-Serie mit nur 4GB VRAM ausgestattet \cite{zbook_review,zbook_specs}.

Selbst bei hoher Effizienz und professioneller Treiberunterstützung sind solche Konfigurationen für LLMs unzureichend:

Bereits für quantisierte 7B-Modelle wird eine Speicherausstattung von mindestens 8GB empfohlen \cite{bacloud_gpu_guide,gpt4all_models}.

Die gewählte Gaming-GPU bietet somit das Dreifache an VRAM und ist klar auf maximale Rechenleistung ausgelegt, anstatt auf zertifizierte Anwendungen in CAD oder wissenschaftlichen Visualisierungen.

Zudem ermöglicht der Laptop durch seine Mobilität eine nahtlose Integration in bestehende IT-Infrastrukturen.

Er kann in regulären Büroräumen betrieben werden und benötigt keinen dedizierten Serverstellplatz.

Diese Entscheidung eröffnet langfristig auch Perspektiven im Sinne des \textit{Edge-Computing}:

Zukünftig könnten mehrere Endgeräte mit GPUs als verteilter Ressourcenpool für Inferenzaufgaben dienen — ein Trend, der durch die Verlagerung von KI-Lasten von der Cloud an die Peripherie (Edge) weiter an Bedeutung gewinnt \cite{edge_medium,datacenter_llm}.






\subsection{Onyx Implementierung}
\label{subsec:onyx-implementierung}

Nachdem die Hardware-Plattform sowie die zu evaluierenden LLM-Modelle festgelegt waren, konnte mit der eigentlichen Implementierung des RAG-Systems begonnen werden.
Gemäß der in Kapitel~\ref{sec:technische-umsetzung} beschriebenen Zielarchitektur fiel die Wahl auf die Open-Source-Plattform \textit{Onyx} (ehemals \textit{Danswer}) als technisches Fundament.
Onyx ist vollständig in Python implementiert und stellt eine modulare RAG-Architektur zur Verfügung, die zentrale Bausteine wie Dokumenten-Connectoren, eine Vektorsuche, Embedding-Pipelines sowie eine flexible Agenten- und Chatoberfläche bereits integriert~\cite{onyx_github}.
Die Community Edition von Onyx wird unter der MIT-Lizenz bereitgestellt und kann damit ohne zusätzliche Lizenzkosten in einer On-Premises-Umgebung betrieben werden~\cite{onyx_foss}.
Damit erfüllt die Plattform sowohl die funktionalen Anforderungen an ein RAG-System als auch die nicht-funktionalen Anforderungen an Wartbarkeit, Erweiterbarkeit und Lizenzkonformität.

\subsubsection{Deployment der Onyx-Plattform in einer containerisierten Umgebung}

Um eine reproduzierbare und weitgehend vom Host-Betriebssystem entkoppelte Umgebung zu gewährleisten, wurde Onyx auf dem zuvor beschriebenen Lenovo-Laptop als Docker-Deployment eingerichtet.
Die Bereitstellung folgte den offiziellen Quickstart- und Deployment-Empfehlungen der Onyx-Dokumentation, die eine \texttt{docker-compose}-basierte Orchestrierung der Backend- und Frontend-Dienste vorsieht~\cite{onyx_github}.
Ausgangspunkt war eine aktuelle Python-Installation (Python~3.13) auf dem Host-System.
Diese Version wurde konsistent sowohl für lokale Hilfsskripte (beispielsweise zur Modellkonfiguration und zum Testen der Schnittstellen) als auch innerhalb der Container verwendet, um Versionskonflikte zu vermeiden und den Übergang zwischen experimentellen Skripten und produktionsnaher Umgebung zu erleichtern.

Darauf aufbauend wurde aus der von Onyx bereitgestellten Datei \texttt{env.prod.template} eine projektspezifische \texttt{.env}-Konfiguration abgeleitet.
In dieser Konfigurationsdatei sind sämtliche zentralen Parameter gebündelt, unter anderem die Verbindungsdaten für die relationale Datenbank (PostgreSQL), die Einstellungen für das Vektorsuchbackend (etwa Vespa), die Konfiguration für Cache-Komponenten wie Redis sowie grundlegende Authentifizierungs- und Betriebsparameter.
Durch den systematischen Einsatz von Umgebungsvariablen kann die Plattform auf unterschiedliche Umgebungen (Entwicklung, Test, Produktion) übertragen werden, ohne Codeänderungen vornehmen zu müssen.

Im nächsten Schritt wurde der von Onyx bereitgestellte \texttt{docker-compose.yml}-Stack gestartet.
Dieser umfasst das Web-Frontend, das API-Backend, die Datenbank und weitere Hilfsdienste innerhalb eines gemeinsamen, isolierten Docker-Netzwerks.
Die Containerisierung kapselt sämtliche Laufzeitabhängigkeiten und erleichtert damit einerseits die Reproduzierbarkeit der Experimente und andererseits die spätere Migration auf andere Hosts oder Server.
Konfigurationen und Zustände werden soweit möglich über persistent gemountete Volumes und die Datenbank verwaltet, sodass ein Austausch der Hardware keine Neuinstallation der Plattform erfordert.

Schließlich wurde die GPU in den Containerverbund integriert.
Da der Lenovo-Laptop über eine NVIDIA RTX~5070~Ti verfügt, wurde dem Onyx- beziehungsweise Ollama-Container explizit GPU-Zugriff gewährt (etwa mittels \verb|--gpus all| im Compose-Stack).
Dadurch können die LLM-Modelle direkt auf der Grafikkarte ausgeführt werden, was – wie die in Abschnitt~\ref{subsec:prototypische-implementierung-zur-evaluierung-der-technischen-machbarkeit} beschriebenen Tests gezeigt haben – die Inferenzzeit im Vergleich zur reinen CPU-Ausführung drastisch reduziert.
Die gesamte Applikation (inklusive ihrer Abhängigkeiten) ist damit in einer definierten, portablen Umgebung gekapselt, was Konfigurationsfehler und \glqq Works-on-my-machine\grqq-Effekte deutlich reduziert.

\subsubsection{Anbindung der Datenquellen: Connectoren und Document Sets}

Zentraler Bestandteil der RAG-Architektur ist die strukturierte Anbindung der vorhandenen Wissensquellen.
Onyx stellt dazu eine Reihe vordefinierter \textit{Connectoren} bereit, mit denen Daten aus typischen Unternehmenssystemen wie SharePoint, OneDrive, Confluence oder Git-Repositories automatisiert in den internen Suchindex übernommen werden~\cite{onyx_github,onyx_docs}.
Diese Connectoren übernehmen nicht nur die Extraktion der Inhalte, sondern auch die Übernahme relevanter Metadaten, etwa des Erstellers, des Änderungsdatums oder der Berechtigungen.

In der vorliegenden Implementierung stand zunächst die Integration von SharePoint als zentrale Ablage für Produktdokumentationen, Prozessbeschreibungen und Vorlagen im Vordergrund.
Im Onyx-Admin-Panel wurde hierfür ein SharePoint-Connector eingerichtet, der auf ausgewählte Sites und Dokumentbibliotheken der Organisation zugreift.
Die Authentifizierung erfolgt über eine in Azure Entra ID (vormals Azure AD) registrierte Applikation, deren Client-ID, Tenant-ID und Geheimnis in der Onyx-Konfiguration hinterlegt werden.
Onyx nutzt anschließend die Microsoft-Graph-API, um die Dokumente und ihre Metadaten auszulesen und die in SharePoint hinterlegten Berechtigungsstrukturen in den eigenen Index zu spiegeln~\cite{onyx_docs}.
Ergänzend dazu können OneDrive-Connectoren genutzt werden, um individuelle Arbeitsbereiche und projektspezifische Dateien zu berücksichtigen, ohne dass diese physisch aus den Ursprungssystemen migriert werden müssen.

Um die Vielzahl unterschiedlicher Dokumente sinnvoll zu strukturieren, werden in Onyx sogenannte \emph{Document Sets} definiert.
Sie erlauben die logische Gruppierung von Dokumenten zu thematischen oder organisatorischen Einheiten.
In dieser Arbeit wurden Document Sets so angelegt, dass sie beispielsweise Produkt- und Release-Dokumentation, interne Prozess- und Organisationsdokumente sowie projektspezifische Unterlagen bündeln.
Bei einer Anfrage an den Chatbot kann damit gezielt festgelegt werden, aus welchen Document Sets Kontextinformationen abgerufen werden sollen.
Diese thematische Partitionierung folgt den Empfehlungen der RAG-Literatur, wonach eine sinnvolle Segmentierung des Wissensraums die Präzision und Nachvollziehbarkeit von Antworten verbessert und gleichzeitig die Sucheffizienz erhöht~\cite{gao2023retrieval}.

\subsubsection{Parsing, Normalisierung und Chunking der Dokumente}

Nach der Anbindung der Datenquellen werden die Dokumente im Rahmen des Onyx-Ingestionsprozesses zunächst geparst und normalisiert.
Onyx bringt hierfür eigene Parser mit, die gängige Office-Formate wie PDF, DOCX und PPTX sowie reine Textformate verarbeiten können~\cite{onyx_docs}.
Im vorliegenden Szenario wurde der Parser so konfiguriert, dass der eigentliche Fließtext möglichst verlustfrei extrahiert und zugleich von Layout-Elementen wie Kopf- und Fußzeilen getrennt wird.
Gleichzeitig werden grundlegende Metadaten wie Titel, Erstellungsdatum, Quelle und Autor in strukturierter Form erfasst, um sie später für Filter und Berechnungen nutzen zu können.
Technische Artefakte – etwa automatisch generierte Inhaltsverzeichnisse, Generator-Kommentare oder redundante Kopfzeilen – werden, sofern sie für die Beantwortung von Fragen keinen Mehrwert liefern, verworfen oder reduziert.

Auf diese Textextraktion folgt das sogenannte \emph{Chunking}.
Hierbei werden die Dokumente in kleinere, semantisch sinnvolle Abschnitte zerlegt, die anschließend als eigenständige Einheiten im Vektorindex gespeichert werden.
Die RAG-Literatur zeigt, dass die Wahl der Chunking-Strategie einen erheblichen Einfluss auf die Effektivität des Retrievals hat~\cite{gao2023retrieval}.
Zu große Chunks erschweren eine präzise Lokalisierung relevanter Passagen und führen dazu, dass das LLM mit umfangreichem, teilweise irrelevanten Kontext konfrontiert wird.
Zu kleine Chunks hingegen zerschneiden den Kontext so stark, dass inhaltliche Zusammenhänge verloren gehen.

In der praktischen Umsetzung wurde daher eine abschnittsorientierte Chunking-Strategie gewählt, bei der Chunk-Grenzen möglichst an bestehenden Strukturmerkmalen wie Überschriften und Absätzen ausgerichtet sind.
Auf diese Weise bleiben thematische Einheiten wie Kapitel oder Unterkapitel erhalten, was insbesondere für erklärende Dokumente (Handbücher, Prozessbeschreibungen) wichtig ist.
Zusätzlich wurde eine maximale Chunk-Länge auf Token-Basis festgelegt, um sicherzustellen, dass einzelne Chunks innerhalb des Kontextfensters der verwendeten LLMs bleiben und mehrere Chunks gemeinsam an das Modell übergeben werden können.
Um Informationsverluste an Abschnittsgrenzen zu vermeiden, wurde eine moderate Überlappung zwischen benachbarten Chunks eingerichtet.
Dadurch können auch Fragen, die sich auf Übergänge oder Verweise zwischen Abschnitten beziehen, zuverlässiger beantwortet werden.

\subsubsection{Vektorisierung mit Nomic-Embedding-Modellen}

Nach dem Chunking werden die resultierenden Textabschnitte in Vektor-Repräsentationen überführt.
Hierfür wurde ein Embedding-Modell aus der \textit{Nomic-Embed}-Familie eingesetzt, wie es in~\cite{nussbaum2024nomic} beschrieben wird.
Diese Modelle sind explizit für lange Kontexte (bis zu 8192 Tokens) und semantische Suchaufgaben optimiert und nutzen Matryoshka Representation Learning, um Embeddings mit variabler Dimensionalität bereitzustellen~\cite{kusupati2022matryoshka}.

Die Wahl eines Nomic-Embedding-Modells ist insbesondere deshalb sinnvoll, weil klassische BERT-basierte Embeddings häufig auf Kontextlängen von 512 Tokens begrenzt sind und längere Abschnitte nur unzureichend abbilden.
Bei umfangreichen technischen Dokumenten führt dies schnell zu verlorenen oder verzerrten Kontexten.
Nomic-Modelle erlauben dagegen die Verarbeitung deutlich längerer Passagen und sind speziell auf semantische Retrieval-Aufgaben hin optimiert.
In veröffentlichten Benchmarks zeigen sie zudem eine höhere Qualität als etablierte kommerzielle Embeddings wie \texttt{text-embedding-ada-002}, insbesondere im Hinblick auf semantische Clusterbildung und Robustheit gegenüber Paraphrasen~\cite{nussbaum2024nomic}.

Ein weiterer Vorteil liegt in der flexiblen Dimensionalität:
Durch Matryoshka Representation Learning können hochdimensionale Embeddings nachträglich auf kleinere Dimensionen projiziert werden, ohne die Struktur des semantischen Raums vollständig zu zerstören~\cite{kusupati2022matryoshka}.
Damit lässt sich der Kompromiss zwischen Suchqualität und Speicherbedarf bewusst steuern, was im Hinblick auf die begrenzten Ressourcen eines einzelnen Laptops relevant ist.
In Onyx erfolgt die Integration des Embedding-Modells über die Konfiguration der Embedding-Pipeline im Admin-Panel.
Das Nomic-Modell wird als Embedding-Backend registriert und den relevanten Document Sets zugewiesen, sodass alle neu eingelesenen Dokumente automatisch in diesem Vektorraum repräsentiert werden.

\subsubsection{Indexierung und Retrieval-Konfiguration}

Für die Speicherung der Embeddings nutzt Onyx eine Vektor-Suchkomponente, die typischerweise mit einer relationalen Datenbank (PostgreSQL) und weiteren Diensten wie Redis kombiniert wird~\cite{onyx_github}.
Jeder Chunk wird dabei zusammen mit seinem Vektor, den zugehörigen Metadaten und einer Referenz auf das Ursprungdokument gespeichert.
Die semantische Suche erfolgt auf Basis einer Ähnlichkeitsmetrik, etwa der Kosinus-Ähnlichkeit, während klassische Filter (z.\,B.\ nach Quelle, Dokumententyp oder Zeitraum) über Metadaten implementiert werden.

Im prototypischen System wurde das Retrieval so konfiguriert, dass für jede Nutzeranfrage eine begrenzte Anzahl der semantisch ähnlichsten Chunks (\textit{Top-$k$}) aus den relevanten Document Sets abgerufen wird.
Ein zu niedriges $k$ birgt die Gefahr, dass relevante Kontexte übersehen werden; ein zu hohes $k$ führt hingegen zu einer Überfrachtung des Modells mit Kontext und kann die Antwortqualität verschlechtern.
Die Wahl eines moderaten Top-$k$ stellt somit sicher, dass ausreichend Kontext vorhanden ist, ohne die Generierung zu überladen.
Ergänzend dazu wurde die von Onyx unterstützte Hybrid-Suche aktiviert, bei der semantische Ähnlichkeit mit einer klassischen Keyword-Suche kombiniert wird~\cite{onyx_docs}.
Gerade in Fachdomänen mit spezifischer Terminologie – etwa Produktnamen oder internen Abkürzungen – erhöht dies die Wahrscheinlichkeit, dass die tatsächlich relevanten Dokumente an die Spitze des Rankings gelangen.
Die Filterung nach Document Sets und Quellen stellt sicher, dass jeweils nur jene Wissensbereiche in das Retrieval einbezogen werden, die für den jeweiligen Agenten vorgesehen sind.

Die gewählte Konfiguration orientiert sich an Erkenntnissen aktueller RAG-Übersichtsarbeiten, die zeigen, dass insbesondere Top-$k$ und die Wahl der Ähnlichkeitsmetrik maßgeblichen Einfluss auf Präzision, Recall und Antwortzeit haben~\cite{gao2023retrieval}.
In der praktischen Umsetzung wurden diese Parameter daher empirisch mit repräsentativen Testanfragen kalibriert.

\subsubsection{LLM-Integration über Ollama und Konfiguration der Modelle}

Für die Generierung der Antworten wurde Onyx mit der lokal betriebenen LLM-Laufzeit \textit{Ollama} gekoppelt.
Ollama stellt eine standardisierte API zur Ausführung lokaler Sprachmodelle auf der GPU bereit und bietet ein Modell-Repository, über das unterschiedliche LLMs (etwa LLaMA~2~7B oder Mistral~7B) bereitgestellt werden können.
Die Integration in Onyx erfolgt durch die Registrierung eines Ollama-Providers im Admin-Panel, in dem die Erreichbarkeit des Ollama-Dienstes (Host, Port) und gegebenenfalls Authentifizierungsinformationen hinterlegt werden.

Die zuvor ausgewählten Modelle wurden über Ollama auf dem Lenovo-Laptop heruntergeladen und stehen damit über die Ollama-API für Inferenzanfragen zur Verfügung.
In Onyx werden die entsprechenden Modellbezeichnungen (z.\,B.\ \texttt{llama2:7b} oder \texttt{mistral:7b}) als eigenständige LLM-Konfigurationen hinterlegt.
Dadurch kann pro Agent festgelegt werden, welches Modell für die Beantwortung von Nutzeranfragen verwendet werden soll.
Für den in dieser Arbeit im Fokus stehenden internen Assistenten der ACTOP GmbH wurde zunächst eines der Modelle als Standardmodell ausgewählt; alternative Modelle können parallel konfiguriert und für Vergleichsstudien oder spezialisierte Szenarien genutzt werden.

Da sowohl Onyx als auch Ollama im selben Docker-Netzwerk betrieben werden, verbleiben sämtliche Daten – einschließlich der Dokumente, Embeddings und Chatverläufe – vollständig im lokalen System.
Es werden weder Nutzereingaben noch Kontextdokumente an externe Cloud-LLMs übertragen.
Dies ist ein zentraler Baustein des datenschutzkonformen On-Premises-Betriebs und fügt sich in die in Abschnitt~\ref{subsec:grundlagen-datenschutz} beschriebenen Anforderungen an Datenresidenz und Vertraulichkeit ein.

\subsubsection{Agenten, Prompt Engineering und System-Instruktionen}

Ein wesentlicher Erfolgsfaktor von RAG-Systemen ist die Gestaltung der Systeminstruktionen (\textit{Prompts}) und Agentenrollen.
Studien zu In-Context-Learning zeigen, dass die Struktur des Prompts einen erheblichen Einfluss auf die Qualität und Zuverlässigkeit der Modellantworten hat~\cite{brown2020language}.
Insbesondere im RAG-Kontext ist es entscheidend, das Modell explizit anzuweisen, den bereitgestellten Kontext zu verwenden und nicht auf sein vortrainiertes Weltwissen auszuweichen, um Halluzinationen zu reduzieren~\cite{gao2023retrieval,ji2023survey}.

Onyx bildet diese Logik über \emph{Agenten} ab.
Ein Agent bündelt eine Rollenbeschreibung samt Systeminstruktionen, die dem Modell seine Aufgabe und seinen Kommunikationsstil vorgeben, mit einer Auswahl von Wissensquellen (Document Sets) und optionalen Aktionen wie interner Suche oder Websuche.
Für den internen Wissensassistenten der ACTOP GmbH wurde ein Agent definiert, der bewusst nur auf interne Dokumentenquellen zugreift; externe Tools wie Websuche wurden deaktiviert, um eine strikte Begrenzung auf das Unternehmenswissen durchzusetzen.

Der zugrunde liegende System-Prompt ist bewusst restriktiv formuliert und lautet:

\begin{quote}
    \small
    You are ACTOP’s internal knowledge assistant. You answer questions using the retrieved documentation provided in the "Context" section.

    Your objectives:
    \begin{enumerate}
        \item Use the RAG context as the primary source of truth. You are only allowed to use the RAG context and nothing else, no own knowledge.
        \item Provide exact, practical, concise instructions.
        \item Use the same language as the user’s question (German if user writes in German).
        \item When the context contains:
        \begin{itemize}
            \item quoted text $\rightarrow$ cite it using numbered citations like [1], [2], \dots
            \item screenshots/images $\rightarrow$ reference them as ``(see screenshot)''
        \end{itemize}
        \item If the context provides step-by-step instructions, reproduce them cleanly.
        \item If the context is insufficient, briefly state that and answer only with what you can confirm.
        \item Do NOT explain that you are an AI or describe your internal reasoning.
        \item You are only allowed to use internal search knowledge. If you do not find an answer, then that information does not exist. Simple as that.
    \end{enumerate}
\end{quote}

Diese Instruktion verfolgt mehrere, in der Literatur diskutierte Ziele.
Zum einen erzwingt sie eine strikte Kontextbindung (\textit{Grounding}):
Das Modell soll seine Antworten ausschließlich auf die im RAG-Kontext bereitgestellten Dokumente stützen und keine externen Wissensanteile einbringen.
Dadurch werden extrinsische Halluzinationen reduziert, bei denen das Modell Inhalte erfindet, die in den Dokumenten nicht belegt sind~\cite{ji2023survey,gao2023retrieval}.
Zum zweiten erhöht die Verpflichtung zu expliziten Zitationen ([1], [2], \dots) die Transparenz und Nachvollziehbarkeit der Antworten und erleichtert eine \glqq human-in-the-loop\grqq{}-Verifikation durch die Nutzenden~\cite{gao2023retrieval}.
Drittens trägt die Vorgabe, die Sprache der Anfrage zu spiegeln und auf Meta-Kommentare zum KI-Status zu verzichten, zu einem konsistenten und anwendungsnahen Interaktionsstil bei.
Schließlich wirkt die explizite Erlaubnis, Unwissen zuzugeben, halluzinationsmindernd, da das Modell nicht dazu angehalten wird, in jedem Fall eine Antwort zu konstruieren, selbst wenn der Kontext unzureichend ist~\cite{ji2023survey}.

\subsubsection{Sicherheits- und Datenschutzaspekte im Onyx-Betrieb}

Neben der reinen Funktionalität mussten auch die Sicherheits- und Datenschutzanforderungen des On-Premises-Betriebs berücksichtigt werden.
Onyx adressiert diese Anforderungen durch ein Rollen- und Rechtekonzept, durch die Möglichkeit, Berechtigungen aus den Quellsystemen zu spiegeln, sowie durch technische Maßnahmen zur Absicherung sensibler Konfigurationsdaten~\cite{onyx_docs}.

Die Plattform unterscheidet verschiedene Benutzerrollen (beispielsweise Admin, Curator, User) und erlaubt die Konfiguration gruppenbasierter Zugriffsrechte.
Damit kann präzise gesteuert werden, welche Personen auf welche Agenten, Document Sets und Konfigurationen zugreifen dürfen.
Für Connectoren wie SharePoint ist zudem vorgesehen, die im Ursprungssystem definierten Berechtigungen in den Onyx-Index zu übernehmen, sodass nur solche Dokumente als Kontext berücksichtigt werden, für die der jeweilige Nutzer auch im Quellsystem leseberechtigt ist.
Dies ist im Hinblick auf datenschutzrechtliche Vorgaben und interne Compliance-Regeln von zentraler Bedeutung.

Zugangsdaten für externe Systeme (etwa API-Keys oder Zertifikate) werden verschlüsselt in der Datenbank abgelegt, um das Risiko einer unbeabsichtigten Offenlegung – etwa durch Fehlkonfigurationen oder Log-Exports – zu reduzieren.
Da alle Komponenten der RAG-Pipeline (Connectoren, Indexierung, Vektorsuche und LLM-Inferenz via Ollama) in der lokalen Infrastruktur betrieben werden, findet keine Übertragung von Inhalten an externe Cloud-Services statt.
In Kombination mit dem beschriebenen Prompt-Design entsteht so ein RAG-System, das nicht nur technisch funktionsfähig und performant ist, sondern auch den Anforderungen an Datenschutz, Nachvollziehbarkeit und Vertrauen in einem unternehmenskritischen Kontext gerecht wird.

\subsection{Erste Instanzierung des Chatbots}\label{subsec:erste-instanzierung-des-chatbots}

Vor der eigentlichen Evaluation wurde der RAG-Chatbot zunächst gemäß der in Kapitel~\ref{sec:technische-umsetzung} beschriebenen Architektur implementiert und in einer ersten lauffähigen Konfiguration in Betrieb genommen.
Ziel dieser Phase war es, eine technisch stabile Instanz zu schaffen, die Anfragen über das Web-Interface entgegennimmt, Dokumente aus SharePoint und OneDrive als Wissensbasis nutzt und Antworten über ein lokal ausgeführtes Sprachmodell generiert.
Dazu gehörten die Installation und Konfiguration von Onyx, die Anbindung der Datenquellen über die bereitgestellten Connectoren, das Aufsetzen der Embedding- und Vektorindex-Pipeline sowie die Integration der LLM-Laufzeit über Ollama.

Von Anfang an wurde dabei konsequent auf spezialisierte Komponenten gesetzt.
Für nahezu jede Stufe der Pipeline kamen dedizierte, für diesen Zweck beworbene Modelle oder Bausteine zum Einsatz: ein spezialisiertes Embedding-Modell für semantische Suche, die RAG-Mechanismen von Onyx inklusive Search-Agenten und Document Sets für das Retrieval, sowie Chat- und QA-optimierte LLM-Varianten für die Antwortgenerierung.
Die Erwartung war, dass diese „Best-of-breed“-Strategie – also hochspezialisierte Modelle für alle Schritte – in Summe zu einem besonders performanten und präzisen System führen würde.
Die erste Instanzierung des Chatbots zeigte jedoch, dass diese Annahme nur eingeschränkt zutraf und dass die Kombination spezialisierter Komponenten ein hohes Maß an Abstimmung erfordert.

Zu Beginn wurde Mistral-7B-Instruct als generatives Modell eingesetzt.
Die Wahl fiel auf dieses Modell, weil es als kompaktes, aktuelles Open-Source-Modell mit Instruct-Fokus verfügbar war und in Benchmarks gute Ergebnisse zeigte.
In isolierten Tests ohne Retrieval – also bei einfachen Chat-Eingaben – funktionierte das Modell zufriedenstellend.
Sobald jedoch längere, aus den Dokumenten extrahierte Kontexte in die Prompts integriert wurden, traten deutliche Schwächen auf:
Antworten blieben vage, wichtige Details aus den Dokumenten wurden übergangen, und die Antwortzeiten waren spürbar höher, als es für eine produktive Nutzung akzeptabel wäre.
Rückblickend wurde an dieser Stelle deutlich, dass der ursprünglichen Modellauswahl zu wenig systematische Überlegungen zugrunde lagen und dass ein „generisches“ Instruct-Modell allein nicht ausreicht, um die Anforderungen eines anspruchsvollen RAG-Szenarios zu erfüllen.

Im nächsten Schritt wurde deshalb ein deutlich größeres Modell, gpt-oss20b, getestet.
Die zugrunde liegende Hypothese war naheliegend: Ein Modell mit höherer Parameteranzahl sollte die semantisch komplexen Kontexte besser verarbeiten und dadurch qualitativ hochwertigere Antworten erzeugen.
Tatsächlich konnten in einigen Fällen inhaltlich präzisere und differenziertere Antworten beobachtet werden.
Gleichzeitig verschärften sich aber andere Probleme.
Zum einen stiegen die Antwortzeiten merklich an; einzelne Anfragen benötigten nun teilweise viele Sekunden, bis eine vollständige Antwort vorlag.
Zum anderen zeigte das Modell ein instabiles Antwortverhalten:
In mehreren Fällen „verhedderte“ sich gpt-oss20b in seinen eigenen Ausgaben, produzierte lange, teils redundante Textpassagen oder brach Antworten an unpassender Stelle ab.
Die bloße Erhöhung der Modellgröße verbesserte zwar einzelne Aspekte, verschärfte aber andere Engpässe und machte deutlich, dass der Engpass nicht allein im Modell, sondern im Zusammenspiel der gesamten Pipeline zu suchen war.

Parallel dazu traten technische Probleme in der Kommunikation zwischen Onyx und der LLM-Laufzeit zu Tage.
Die Anbindung erfolgt über litellm, das standardmäßig mit einem sehr konservativen \textit{keepalive}-Timeout von fünf Sekunden konfiguriert ist.
Diese Voreinstellung ist für leichte Inferenzaufgaben und kleine Modelle ausreichend, erweist sich für lokal ausgeführte, mittelgroße oder große LLMs jedoch als zu restriktiv.
In der Praxis führte dies dazu, dass Anfragen zwar korrekt an das Modell übermittelt wurden, die Verbindung jedoch abbrach, bevor die vollständige Antwort zurückgeliefert werden konnte.
Aus Sicht von Onyx äußerte sich dies in Form von scheinbar zufälligen Timeouts und abgebrochenen Chatverläufen.
Erst eine detaillierte Analyse der Log-Dateien und der litellm-Konfiguration machte deutlich, dass hier kein „denkfaules“ oder fehlerhaftes Modell vorlag, sondern eine unpassende Timeout-Konfiguration, die schlicht nicht zu den Laufzeiten komplexer LLM-Inferenz passte.
Die Anpassung der entsprechenden Parameter (erhöhtes Timeout, längerer \textit{keepalive}) war eine notwendige Voraussetzung, um das System überhaupt unter realistischen Bedingungen testen zu können.

Nachdem mit gpt-oss20b kein zufriedenstellendes Gleichgewicht zwischen Antwortzeit und Antwortqualität erreicht werden konnte, wurde im nächsten Schritt ein kleineres, explizit für Frage-Antwort-Szenarien beworbenes Modell gewählt.
Mit der Umstellung auf \texttt{llama3-chatqa:8b} verbesserten sich die Antwortzeiten deutlich.
Das Modell ließ sich vollständig im GPU-Speicher der RTX~5070~Ti betreiben, sodass die Generierung spürbar flüssiger und reaktionsschneller war.
Auch die inhaltliche Qualität der Antworten nahm zunächst zu:
Insbesondere bei klar fokussierten Fragen, die sich direkt auf Passagen aus der Dokumentation beziehen, lieferte das Modell verständliche und fachlich stimmige Antworten.
Damit schien sich die anfängliche Erwartung zu bestätigen, dass ein hochspezialisiertes ChatQA-Modell für diesen Anwendungsfall besonders gut geeignet sei.

In einem weiteren Schritt wurde dann das Onyx-\enquote{Search}-Agentenprofil angepasst, um das Zusammenspiel zwischen Retrieval und Generierung zu optimieren.
Die Systeminstruktionen des Agents wurden so formuliert, dass das Modell explizit angewiesen wurde, zunächst interne Suchfunktionen zu verwenden, relevante Chunks aus den Document Sets abzurufen und anschließend auf Basis dieses Kontextes eine Antwort zu generieren.
Diese Architektur folgt den Empfehlungen vieler RAG-Frameworks, bei denen das LLM nicht nur passiv Kontext erhält, sondern aktiv Suchschritte anstoßen kann.
In der Praxis führte die Kombination aus dem stark spezialisierten ChatQA-Modell und dem angepassten Search-Agenten jedoch zu einem unerwarteten Verhalten:
Statt die Suchfunktion zu nutzen und einen Antworttext zu formulieren, begann \texttt{llama3-chatqa:8b} häufig, lediglich die ursprüngliche Nutzereingabe oder interne Query-Repräsentationen zu reproduzieren.
Anstelle einer inhaltlichen Antwort erschienen im Chat nur noch Suchstrings oder stark paraphrasierte Versionen der Frage.
Das Modell war offenkundig nicht in der Lage, die von Onyx erwartete Tool-Logik korrekt zu interpretieren und die Suche als eigenständigen Schritt im Antwortprozess zu nutzen.

Diese Beobachtung war in zweierlei Hinsicht aufschlussreich.
Zum einen zeigte sich, dass Marketingbegriffe wie „für RAG geeignet“ oder „optimiert für QA mit Kontext“ nur begrenzt aussagekräftig sind, wenn ein Modell in eine konkrete, toolbasierte Umgebung eingebettet wird.
Ein Modell kann in Benchmarks sehr gut mit einem statisch übergebenen Kontext umgehen, ohne gleichzeitig robust mit Tool-Aufrufen, Agentenlogik und komplexen Systemprompts zurechtzukommen.
Zum anderen wurde deutlich, dass die starke Spezialisierung aller Pipeline-Komponenten das System zwar theoretisch maximiert, praktisch aber eine erhebliche Kopplung erzeugt:
Das Embedding-Modell, der Vektorindex, die Retrieval-Parameter, der Onyx-Agent und das LLM mussten fein aufeinander abgestimmt werden, damit das Gesamtsystem sinnvoll funktioniert.
Schon kleine Änderungen in den Systeminstruktionen oder am Agentenprofil konnten dazu führen, dass ein ansonsten leistungsfähiges Modell „aus dem Tritt“ geriet und nur noch triviale Antworten lieferte.

Im weiteren Verlauf dieser ersten Instanzierungsphase wurde daher schrittweise nachjustiert.
Die Agentenprompts wurden so angepasst, dass sie für das Modell möglichst eindeutig und wenig „magisch“ formuliert sind, die Retrieval-Parameter (z.\,B.\ Wahl von Top-$k$ und Chunk-Längen) wurden mit realistischen Testanfragen kalibriert, und die Timeout-Einstellungen der LLM-Anbindung wurden auf die tatsächlichen Inferenzzeiten abgestimmt.
Gleichzeitig wurde der Einsatz spezialisierter Modelle partiell pragmatisiert:
Während auf der Embedding-Seite weiterhin ein dediziertes Modell eingesetzt wurde, das für semantische Suche optimiert ist, wurde bei der Antwortgenerierung nicht mehr ausschließlich auf maximal spezialisierte ChatQA-Varianten gesetzt, sondern auf Modelle, die im Zusammenspiel mit Onyx und der konkret gewählten Agentenlogik robustes Verhalten zeigen.

Insgesamt war die erste Instanzierung des Chatbots damit weniger ein linearer Implementierungsschritt als ein iterativer, experimenteller Prozess.
Die Kombination aus hochspezialisierten Modellen in allen Pipeline-Stufen, einer komplexen RAG-Architektur und den praktischen Restriktionen der Hardware führte zu einem System, das zunächst nur eingeschränkt nutzbar war.
Erst durch wiederholtes Anpassen von Modellauswahl, Agentenprompts, Retrieval-Konfiguration und technischen Parametern wie den litellm-Timeouts konnte eine Konfiguration gefunden werden, die für eine systematische Evaluation geeignet war.
Diese Erfahrungen prägen die spätere Bewertung der technischen Machbarkeit und fließen unmittelbar in die Diskussion der Grenzen und Erfolgsfaktoren eines datenschutzkonformen On-Premises-RAG-Chatbots in Kapitel~\ref{sec:evaluation} ein.
