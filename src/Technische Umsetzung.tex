\newpage
\section{Technische Umsetzung}\label{sec:technische-umsetzung}
Die technische Umsetzung des datenschutzkonformen RAG-Chatbots gliedert sich in mehrere
komponenten, die zusammenarbeiten, um die Anforderungen zu erfüllen.

\subsection{Notwendige Architekturkomponenten}\label{subsec:notwendige-architekturkomponenten}
Die Architektur des RAG-Chatbots umfasst folgende Hauptkomponenten:
\begin{itemize}
    \item \textbf{Dokumenten-Connectoren:} Schnittstellen, um Dokumente aus SharePoint, OneDrive und anderen Quellen zu extrahieren.
    \item \textbf{Dokumenten-Parser:} Tools zur Textextraktion und -bereinigung aus verschiedenen Dateiformaten (PDF, Word, E-Mail etc.).
    \item \textbf{Chunking-Modul:} Logik zur Aufteilung langer Dokumente in kleinere, semantisch sinnvolle Abschnitte.
    \item \textbf{Embedding-Modell:} Vortrainiertes Modell (z.B. Sentence-BERT), das Textabschnitte in Vektor-Repräsentationen umwandelt.
    \item \textbf{Vektordatenbank:} Speicherung und effiziente Suche der Embeddings (z.B. FAISS, Milvus).
    \item \textbf{Retrieval-Modul:} Komponente, die bei Nutzeranfragen relevante Dokumentenabschnitte aus der Vektordatenbank abruft.
    \item \textbf{LLM-Integration:} Anbindung eines Large Language Models (z.B. GPT-4, LLaMA) zur Generierung der Antworten basierend auf den abgerufenen Kontextinformationen.
    \item \textbf{Chat-Interface:} Frontend-Komponente für die Interaktion mit den Nutzern (z.B. Web-App, Teams-Bot).
    \item \textbf{Sicherheits- und Datenschutzmodule:} Mechanismen zur Gewährleistung des On-Premises-Betriebs, Zugriffskontrolle und Datenminimierung.
\end{itemize}

\subsection{Prototypische Implementierung zur Evaluierung der technischen Machbarkeit}\label{subsec:prototypische-implementierung-zur-evaluierung-der-technischen-machbarkeit}

\subsubsection{CPU-Tests und Auswahl der LLM-Modelle}

Um die technische Machbarkeit eines On-Premises RAG-Chatbots zu evaluieren, wurde ein Prototyp entwickelt.
Zunächst wurden verschiedene LLM-Modelle auf ihre Leistungsfähigkeit auf reiner CPU-Hardware hin getestet.
Ziel war es festzustellen, welche Modelle akzeptable Antwortzeiten und ausreichende Genauigkeit bieten können, wenn keine GPU-Beschleunigung zur Verfügung steht.
Getestet wurden dabei unter anderem:

\begin{itemize}
    \item \textbf{GPT-4All:} Ein Framework, das verschiedene offene Sprachmodelle bündelt, die lokal auf CPU ausgeführt werden können. Viele dieser Modelle basieren auf GPT-J oder LLaMA-Derivaten und sind für CPU-Inferenz optimiert (z. B. durch Quantisierung).
    \item \textbf{LLaMA 2 (7B):} Ein Open-Source-Modell von Meta mit 7 Milliarden Parametern. Diese kompakte Modellgröße erlaubt grundsätzlich den Betrieb auf Systemen ohne GPU.
    \item \textbf{Mistral 7B:} Ein modernes 7B-Modell der Firma Mistral AI, das 2023 veröffentlicht wurde und in seiner Parameterklasse als besonders leistungsfähig gilt.
\end{itemize}

\paragraph{Ergebnisse der CPU-Tests}

Die Experimente zeigten, dass selbst kleinere LLM-Modelle auf einer CPU grundsätzlich lauffähig sind, jedoch mit deutlich längeren Antwortzeiten im Vergleich zu GPU-gestützten Umgebungen.
So dauerte auf unserem Windows-Server die Generierung einer Antwort auf eine einfache Begrüßung ("Hallo") etwa 127 Sekunden – also über zwei Minuten Wartezeit für eine einzelne kurze Antwort.
Dieses Ergebnis verdeutlicht die erheblichen Geschwindigkeitsnachteile beim Einsatz von LLMs ohne spezialisierte Hardware.
Ähnliche Verzögerungen wurden auch in anderen Projekten beobachtet: Ein Erfahrungsbericht beschreibt beispielsweise eine Inferenzzeit von über 10 Minuten pro Anfrage für ein GPT4All-Falcon-Modell auf CPU \cite{cpu_reasoning_langchain}.

Die genauen Antwortzeiten hängen stark von der verfügbaren Hardware und vom Optimierungsgrad ab.
Community-Benchmarks zeigen zum Beispiel:

\begin{itemize}
    \item Raspberry Pi 4 (8 GB RAM): etwa 0.75 Token/s \cite{gguf_hardware_specs}
    \item Desktop-CPU Ryzen 5700G: etwa 11 Token/s \cite{gguf_hardware_specs}
    \item Günstige GPU (Radeon RX 6600, 8 GB VRAM): etwa 33 Token/s \cite{gguf_hardware_specs}
\end{itemize}

Diese Werte verdeutlichen die enorme Leistungsdifferenz zwischen CPU- und GPU-Inferenz.

\paragraph{Herausforderungen bei der CPU-Inferenz}

Viele der Schwierigkeiten bei der Nutzung von LLMs auf CPUs ergeben sich aus der Architektur der Modelle und den Limitierungen von CPUs:

\begin{itemize}
    \item \textbf{Rechenintensive Architektur:} Transformermodelle wie GPT oder LLaMA führen eine große Anzahl an Matrixmultiplikationen durch. GPUs sind für solche Operationen optimiert und können diese massiv parallelisieren, während CPUs deutlich langsamer arbeiten \cite{cpu_vs_gpu_transformer}.

    \item \textbf{Begrenzte Parallelisierung:} Zwar unterstützen moderne CPUs Multithreading, dennoch fehlt ihnen die breite Parallelverarbeitungskapazität moderner GPUs. Zusätzlich kann die verfügbare Speicherbandbreite (RAM) sehr schnell zum Flaschenhals werden.

    \item \textbf{Optimierungsbedarf:} Viele CPU-optimierte Modelle nutzen starke Quantisierung (z. B. 4-bit), um Rechenlast und Speicherbedarf zu reduzieren \cite{cpu_reasoning_langchain}. Zusätzlich müssen optimierte BLAS/LAPACK-Bibliotheken oder CPU-Befehlssatzerweiterungen wie AVX2, AVX-512 oder AMX genutzt werden.

    \item \textbf{Hohe Auslastung und Engpässe:} Während der Inferenz war der Server nahezu vollständig ausgelastet. Dadurch standen kaum Ressourcen für andere Prozesse zur Verfügung, etwa den Vektor-Suchdienst oder das Chat-Interface selbst. Die parallele Ausführung mehrerer Prozesse führte zu deutlichen Latenzsteigerungen und einer schlechten Nutzererfahrung.

    \item \textbf{Begrenzte Skalierbarkeit:} In unserem Anwendungsfall sollen im Worst-Case bis zu 20 Nutzer gleichzeitig den Chatbot abfragen. Eine rein CPU-basierte Lösung skaliert hierfür nicht: Bereits eine Anfrage dauert mehrere Minuten, parallele Anfragen überlasten das System vollständig.
\end{itemize}

\paragraph{Fazit: Notwendigkeit von GPU-Unterstützung}

Die Testergebnisse zeigen klar, dass der Chatbot ohne GPU-Unterstützung nicht sinnvoll betrieben werden kann.
Zwar ist die Ausführung moderner Open-Source-LLMs auf CPU technisch möglich, jedoch weder performant noch skalierbar.
Spezialisierte Hardware wie GPUs oder alternative Beschleuniger ist daher unerlässlich, um Inferenzzeiten in akzeptablen Grenzen zu halten und mehrere parallele Nutzeranfragen zuverlässig bedienen zu können.
Daher wurde der ursprüngliche Ansatz eines rein CPU-basierten Systems verworfen und stattdessen auf GPU-Beschleunigung gesetzt.

\subsubsection{GPU Testing und Auswahl der LLM-Modelle}
Um einen lokalen Retrieval-Augmented-Generation (RAG)-Chatbot effizient betreiben zu können, ist eine leistungsfähige GPU-Unterstützung von zentraler Bedeutung.

Die Hardwareanforderungen variieren hierbei erheblich in Abhängigkeit von der Modellgröße.

Kleinere Sprachmodelle lassen sich bereits auf Consumer-Hardware mit 4–8GB VRAM ausführen, wie beispielsweise einige GPT4All-Modelle zeigen \cite{gpt4all_models}.

Dagegen übersteigen große Modelle wie LLaMA~2 mit 70~Milliarden Parametern die Kapazität einzelner Grafikkarten deutlich: Für Inferenz in 16-bit-Darstellung werden über 130GB VRAM benötigt \cite{bacloud_gpu_guide}.

Derartige Modelle erfordern demnach entweder mehrere High-End-GPUs (z.B. zwei oder mehr Karten mit jeweils 24GB VRAM) oder quantisierte Varianten, was für unser On-Premises-Szenario nicht praktikabel war.

Eine akzeptable Leistungsfähigkeit und Antwortzeit auf nur einer GPU ließ sich realistisch nur mit mittelgroßen Modellen wie LLaMA~2~7B oder Mistral~7B erzielen, die mit etwa 7~Milliarden Parametern einen geeigneten Kompromiss zwischen Rechenleistung und Speicherbedarf bieten \cite{bacloud_gpu_guide,obotaiblog}.

Neuere Modelle beweisen, dass Effizienz nicht zwangsläufig mit Modellgröße korreliert.

So wurde im Jahr 2023 das Modell Mistral~7B vorgestellt, das in zahlreichen Benchmarks Modelle mit deutlich höherer Parameteranzahl — wie LLaMA~2~13B oder in Einzelfällen sogar LLaMA~2~70B — in der Leistung übertrifft \cite{obotaiblog}.

Auch andere aktuelle LLMs fokussieren zunehmend auf ressourcenschonende Architekturen, wodurch die Einstiegshürde für On-Premises-Lösungen weiter sinkt \cite{edge_medium}.

Für dieses Projekt wurde ein \textit{Lenovo Legion Pro 5 16IAX10H} Laptop mit einer \textit{NVIDIA RTX~5070~Ti} Grafikeinheit gewählt.

Diese auf NVIDIAs aktueller Architektur basierende GPU verfügt über 12GB GDDR7-VRAM \cite{notebookcheck_legion}.

Damit ist sie in der Lage, gängige 7B-Modelle vollständig im Grafikspeicher zu laden und effizient zu betreiben.

Beispielsweise benötigt LLaMA~2~7B in 16-bit etwa 12GB VRAM, was exakt dem vorhandenen Speicher der RTX~5070~Ti entspricht \cite{bacloud_gpu_guide}.

Durch quantisierte Repräsentationen (8-bit bzw. 4-bit) kann der Speicherbedarf sogar auf rund 6GB bzw. 3GB reduziert werden, wodurch theoretisch auch kleinere GPUs für die Inferenz herangezogen werden könnten.

Die restliche Hardwareausstattung des Laptops trägt ebenfalls zur Gesamtperformance bei:

Der verbaute \textit{Intel Core Ultra i9-275HX} Prozessor basiert auf der Arrow-Lake-Architektur und kombiniert 8 Performance-Kerne mit 16 Effizienz-Kernen bei einer TDP von 55W \cite{notebookcheck_legion}.

Durch Boost-Taktraten von bis zu 5,4GHz erreicht er eine Performance, die der von Desktop-Prozessoren nahekommt.

Ergänzt wird dies durch 32GB DDR5-Arbeitsspeicher im Dual-Channel-Betrieb mit bis zu 6400MT/s, wodurch eine hohe Speicherbandbreite zur Verfügung steht \cite{microcenter_legion}.

Besonders beim Laden großer Modelle oder beim parallelen Zugriff auf eine Vektordatenbank wirkt sich dies positiv auf die Laufzeitleistung aus.

Trotz des Notebook-Formfaktors ist die Leistungsfähigkeit nahezu mit Desktop-Systemen vergleichbar.

Das 300W-Netzteil erlaubt es der RTX~5070~Ti, mit einer Total Graphics Power (TGP) von bis zu 140W zu arbeiten \cite{notebookcheck_legion}.

Leistungsanalysen zeigen, dass sie in diesem Modus eine mit der Desktop-Variante der RTX~4070 vergleichbare Rechenleistung erzielen kann \cite{reddit_rtx_comparison}, welche typischerweise 200–250W benötigt.

Die Energieeffizienz der mobilen Variante ist dabei deutlich höher:

Anwenderberichte verweisen auf eine Reduktion der Leistungsaufnahme um 30–40W bei ähnlicher Taktrate \cite{reddit_rtx_comparison}.

Mit einem Marktpreis von etwa 2.490€ stellt diese Konfiguration eine wirtschaftlich sinnvolle Lösung zur lokalen Ausführung großer Sprachmodelle dar.

Zum Vergleich: Mobile Workstations wie etwa das \textit{HP ZBook 14 G1} bewegen sich im selben Preissegment, sind jedoch meist mit GPUs der Ada-Serie mit nur 4GB VRAM ausgestattet \cite{zbook_review,zbook_specs}.

Selbst bei hoher Effizienz und professioneller Treiberunterstützung sind solche Konfigurationen für LLMs unzureichend:

Bereits für quantisierte 7B-Modelle wird eine Speicherausstattung von mindestens 8GB empfohlen \cite{bacloud_gpu_guide,gpt4all_models}.

Die gewählte Gaming-GPU bietet somit das Dreifache an VRAM und ist klar auf maximale Rechenleistung ausgelegt, anstatt auf zertifizierte Anwendungen in CAD oder wissenschaftlichen Visualisierungen.

Zudem ermöglicht der Laptop durch seine Mobilität eine nahtlose Integration in bestehende IT-Infrastrukturen.

Er kann in regulären Büroräumen betrieben werden und benötigt keinen dedizierten Serverstellplatz.

Diese Entscheidung eröffnet langfristig auch Perspektiven im Sinne des \textit{Edge-Computing}:

Zukünftig könnten mehrere Endgeräte mit GPUs als verteilter Ressourcenpool für Inferenzaufgaben dienen — ein Trend, der durch die Verlagerung von KI-Lasten von der Cloud an die Peripherie (Edge) weiter an Bedeutung gewinnt \cite{edge_medium,datacenter_llm}.




